
# Setup

## Installation und Kursverzeichnis

- Laden Sie die [Statistik-Software R](https://ftp.fau.de/cran/) herunter und installieren Sie sie. Die neueste Version ist derzeit `4.0.x` (Stand: April 2021).

- Laden Sie außerdem [RStudio](https://rstudio.com/products/rstudio/download/#download) herunter und installieren Sie es.

- Schauen Sie sich diese [kurze Einführung in RStudio](https://www.youtube.com/watch?v=tyvEHQszZJs) an.

- Legen Sie außerdem ein Verzeichnis für diesen Kurs auf Ihrer Festplatte an.

## R projects

Für diesen Kurs werden wir ein *R project* anlegen, da Ihnen dies die Arbeit mit R auf lange Sicht erleichtern wird.

- Öffnen Sie RStudio und klicken Sie oben rechts auf die Schaltfläche `Project: (None)`.

- Klicken Sie dann auf `New Project` und im Folgenden auf `Existing Directory` und wählen Sie mit `Browse` das Kursverzeichnis aus, das Sie eben angelegt haben. 

- Schließen Sie den Vorgang ab, indem Sie auf `Create Project` klicken. 

RStudio öffnet nun automatisch das Projekt, das genauso benannt ist wie Ihr Kursverzeichnis (siehe Schaltfläche oben rechts). Sie können das Projekt über diese Schaltfläche schließen und anschließend auch wieder öffnen. Das Projekt lässt sich außerdem öffnen, indem Sie in Ihrem Kursverzeichnis auf die neu angelegte Datei mit der Endung `.Rproj` klicken.

Sollten Sie Probleme beim Erstellen des Projekts haben, empfehlen wir Ihnen diese [Video-Kurzanleitung](https://www.youtube.com/watch?v=hKoSJGWnFFA).

<div class="gray">
**Weiterführende Infos: R Projekte**

R Projekte haben viele Vorteile, insbesondere wenn Sie in mehreren Kursen mit R arbeiten und für jeden ein eigenes R Projekt haben. Das Projekt merkt sich, welche Dateien Sie geöffnet haben, und stellt diese beim nächsten Öffnen des Projekts wieder her, sodass Sie da weiterarbeiten können, wo Sie aufgehört haben. Außerdem ist das *working directory* des Projekts Ihr Kursverzeichnis, das heißt alle Dateien, die Sie im Verlauf des Kurses dort ablegen, können Sie ganz einfach über das Panel unten links (Tab `Files`) oder über relative Pfade öffnen. Ihr Arbeitsverzeichnis können Sie übrigens überprüfen, indem Sie in der Konsole `getwd()` eingeben und Enter drücken.
</div>

# Erste Berechnungen in R

Unten links in RStudio sehen Sie die `Console`. Darin wird Code ausgeführt, d.h. die Konsole ist die Verbindung zwischen R und dem Computer, der die Berechnungen umsetzt. Geben Sie z.B. den folgenden Code Zeile für Zeile in die Konsole ein und drücken Sie danach Enter:

```{r}
1 + 1
10 - 5
3 * 4
12 / 6
2^4
```

Die Konsole gibt Ihnen die Antwort auf die eingegebene Rechnung zurück.

## Variablen & Funktionen

Natürlich werden wir die Konsole nicht nur als Taschenrechner benutzen. Ganz häufig wollen wir bestimmte Werte mehrfach verwenden, ohne jedes Mal den Wert oder die Berechnung eingeben zu müssen. Deshalb speichern wir Werte als sogenannte **Variablen** ab. Diese Variablen sind dann in Ihrem *Environment* (Panel oben rechts) vorhanden.

Um eine neue Variable anzulegen, gibt man in der Konsole zuerst den gewünschten Variablennamen ein, dann den Zuweisungspfeil `<-` und dann den zu speichernden Wert. Im folgenden legen wir eine Variable namens `summe` an, die den Wert `1 + 1` enthält:

```{r}
summe <- 1 + 1
```

Um den Inhalt der Variable anzusehen, geben Sie einfach den Variablennamen in die Konsole ein und drücken wieder Enter:

```{r}
summe
```

Sie sehen, dass nicht `1 + 1` zurückgegeben wird, sondern `2`. Wann immer wir den Wert der Berechnung `1 + 1` verwenden wollen, können wir stattdessen auch `summe` verwenden:

```{r}
summe + 3
```

Achtung! Variablen werden in R **ohne Warnmeldung überschrieben**:

```{r}
x <- 4
x
x <- 3
x
```

In Ihrem Environment haben Sie nun zwei Variablen: `summe` hat den Wert 2, `x` hat den Wert 3. Sie können auch herausbekommen, welche Variablen in Ihrem Environment sind, indem Sie eine sogenannte **Funktion** verwenden. Funktionen (auch: Befehle) führen Aktionen aus. Dahinter steht Code, den jemand geschrieben und für alle NutzerInnen verfügbar gemacht hat (man kann auch selbst Funktionen schreiben, aber das werden wir in diesem Kurs nicht behandeln). Geben Sie folgendes in Ihre Konsole ein und drücken Sie Enter:

```{r}
ls()
```

Die Funktion heißt `ls()`, das steht für *list* (auflisten), und gibt Ihnen die Namen aller Variablen in Ihrem Environment zurück. Funktionen können Sie an den runden Klammern nach dem Funktionsnamen erkennen. In den runden Klammern stehen die sogenannten **Argumente** der Funktion, also Angaben, die die Funktion benötigt, um etwas berechnen zu können. `ls()` ist eine der wenigen Funktionen in R, die keine Argumente benötigen.

Eine weitere nützliche Funktion ist `rm()` (*remove*), damit können Sie Variablen aus dem Environment entfernen (Achtung: diese Entscheidung ist endgültig!). Die Funktion bekommt als Argumente die Namen der Variablen, die Sie löschen wollen. Im Folgenden entfernen wir die Variable `x`:

```{r}
rm(x)
ls()
```

## Objektklassen

Bisher haben wir nur mit Zahlen (sog. *numerics*) gearbeitet. In R gibt es noch viele weitere Arten von Objekten. Bei den numerischen Objekten unterscheidet man zwischen *double* (Dezimalzahlen) und *integer* (Ganzzahlen).

```{r}
x <- 3.2   # double
x
y <- 4     # integer
y
```

Es gibt außerdem Schriftzeichen-Objekte (engl. *strings* oder *character*), die immer in Anführungszeichen gesetzt werden müssen:

```{r}
z <- "wir lernen programmieren"
z
```

... und die zwei booleschen Werte (engl. *booleans* oder *logicals*), die ohne Anführungszeichen aber in Großbuchstaben geschrieben werden:

```{r}
a <- TRUE    # Kurzform: T
a
b <- FALSE   # Kurzform: F
b
```

Es gibt zusätzlich noch eine wichtige Objektklasse namens *factor*, die für kategorische Daten verwendet wird. Diese Art von Daten werden wir, zusammen mit noch ein paar weiteren Objektklassen, später kennenlernen.

Um herauszufinden, welche Objektklasse eine Variable hat, verwendet man die Funktion `class()`. Diese Funktion bekommt nur ein Argument, nämlich den Namen der Variable, deren Objektklasse Sie erfragen wollen.

```{r}
class(a)
class(y)
class(z)
```

## Vektoren

Die Funktion `c()` (*concatenate*) erzeugt einen Vektor, d.h. eine Datenstruktur mit mehreren Elementen desselben Typs. 

```{r}
vec <- c("a", "b", "c") # alle Elemente sind Schriftzeichen
vec

vec <- c(3, 6, 89.3, 0, -10)  # alle Elemente sind numerisch (genauer: doubles)
vec
```

Sollten die Elemente unterschiedlichen Klassen angehören (*strings*, *booleans*, *numerics*), werden die Elemente still, also ohne Warnmeldung, in denselben Typ umgewandelt.

```{r}
c(3, 4, "string", T)      # alle Elemente werden in strings umgewandelt
c(2, 5, T, F)             # alle Elemente werden in numerics umgewandelt; TRUE = 1, FALSE = 0
```

## Arithmetik und logische Operatoren

Sie haben schon gesehen, dass die Konsole wie ein Taschenrechner funktioniert. Die Grundrechenarten sowie arithmetische Funktionen können Sie auf alle numerischen Objekte anwenden, auch auf numerische Vektoren:

```{r}
a <- c(10, 4, 20)
a * 10

b <- c(5, 2, 7)
a + b
```

In R gibt es viele arithmetische Funktionen, die als Argument einfach eine numerische Variable bekommen:

```{r}
sum(a)        # Summe aller Elemente
sqrt(a)       # Wurzel (square root) pro Element
log(a)        # Logarithmus eines jeden Elements
exp(a)        # Exponential für jedes Element
```

Logische Operatoren vergleichen zwei Variablen derselben Objektklasse miteinander. Die folgenden logischen Operatoren existieren in R:

```{r, eval = F}
x < y       # weniger als
x > y       # mehr als
x <= y      # weniger als oder gleich
x >= y      # mehr als oder gleich
x == y      # genau gleich
x != y      # ungleich
!x          # nicht x
x | y       # x ODER y
x & y       # x UND y
isTRUE(x)   # prüfen, ob x TRUE ist
x %in% y    # prüfen, ob ein Wert x in einem Vektor y enthalten ist
```

Die Antwort auf diese Operatoren sind die booleschen Werte, also entweder `TRUE` oder `FALSE`, wie folgende Beispiele zeigen:

```{r}
x <- 3
y <- 4
x == y
x != y
x > y
x <- c(1, 2, 3, 4, 5)
x == 3
"a" %in% c("a","b","c")
```

Die logischen Operatoren werden später noch sehr wichtig werden.

## Vektoren manipulieren

Wir wollen Ihnen noch ein paar Funktionen vorstellen, die hilfreich bei der Arbeit mit Vektoren sind. 

Neben `c()` gibt es noch weitere Funktionen, mit denen man Vektoren erstellen kann. Zuerst gibt es eine Kurznotation für numerische Vektoren von aufeinander folgenden Ganzzahlen, nämlich den Doppelpunkt:

```{r}
1:10
10:1
```

Die Funktion `seq()` erzeugt einen Vektor mit numerischen Intervallen, also Zahlensequenzen, bei denen alle Zahlen gleich weit voneinander entfernt sind. Die Funktion bekommt drei Argumente: den ersten (`from`) und den maximalen (nicht zwangsläufig letzten) Wert des Intervalls (`to`), und dann entweder die gewünschte Länge des Vektors (`length.out`) oder die Abstufung des Intervalls (`by`).

```{r}
seq(from = 10, to = 20, length.out = 5)	  # 5 Intervalle zwischen 10 und 20
seq(from = 10, to = 20, by = 1.5)	    # In Intervallen von 1.5
```

<div class="gray">
**Weiterführende Infos: Argumente in Funktionen**

Oben sehen Sie zum ersten Mal, dass Argumente Namen haben können (z.B. `from`, `to`, `length.out` und `by`). Wenn Argumente Namen haben (was fast immer der Fall ist), können Sie entweder die Namen dieser Argumente verwenden wie oben gezeigt, oder sie lassen die Namen komplett weg.

```{r}
seq(10, 20, length.out = 5)
```

Wenn Sie die Namen weglassen, müssen Sie die Argumente in der richtigen Reihenfolge verwenden. Die Reihenfolge der Argumente können Sie auf den Hilfeseiten herausfinden. Navigieren Sie dazu in den Tab `Help` im Panel unten rechts und geben Sie den Funktionsnamen `seq` in die Suchleiste ein. Wenn Sie Enter drücken, sollte die Hilfeseite der Funktion erscheinen. Die Reihenfolge der Argumente für diese Funktion ist `from`, `to` und dann entweder `by` oder `length.out`, wie Sie sehen können. Wir können die Argumentnamen `from` und `to` weglassen, solange wir die richtigen Zahlen an der richtigen Stelle in der Funktion verwenden (also zuerst den Wert für `from`, dann den Wert für `to`). Der Funktion ist aber nicht automatisch klar, ob als drittes Argument `by` oder `length.out` kommt, daher schreiben wir das Argument hier ausführlich als `length.out = 5`. Wenn wir dies nicht machen würden, würde die Funktion das Argument an dritter Stelle in der Funktion als den Wert für `by` interpretieren:

```{r}
seq(10, 20, 5)
```

Wenn wir allerdings die Namen der Argumente verwenden, ist die Reihenfolge egal:

```{r}
seq(to = 20, by = 1.5, from = 10)
```

</div>

Die Funktion `rep()` wiederholt Werte (egal ob numerisch, logisch oder *strings*). Neben den zu wiederholenden Werten bekommt die Funktion das Argument `times` und/oder das Argument `each` (oder das Argument `length.out`, das wir hier erstmal ignorieren). Im folgenden demonstrieren wir, was diese Argumente ausrichten (schauen Sie auch auf die Hilfsseite für diese Funktion!):

```{r}
rep(1, times = 3)
rep("a", times = 2)

vek <- c("a", "b")
rep(vek, times = 4)
rep(vek, each = 4)
rep(vek, times = 3, each = 3)
```

Zuletzt gibt es noch zwei verwandte Funktionen, die Schriftzeichen-Vektoren generieren: `paste()` und `paste0()`. Die Funktion `paste()` bekommt zunächst all die Elemente, die Sie miteinander verbinden wollen, und optional das Argument `sep` (*separator*), das bestimmt, mit welchem Zeichen die Elemente verbunden werden. `paste0()` bekommt nur die Elemente, die dann direkt aneinander gebunden werden. Wenn die Elemente alle einfache Variablen sind, erstellen `paste()` und `paste0()` ein einfaches Schriftzeichenobjekt, wenn eines der Elemente ein Vektor ist, ergibt sich ein Schriftzeichen-Vektor. 

```{r}
paste("a", "b", "c", sep = " ")
paste("a", "b", "c", sep = "")
paste0("a", "b", "c")

paste("subject", 1:5, sep = "_")
paste0("subject", 1:5)
```

Es gibt noch einige weitere nützliche Funktionen, um Vektoren zu manipulieren oder etwas über den Inhalt der Vektoren zu erfahren. Schauen Sie mal in Ihr Environment. Dort befinden sich jetzt drei einfache Variablen: `summe`, `y` und `z`. Alle anderen Variablen sind Vektoren. Sie sehen z.B. bei der Variable `a`, dass es sich um einen numerischen Vektor der Länge 3 (d.h. er enthält 3 Elemente) handelt: dort steht `num [1:3]`. `num` steht für *numeric*, die Notation `[1:3]` bedeutet, dass dies ein 1-dimensionales Objekt der Länge 3 ist. Um die Länge eines Vektors herauszufinden, ohne in Ihr Environment zu schauen, gibt es die Funktion `length()`:

```{r}
length(a)
length(vec)
```

Wenn Sie erfahren möchten, welche unterschiedlichen (einzigartigen) Elemente in einem Vektor sind, nutzen Sie `unique()`:

```{r}
vec <- c(1, 5, 2, 7, 6, 3, 7, 5)
unique(vec)

vec <- c("i", "i", "a", "a", "E", "E", "E", "E", "U")
unique(vec)	
```

Zuletzt stellen wir Ihnen noch eine sehr vielseitig einsetzbare Funktion vor, nämlich `table()`. Wenn diese Funktion auf einen Vektor angewendet wird, erhalten Sie als Ergebnis eine Auflistung der unterschiedlichen Elemente des Vektors mit ihrer Vorkommenshäufigkeit in diesem Vektor:

```{r}
table(vec)
```

Im Vektor `vec` gibt es also zwei Mal das Element "a", vier Mal das Element "E", usw. Wir werden später noch sehen, wofür `table()` sonst noch eingesetzt werden kann.

## Faktoren

Nachdem Sie jetzt wissen, was ein Vektor ist, möchten wir Ihnen noch eine Objektklasse namens Faktor (*factor*) vorstellen. Faktoren werden für kategoriale Daten verwendet, also solche, die nur eine begrenzte Anzahl an verschiedenartigen Werten annehmen können. Einen Faktor erzeugt man mit der Funktion `factor()`. Hier erzeugen wir zum Beispiel einen Faktoren-Vektor, der verschiedene Alterskategorien enthält:

```{r}
age <- factor(c("jung", "alt", "alt", "mittel", "jung", "jung"))
class(age)
age
```

Bei der Ausgabe des Faktors `age`, den wir angelegt haben, sehen wir zuerst die eingegebenen Werte in der eingegebenen Reihenfolge. Obwohl wir die Werte als Schriftzeichen eingegeben haben, werden sie nicht als Schriftzeichen (in Anführungszeichen) ausgegeben. Das liegt daran, dass dies jetzt nicht mehr einfache Schriftzeichen, sondern Kategorien sind. Die Kategorien werden auch Levels genannt und in der Ausgabe ebenfalls angezeigt. Man kann die Levels auch mittels der Funktion `levels()` abfragen:

```{r}
levels(age)
```

Die Funktion `factor()` kann ein Argument namens `levels` bekommen, mit dem man die Levels selbst bestimmen und ihre Reihenfolge festlegen kann (R ordnet die Levels sonst alpha-numerisch aufsteigend).

```{r}
age <- factor(c("jung", "alt", "alt", "mittel", "jung", "jung"), 
              levels = c("jung", "mittel", "alt"))
age
```

Auch numerische Werte können (in eher seltenen Fällen) kategorial sein. Nehmen wir an, Sie haben fünf Kommilitoninnen nach ihrem Alter in Jahren gefragt und die Werte in einem Vektor abgespeichert:

```{r}
age <- c(22, 25, 23, 22, 23)
```

Wenn Sie das Alter in Jahren als kategorial betrachten, können Sie den Vektor in einen Faktor umwandeln.

```{r}
age <- factor(c(22, 25, 23, 22, 23))
age
```

# Data Frames

Eine äußerst wichtige Datenstruktur in R ist der Data Frame. Das ist eine zwei-dimensionale Tabelle. Die Zeilen werden auch *observations* genannt und die Spalten *variables* (nicht zu verwechseln mit den Variablen von vorhin!). In der Phonetik arbeiten wir sehr häufig mit Data Frames, z.B. wenn wir akustische Informationen aus Sprachaufnahmen oder Messungen aus einem Perzeptionsexperiment extrahiert haben und diese auswerten oder statistisch analysieren wollen.

## Import & Export

In R gibt es verschiedene Möglichkeiten, eine Tabelle im Format `.csv` oder `.txt` einzulesen. Wenn Sie eine Tabelle von Ihrer Festplatte einlesen wollen, können Sie im Panel mit dem Environment in der Werkzeugleiste auf `Import Dataset` klicken und sich von dem Assistenten leiten lassen. Der Befehl, den der Assistent zum Laden des Data Frames verwendet, wird übrigens in der Konsole angezeigt.

In diesem Kurs werden wir u.a. Data Frames von einer Webseite benutzen, daher müssen wir den Befehl zum Einlesen selbst schreiben. Der Befehl, den wir verwenden, lautet `read.table()`, und bekommt als wichtigstes Argument in Anführungszeichen den Pfad (bzw. die URL) zu dem Data Frame (aber schauen Sie auf die Hilfeseite dieser Funktion, um alle Argumente und weitere verwandte Funktionen zum Einlesen von Daten zu sehen):
  
```{r}
ai <- read.table("http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf/ai.txt")
```

Da wir im Verlauf der Vorlesung mehrere Data Frames von derselben Webseite verwenden werden, können wir das Einlesen etwas cleverer gestalten, damit wir nicht immer wieder die komplizierte URL abtippen oder kopieren müssen. Dafür legen wir eine Variable mit der URL als Schriftzeichenobjekt an und verwenden dann die Funktion `file.path()`, mittels derer die URL und der Dateiname zusammengefügt werden:
  
```{r}
url <- "http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf"
file.path(url, "ai.txt")

# zusammen mit dem Befehl zum Einlesen:
ai <- read.table(file.path(url, "ai.txt"))
```

Das Gegenstück zu `read.table()` ist `write.table()`, womit Sie einen Data Frame abspeichern können. Diese Funktion bekommt zuerst den Namen des Objekts, das abgespeichert werden soll, dann den Pfad samt gewünschtem Dateinamen (`./` steht für das aktuelle Verzeichnis), und als weiteres optionales Argument nutzen wir `row.names = FALSE`, damit es nicht eine Spalte mit den (hier nicht vorhandenen) Zeilennamen in dem abgespeicherten Data Frame gibt.
  
```{r, eval = F}
write.table(ai, file.path("./", "ai.txt"), row.names = FALSE)
```

Natürlich können Sie Data Frames nicht nur einlesen, sondern auch selbst erstellen. Dafür wird der Befehl `data.frame()` verwendet. Diese Funktion bekommt als Argumente die Spaltennamen und anschließend die Werte, die in der Spalte stehen sollen. Hier ein Beispiel:
  
```{r}
df <- data.frame(F1 = c(240, 220, 250, 210, 280, 520, 510, 605, 670, 613),
                 vowel = rep(c("i","o"), each = 5))
df
```

Data Frames haben eine eigene Objektklasse:

```{r}
class(df)
```

## Eigenschaften

Wenn wir mit Datenstrukturen arbeiten, die viele Informationen enthalten, ist es wichtig, uns mit dem Objekt vertraut zu machen. R bietet viele nützliche Funktionen, um sich Data Frames anzuschauen und etwas über deren Eigenschaften zu erfahren:

```{r, eval = F}
# Data Frame in einem Sub-Fenster anschauen
View(ai)
```

```{r}
# Nur die ersten oder letzten paar Zeilen (Beobachtungen) ausgeben
head(ai)
tail(ai)

# Reihen- und Spaltenanzahl
nrow(ai)
ncol(ai)
dim(ai)

# Spaltennamen (Variablennamen)
colnames(ai)
names(ai)
```

## Auf Spalten zugreifen 

Obwohl wir in den kommenden Wochen mit der modernen *tidyverse*-Syntax arbeiten werden, wollen wir hier zeigen, wie man in traditioneller Weise auf Spalten in einem Data Frame zugreifen kann. Manchmal werden Sie nicht darum herum kommen, diese traditionelle Notation zu verwenden.

Wenn Sie in Ihr Environment schauen, sehen Sie, dass dort unter "Values" die ganzen einfachen Variablen und Vektoren stehen und unter "Data" finden Sie die zwei Data Frames `ai` und `df`. Als Informationen zu den Data Frames stehen im Environment immer die Anzahl an *observations* (Zeilen) und *variables* (Spalten), z.B. `25 obs. of 3 variables`. Wenn Sie auf das kleine blaue Icon links neben dem Namen des Data Frames klicken, sehen Sie die Spaltennamen, welcher Objektklasse die Spalten angehören (`int` für *integer*, `num` für *numerics* bzw. *doubles*, usw.) und die ersten paar Werte aus der Spalte. Dieselben Informationen erhalten Sie, wenn Sie die Funktion `str()` (*structure*) auf einen Data Frame anwenden:

```{r}
str(ai)
str(df)
```

Vor jeder Spalte in dieser Auflistung steht ein Dollarzeichen. Genau so können Sie auf Spalten in einem Data Frame zugreifen: Sie schreiben den Namen des Data Frames, dann (ohne Lehrzeichen!) das Dollarzeichen, und anschließend (wieder ohne Leerzeichen!) den Spaltennamen:

```{r}
df$F1
```

Sie sehen, dass eine Spalte an und für sich nichts anderes ist als ein Vektor! Das heißt, Sie können die Funktionen, mit denen wir zuvor Vektoren manipuliert haben, jetzt auch auf Spalten in Data Frames anwenden:

```{r}
length(df$F1)
table(df$vowel)
```

<div class="gray">
**Weiterführende Informationen: Factors in Data Frames**

Für den Data Frame `df`, den wir oben selbst angelegt haben, sehen Sie im Environment, dass die Spalte `vowel` der Objektklasse *factor* angehört, und dass dieser *factor* zwei *levels* hat -- obwohl wir die Spalte `vowel` mit Schriftzeichen gefüllt hatten! Die Funktion `data.frame()`, mit der wir `df` erstellt haben, hat ein Argument namens `stringsAsFactors`, das, wenn nicht anders angegeben, automatisch `TRUE` ist. Das heißt, beim Erstellen des Data Frames wurden die Schriftzeichen in der Spalte `vowel` in einen *factor* umgewandelt. Die zwei verschiedenartigen Werte (Kategorien) in dieser Spalte sind "i" und "o", dementsprechend hat dieser *factor* zwei *levels*.

Wenn Sie sich nochmal die Informationen zu dem Data Frame `df` im Environment oder Ihrer Konsole anschauen, werden Sie feststellen, dass die Spalte `vowel` nicht nur ein Faktor mit zwei Levels ist, sondern dass die ersten Werte in dieser Spalte komischerweise Zahlen sind und nicht "i" und "o". Das liegt daran, dass Faktoren im Hintergrund (also für die NutzerInnen meist unsichtbar) als *integer* abgespeichert werden. Diese *integer* sind mit den einzigartigen Levels des Faktors assoziiert. Wenn also in unserem Environment bei der Spalte `vowel` der Wert 1 steht, dann repräsentiert dies das Level "i", der Wert 2 repräsentiert das Level "o".

Wenn Sie beim Erstellen des Data Frames verhindern möchten, dass *strings* in *factors* umgewandelt werden, verwenden Sie das Argument `stringsAsFactors = FALSE`:

```{r}
df <- data.frame(F1 = c(240, 220, 250, 210, 280, 520, 510, 605, 670, 613),
                 vowel = rep(c("i","o"), each = 5),
                 stringsAsFactors = FALSE)
```

</div>

# Arbeit mit RStudio

Zuletzt möchten wir Ihnen noch einige wichtige Informationen zur Arbeit mit R und RStudio mitgeben.

## Packages und R Version

Für R gibt es viele Tausend *packages* bzw. *libraries*, die uns die Arbeit erleichtern werden. Bitte installieren Sie nun folgende Packages (das dauert eine Weile!):

```{r, eval = F}
install.packages(c("Rcpp", "remotes", "knitr", "tidyverse", "magrittr", "rmarkdown", "emuR", "gridExtra"))
```

<div class="gray">
**Weiterführende Infos: Installation von R Paketen**

Sollte der obige Befehl den Fehler `installation of package had non-zero exit status` werfen, hat die Installation nicht geklappt. Für Windows kann es sein, dass Sie in diesem Fall zusätzlich [Rtools](https://cran.r-project.org/bin/windows/Rtools/) installieren müssen. Für MacOS müssen Sie ggf. die *XCode command-line tools* installieren und/oder resetten. Öffnen Sie dafür ein Mac Terminal und führen Sie folgende Befehle aus:

```{bash, eval = F}
xcode-select --install
# Falls die Installation der R Packages dann immer noch nicht klappt:
xcode-select --reset
```

**Wenn Sie sich unsicher sind, wie Sie auftretende Fehler bei der Installation der R Packages für Ihr Betriebssystem beheben können, können Sie uns auch gerne fragen!**
</div>

Einige Basispakete werden automatisch aktiviert beim Öffnen von RStudio, die meisten aber müssen Sie erst laden, bevor Sie die Funktionen verwenden können, die die Pakete anbieten. Zum Laden von Packages benutzen Sie den Befehl `library()`:

```{r, eval = F}
library(tidyverse)
library(emuR)
```

<div class="gray">
**Weiterführende Infos: Updates**

Bitte überprüfen Sie regelmäßig, ob Ihre Packages Updates benötigen -- die Updates werden in R nicht automatisch eingespielt! Klicken Sie hierfür in der Werkzeugleiste auf `Tools > Check for Package Updates`. Auch RStudio selbst erhält ab und zu Updates, dies können Sie überprüfen mit `Help > Check for Updates`.

R muss ebenfalls aktuell gehalten werden. Sie können Ihre R Version überprüfen mit `getRversion()`. Besuchen Sie einfach in regelmäßigen Abständen die [R Webseite](https://ftp.fau.de/cran/) und schauen Sie, ob eine neue stabile Version verfügbar ist.
</div>

## Sessions

Eine Session beginnt, wenn man R bzw. RStudio startet bzw. wenn man ein Projekt öffnet. Man beendet eine Session entweder mit `Session > Quit Session` in der Werkzeugleiste oder mit `Strg+Q` bzw. `Ctrl+Q`. Die Session endet außerdem automatisch, wenn Sie RStudio schließen. 

Sie werden dann gefragt, ob Sie das *workspace image* speichern wollen. Wenn Sie die Variablen, die Sie in der Session angelegt haben, im Environment behalten und in der nächsten Session wieder verwenden wollen, klicken Sie auf `Save`. Der *workspace* wird dann in Ihrem Kursverzeichnis in einer Datei mit der Endung `.RData` abgelegt. Wenn Sie den *workspace* nicht speichern möchten, klicken Sie auf `Don't save`. Falls Sie die Session doch nicht beenden wollen, klicken Sie auf `Cancel`. 

**Für diesen Kurs bitten wir Sie, den *workspace* nicht zu speichern (`Don't save`).**

## Dokumentarten

### R Skripte

Die Konsole in RStudio ist die direkte Verbindung zu R, d.h. dort kann R Code direkt ausgeführt werden, so wie Sie das in dieser Vorlesung bisher gemacht haben. Um aber Ihren Code jederzeit replizieren zu können, müssen Sie ihn in einem Dokument festhalten. Üblicherweise werden Sie das in Ihrem Arbeitsalltag in einem **R Skript** machen. Ein R Skript kann einfach erstellt werden über `File > New File > R Script` (bzw. `Strg + Shift + N`) und sollte immer mit der Dateiendung `.R` abgespeichert werden. Ein R Skript enthält ausschließlich ausführbaren Code. Beim Ausführen eines Skriptes wird eine Zeile nur dann von R ignoriert, wenn sie mit `#` beginnt; dann ist die Zeile auskommentiert. Es gibt verschiedene Möglichkeiten, ein R Skript auszuführen. Markieren Sie die gewünschten Zeilen (wenn es nur eine Zeile ist, setzen Sie einfach den Cursor in die Zeile), und klicken Sie in der kleinen Werkzeugleiste im Panel mit dem geöffneten Skript auf `Run` oder drücken Sie `Strg+Enter` bzw. `Ctrl+Enter`. Das Ergebnis sehen Sie sofort in der Konsole.

### R Markdown

In den letzten Jahren hat sich aber auch eine andere Dokumentart etabliert, insbesondere für die Erstellung von Berichten und Lehrmaterial: das **R Markdown**. R Markdown ist eine Art Textdokument, in das man Code Snippets einbetten kann, die ganz normal ausgeführt werden können (wie oben beschrieben). Ein solches Dokument enthält häufig mehr Text als Code. Sie können ein R Markdown erstellen mit `File > New File > R Markdown` und es ist Konvention, das Dokument mit der Dateiendung `.Rmd` abzuspeichern. Eine R Markdown Datei wird im Normalfall in ein anderes Format umgewandelt ("ge-*knitted*"), z.B. in eine HTML, eine PDF, oder sogar ein Word Dokument. Dies geschieht entweder über den Wollknäuel-Button mit der Aufschrift `Knit` oder mittels:

```{r, eval = F}
library(rmarkdown)
render("Uebung1.Rmd")
```

Wir benutzen R Markdown, um z.B. die HTML herzustellen, die Sie gerade lesen. **Auch Ihre Übungen werden Sie in einem R Markdown erledigen.** Der Vorteil davon ist, dass Sie für jede Ihrer Übungen einen schönen HTML-Report haben und trotzdem gut am Code arbeiten können.

Ab der zweiten Vorlesungswoche werden wir Ihnen anstatt der HTML die rohe R Markdown Datei für die Vorlesung hochladen. Dies wird es Ihnen erleichern, den Code auszuprobieren, denn im Markdown können die Code Snippets wie gesagt direkt ausgeführt werden (u.a. auch über den kleinen grünen Pfeil oben rechts in jedem Code Snippet). Trotzdem können Sie die Vorlesung, wenn Sie sie in eine HTML umgewandelt haben, entspannt im Browser oder im RStudio Viewer (Panel unten rechts) lesen.

Im Markdown Dokument werden für Textmarkierungen besondere Zeichen verwendet, die dann beim *knitten* interpretiert und umgesetzt werden:

\# Überschrift: Mit einem Hashtag bekommt man die größtmögliche Überschrift; je mehr Hashtags man benutzt, desto kleiner wird die Überschrift

\*\*fett\*\*: Mit doppeltem Asterisk vor und hinter einer Textpassage wird der Text fett gesetzt

\*kursiv\*: Mit einfachem Asterisk wird der Text kursiv

\`code\`: die einfachen rückwärts gewandten Anführungszeichen heben den darin enthaltenen Text hervor; das wird üblicherweise für Code oder Variablen benutzt, wenn man sich außerhalb eines Code Snippets befindet; dieser Code kann aber nicht ausgeführt werden!

\`\`\`: Die dreifachen rückwarts gewandten Anführungszeichen markieren den Anfang und das Ende eine Code Snippets (auch Code Block genannt). Dazwischen darf nur Code geschrieben werden; Text muss mit einem Hashtag als Kommentar verfasst werden. Am Anfang des Code Snippets wird außerdem in geschweiften Klammern angegeben, welche Programmiersprache man im Code Block schreibt (in unserem Fall: {r}).

Noch mehr Informationen dazu finden Sie im [Cheatsheet zu R Markdown](https://www.rstudio.org/links/r_markdown_cheat_sheet) (insb. Seite 2, linke Spalte).

## Hilfe zur Selbsthilfe

### Fehler erkennen

- Warnzeichen: Wenn Sie einen groben Syntaxfehler in einem Dokument haben, sehen Sie am Rand kleine rote Warnzeichen. Diese sollten nicht ignoriert werden, denn sie weisen darauf hin, dass Sie einen Fehler gemacht haben. Wenn Sie den Fehler korrigieren, verschwinden die Warnzeichen.

- "Knit": Wir empfehlen, dass Sie Ihr Markdown-Dokument regelmäßig, während Sie an der Übung arbeiten, in eine HTML überführen, indem Sie oben in der Werkzeugleiste auf "Knit" klicken. Wenn alles klappt, sehen Sie hoffentlich in einem neuen Fenster oder im Viewer (Panel unten rechts in RStudio) die kompilierte HTML. Wenn Sie aber Syntaxfehler oder andere Fehler in Ihrem Code haben, wird die HTML nicht erstellt und Sie kriegen stattdessen einen Fehler in der Konsole angezeigt. Dort sehen Sie auch, in welcher Zeile der Fehler ist.

- Code einzeln ausführen: Führen Sie jede neu geschriebene Zeile Code aus. So sehen Sie Ihr Ergebnis und können überlegen, ob das Ergebnis das gewünschte ist oder nicht.

### Community nutzen

Es gibt eine sehr große und hilfsbereite R Community, die Ihnen das Programmieren lernen mit R erleichtern wird. Hier ein paar gute Links und Befehle, falls Sie mal nicht weiter wissen:

- [Stack Overflow](https://stackoverflow.com/questions/tagged/r): Ein Blog, auf dem Sie höchstwahrscheinlich eine Antwort auf Ihre Frage zu R finden werden. Am einfachsten googlen Sie Ihre Frage auf Englisch; die Antwort eines Mitglieds von Stack Overflow wird bei den ersten Suchergebnissen dabei sein.

- [Hadley Wickham's "R for Data Science"](https://r4ds.had.co.nz/): Hadley Wickham ist der Chief Programmer des "tidyverse", mit dem wir uns noch auseinandersetzen werden. Seine Bücher sind sehr verständlich, gut strukturiert und kurzweilig zu lesen.

- Cheatsheets: Das sind PDFs, die eine Funktionsübersicht mit Erklärungen und ggf. Beispielen in absoluter Kurzform bieten. Sie finden einige Cheatsheets in der obersten Werkzeugleiste unter `Help > Cheatsheets`. Insbesondere die ersten drei sind für Sie interessant. Ansonsten kann man Cheatsheets auch googlen und findet dann z.B. [Data Transformation with dplyr](https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf) oder diese sehr ausführliche [Reference Card](https://cran.r-project.org/doc/contrib/Short-refcard.pdf).

- Vignetten: Zu einigen essentiellen Paketen gibt es so genannte "Vignetten", das sind meist HTMLs oder PDFs, die die Autoren eines Pakets geschrieben haben. Sie können mit folgender Konsoleneingabe nach Vignetten suchen:

```{r, eval = F}
# zum Beispiel zu einer Library aus dem tidyverse:
vignette("dplyr")
```

- In RStudio können Sie sich über die Eigenschaften einer Funktion informieren, indem Sie im Panel unten rechts mit dem Tab `Help` die gewünschte Funktion ins Suchfeld eingeben. Sie erhalten dann u.a. Informationen über die Argumente der Funktion und Beispiele. Dasselbe erreichen Sie über diese Konsoleneingaben (beispielhaft für Hilfe zur Funktion `getwd()`):

```{r, eval = F}
?getwd
help("getwd")
```



<style>
div.green {background-color: #ecffeb; border-radius: 5px; padding: 20px;}
div.gray {background-color: #e8e8e8; border-radius: 5px; padding: 20px;}
</style>

# Packages und Daten laden

Starten Sie das R Projekt, das Sie letzte Woche für diesen Kurs angelegt haben. Öffnen Sie hierfür RStudio und benutzen Sie die Schaltfläche oben rechts oder navigieren Sie zu Ihrem Kursverzeichnis und klicken Sie auf die `.Rproj` Datei. 

Stellen Sie bitte außerdem sicher, dass Sie die Pakete aus Vorlesung 1 installiert haben. Zwei davon werden wir nun laden.

```{r}
library(tidyverse)
library(magrittr)
```

Wir werden diese Woche mit verschiedenen Data Frames arbeiten, die wir vom IPS-Server laden. Die URL legen wir als Variable an und benutzen diese dann in der Funktion `file.path()`.

```{r}
url <- "http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf"
asp <- read.table(file.path(url, "asp.txt"))
int <- read.table(file.path(url, "intdauer.txt"))
vdata <- read.table(file.path(url, "vdata.txt"))
```

Nutzen Sie die Methoden, die Sie in der letzten Woche kennengelernt haben, um sich mit den einzelnen Data Frames vertraut zu machen!

# Einführung ins *tidyverse*

Das *tidyverse* ist eine Sammlung von Packages, die bei unterschiedlichen Aspekten der Datenverarbeitung helfen. Wir werden uns im Verlauf der nächsten Wochen mit einigen dieser *tidyverse*-Packages beschäftigen. Wenn Sie das *tidyverse* laden, sehen Sie folgendes:

![](img/library_tidyverse.png)

Zum *tidyverse* in der Version 1.3.0 gehören die acht dort aufgelisteten Packages (`ggplot2`, `tibble`, `tidyr`, `readr`, `purrr`, `dplyr`, `stringr`, `forcats`). All diese Pakete können Sie auch einzeln laden, wenn Sie das möchten. Zusätzlich wird angezeigt, dass es zwei Konflikte gibt: Die Notation `dplyr::filter()` bedeutet "die Funktion `filter()` aus dem Paket `dplyr`". Diese Funktion überschreibt die Funktion `filter()` aus dem Paket `stats` (das ist ein Paket, das den NutzerInnen ohne vorheriges Laden mittels `library()` immer zur Verfügung steht). Funktionen aus verschiedenen Paketen können sich gegenseitig überschreiben, wenn sie denselben Funktionsnamen haben, wie z.B. `filter()`. Wenn man nun also `filter()` verwendet, wird die Funktion von `dplyr` verwendet, nicht die Funktion von `stats`. Wenn man explizit die Funktion von `stats` verwenden will, kann man die obige Notation verwenden, also `stats::filter()`.

Viele Funktionen aus dem *tidyverse* dienen dazu, traditionelle R Notationen abzulösen. Diese traditionellen Notationen sind häufig recht sperrig; *tidyverse*-Code hingegen ist leicht zu lesen und zu schreiben. Wir verwenden *tidyverse*, um Data Frames aufzuräumen, zu filtern oder zu verändern.

## Pipes

Dazu müssen wir zuerst lernen, wie die *tidyverse*-Syntax funktioniert:

```{r}
asp %>% head()
```

Wir beginnen den Code immer mit dem Date Frame und hängen dann einfach alle Funktionen, die wir auf den Data Frame anwenden wollen, in chronologischer Reihenfolge an den Data Frame. Zwischen jeder Funktion steht die **Pipe `%>%`**. Die Pipe nimmt immer das, was links von der Pipe steht, und reicht es weiter an die Funktion, die rechts von der Pipe steht. Im Code oben wird also die Funktion `head()` auf den Data Frame `asp` angewendet. Dies ist genau dasselbe wie:

```{r}
head(asp)
```

In der *tidyverse*-Schreibweise mit der einfachen Pipe wird der Data Frame nicht verändert; das Ergebnis des Codes wird einfach in der Konsole ausgegeben. Wenn Sie das Ergebnis einer *tidyverse*-Pipe in einer Variable abspeichern wollen, nutzen Sie die übliche Notation mit dem Zuweisungspfeil `<-`:

```{r}
numberOfRows <- asp %>% nrow()
numberOfRows
```

Das Besondere ist, dass sie so viele Funktionen mit der Pipe aneinanderhängen können wie Sie wollen. Die Funktionen werden immer auf das Ergebnis der vorherigen Funktion angewendet, wie wir gleich sehen werden. Innerhalb der Funktionen können wir dank der Pipe auf alle Spalten des Data Frames einfach mittels ihres Namens zugreifen.

## Daten manipulieren mit `dplyr`

Die wichtigsten Funktionen, die Sie in Ihrem Alltag mit R brauchen werden, stammen aus dem Package [`dplyr`](https://dplyr.tidyverse.org/index.html). Wir unterteilen hier nach verschiedenen Arten von Operationen, die Sie auf Data Frames ausführen können.

### Filtering

Häufig werden wir aus Data Frames nur bestimmte Zeilen und/oder Spalten auswählen. Das Auswählen von Zeilen erfolgt mit der Funktion **`filter()`**. Als Argument(e) bekommt die Funktion einen oder mehrere logische Ausdrücke. Hierfür benötigen Sie die logischen Operatoren aus Vorlesung 1. Wenn Sie aus dem Data Frame `asp` alle Zeilen auswählen wollen, bei denen in der Spalte `Wort` "Montag" steht, nutzen Sie z.B. den Operator `==`:

```{r}
asp %>% filter(Wort == "Montag")
```

Alle Zeilen, bei denen die Dauer `d` kleiner ist als 10 ms, erhält man mit folgendem Ausdruck:

```{r}
asp %>% filter(d < 10)
```

Sie können natürlich auch mehrere logische Ausdrücke miteinander verbinden, nämlich mittels den Operatoren für "und" `&` oder für "oder" `|`. Mit dem folgenden Ausdruck werden nur Zeilen zurückgegeben, bei denen die Versuchsperson `Vpn` entweder "k01" oder "k02" oder "k03" ist und der Konsonant `Kons` ungleich "t":

```{r}
asp %>% filter(Vpn %in% c("k01", "k02", "k03") & Kons != "t")
```

Die Zeilen in einem Data Frame sind normalerweise durchnummeriert, d.h. die Zeilen haben einen Index. Wenn wir mittels des Index Zeilen auswählen wollen, nutzen wir **`slice()`** bzw. die verwandten Funktionen `slice_head()`, `slice_tail()`, `slice_min()` und `slice_max()`. Die Funktion `slice()` bekommt als Argument den Index der auszuwählenden Zeilen:

```{r}
asp %>% slice(4)             # Zeile 4 auswählen
asp %>% slice(1:10)          # die ersten 10 Zeilen auswählen
```

Die Funktionen `slice_head()` und `slice_tail()` bekommen als Argument die Anzahl der Zeilen `n`, die, angefangen bei der ersten bzw. der letzten Zeile, ausgewählt werden sollen.

```{r}
asp %>% slice_head(n = 2)   # die ersten zwei Zeilen auswählen
asp %>% slice_tail(n = 3)   # die letzten drei Zeilen auswählen
```

Die Funktionen `slice_min()` und `slice_max()` geben die `n` Zeilen zurück, die die niedrigsten bzw. höchsten Werte in einer Spalte haben. Wenn `n` nicht angegeben wird, wird automatisch `n = 1` verwendet, es wird also nur eine Zeile zurückgegeben. 

<div class="gray">
**Weiterführende Infos: Defaults für Argumente**

Wenn man bestimmte Argumente in Funktionen nicht spezifiziert, werden häufig sog. *defaults* verwendet. Schauen Sie sich zum Beispiel die Hilfeseite der Funktion `seq()` an. Dort wird die Funktion mit ihren Argumenten wie folgt aufgeführt:

![](img/seq.png)

Die Argumente `from` und `to` haben den *default*-Wert 1. Und da dies die einzigen obligatorischen Argumente sind, können Sie die Funktion auch völlig ohne Angabe der Argumente ausführen:

```{r}
seq()
```

Auch das Argument `by` hat einen *default*-Wert, der anhand der Werte von `to`, `from` und `length.out` berechnet wird, falls der/die NutzerIn keinen anderen Wert eingibt.

Meist finden Sie die *default*-Werte für die Argumente einer Funktion auf der Hilfeseite unter *Usage*, manchmal stehen die *default*-Werte auch erst in der Beschreibung der Argumente darunter.
</div>

Im folgenden zeigen wir Beispiele für die zwei Funktionen, die sich auf die Dauer in Spalte `d` des Data Frames `asp` beziehen.

```{r}
asp %>% slice_min(d)        # die Zeile auswählen, wo d den niedrigsten Wert hat
asp %>% slice_min(d, n = 5) # die fünf Zeilen auswählen, wo d die niedrigsten Werte hat
asp %>% slice_max(d)        # die Zeile auswählen, wo d den höchsten Wert hat
asp %>% slice_max(d, n = 5) # die fünf Zeilen auswählen, wo d die höchsten Werte hat
```

Diese beiden Funktionen lassen sich sogar auf Spalten anwenden, die Schriftzeichen enthält. In diesem Fall wird alphabetisch vorgegangen.

```{r}
asp %>% slice_min(Wort)     # die Zeilen, wo Wort den "niedrigsten" Wert hat
asp %>% slice_max(Wort)     # die Zeilen, wo Wort den "höchsten" Wert hat
```

Da es jeweils mehrere Zeilen gibt, wo `Wort` den niedrigsten ("abkaufen") bzw. höchsten Wert ("Zwischenstop") hat, werden all diese Zeilen zurückgegeben (trotz `n = 1`).`

### Selecting

Für das Auswählen von Spalten ist die Funktion **`select()`** da, die auf verschiedene Art und Weise benutzt werden kann. Als Argumente bekommt diese Funktion die Spaltennamen, die ausgewählt werden sollen. In den folgenden Beispielen sehen Sie außerdem zum ersten Mal, wie man mehrere Funktionen mit einfachen Pipes aneinander hängen kann, denn wir nutzen nach `select()` hier noch `slice(1)`, damit der Output der Funktionen nicht so lang ist.

```{r}
asp %>% select(Vpn) %>% slice(1)         # nur die Spalte Vpn
asp %>% select(Vpn, Bet) %>% slice(1)    # die Spalten Vpn und Bet
asp %>% select(d:Kons) %>% slice(1)      # die Spalten d bis einschl. Kons
asp %>% select(!(d:Kons)) %>% slice(1)   # alle Spalten außer die Spalten von d bis einschl. Kons
asp %>% select(-Wort) %>% slice(1)       # alle Spalten außer Wort
```

Innerhalb der Funktion `select()` können die Funktionen **`starts_with()`** und **`ends_with()`** sehr praktisch sein, wenn Sie alle Spalten auswählen wollen, deren Namen mit demselben Buchstaben oder derselben Buchstabenfolge beginnen bzw. enden. Dies demonstrieren wir anhand des Data Frames `vdata`, der folgende Spalten hat:

```{r}
vdata %>% colnames()
```

`starts_with()` erlaubt es uns, die beiden Spalten `F1` und `F2` auszuwählen, weil beide mit "F" beginnen:

```{r}
vdata %>% select(starts_with("F")) %>% slice(1)
```

Wie auch beim Filtern, können Sie mit den logischen Operatoren `&` bzw. `|` die Funktionen `starts_with()` und `ends_with()` verbinden. Hier wählen wir (auf etwas umständliche Weise) die Spalte `F1` aus:

```{r}
vdata %>% select(starts_with("F") & !ends_with("2")) %>% slice(1)
```

Es wird ab und zu vorkommen, dass wir (nach einer ganzen Reihe an Funktionen) nur eine Spalte ausgegeben haben wollen, aber nicht als Spalte (bzw. um genau zu sein: als Data Frame mit nur einer Spalte), sondern einfach als Vektor. Dafür nutzen wir **`pull()`**. Im folgenden Beispiel wählen wir zuerst die ersten zehn Zeilen von `asp` aus und lassen und davon dann die Spalte `Bet` als Vektor ausgeben:

```{r}
asp %>% slice(1:10) %>% pull(Bet)
```

An der Ausgabe sehen Sie, dass es sich bei `Bet` um einen Vektor handelt.

### Mutating

Mit *Mutating* können wir Spalten an Data Frames anhängen oder verändern. Der Befehl heißt **`mutate()`** und bekommt als Argumente den gewünschten neuen Spaltennamen mit den Werten, die in der Spalte stehen sollen. Wenn mehrere Spalten angelegt werden sollen, können Sie sie innerhalb der Funktion aneinanderreihen. Folgender Code legt zum Beispiel zwei neue Spalten namens `F1` und `F2` an:

```{r}
int %>% head()
int %>% mutate(F1 = c(282, 277, 228, 270, 313, 293, 289, 380, 293, 307, 238, 359, 300, 318, 231),
               F2 = c(470, 516, 496, 530, 566, 465, 495, 577, 501, 579, 562, 542, 604, 491, 577))
```

Diese neuen Spalten werden nicht automatisch im Data Frame abgespeichert! Es gibt zwei Möglichkeiten, um die Spalten dauerhaft an den Data Frame anzuhängen. Die erste ist wie üblich mit dem Zuweisungspfeil. Wir erstellen hier eine neue Variable `int_new`, die den erweiterten Data Frame enthält; man hätte auch den originalen Data Frame überschreiben können, indem man statt `int_new` nur `int` schreibt.

```{r}
int_new <- int %>% 
  mutate(F1 = c(282, 277, 228, 270, 313, 293, 289, 380, 293, 307, 238, 359, 300, 318, 231),
         F2 = c(470, 516, 496, 530, 566, 465, 495, 577, 501, 579, 562, 542, 604, 491, 577))
int_new %>% head()
```

Die zweite Möglichkeit ist die sogenannte **Doppelpipe** aus dem Paket `magrittr`: **`%<>%`**. Die Doppelpipe kann nur als erste Pipe in einer Reihe von Pipes eingesetzt werden (auch das werden wir noch sehen). Zudem muss als erstes Argument nach der Doppelpipe nicht mehr der Data Frame stehen, denn der steht schon links von der Doppelpipe.

```{r}
int %<>% mutate(F1 = c(282, 277, 228, 270, 313, 293, 289, 380, 293, 307, 238, 359, 300, 318, 231),
                F2 = c(470, 516, 496, 530, 566, 465, 495, 577, 501, 579, 562, 542, 604, 491, 577))
int %>% head()
```

Es gibt zwei Funktionen, die sehr hilfreich innerhalb von `mutate()` sind, wenn eine neue Spalte auf den Werten einer bereits existierenden Spalte beruhen soll. Für binäre Entscheidungen nutzen Sie **`ifelse()`**, für nicht binäre Entscheidungen nutzen Sie **`case_when()`**. 

Nehmen wir an, Sie wollen eine weitere Spalte an den Data Frame `int` anhängen. Sie wissen, dass Versuchsperson "S1" 29 Jahre alt ist, Versuchsperson "S2" ist 33 Jahre alt. Sie wollen eine Spalte `age` anlegen, die genau das festhält. Dann benutzen Sie die Funktion `ifelse()` innerhalb von `mutate()`. `ifelse()` bekommt als Argumente zuerst einen logischen Ausdruck, dann den Wert, der eingesetzt werden soll, wenn der logische Ausdruck für eine Zeile wahr ist (`TRUE`), und zuletzt den Wert für Zeilen, für die der logische Ausdruck unwahr ist (`FALSE`). Um also die neue Spalte zu erstellen, wird für jede Zeile geprüft, ob die Versuchsperson "S1" ist; wenn ja, wird in die neue Spalte `age` der Wert 29 eingetragen, ansonsten der Wert 33.

```{r}
int %>% mutate(age = ifelse(Vpn == "S1", 29, 33))
```

Bei nicht binären Entscheidungen wird statt `ifelse()` die Funktion `case_when()` eingesetzt. Diese Funktion bekommt so viele logische Ausdrücke und entsprechende Werte wie gewünscht. Zum Data Frame `int` wollen Sie eine weitere Spalte namens `noise` hinzufügen. Wenn in der Spalte `dB` ein Wert unter 25 Dezibel steht, soll in der Spalte `noise` "leise" stehen, bei Dezibelwerten zwischen 25 und 35 soll "mittel" und bei Dezibelwerten über 35 soll "laut" eingetragen werden. Die Schreibweise dieser Bedingungen ist wie folgt: Zuerst kommt der logische Ausdruck, dann die Tilde `~`, und abschließend der einzutragende Wert, wenn der logische Ausdruck für eine Zeile wahr ist.

```{r}
int %>% mutate(noise = case_when(dB < 25 ~ "leise",
                                 dB > 25 & dB < 35 ~ "mittel",
                                 dB > 35 ~ "laut"))
```

### Renaming

Häufig ist es sinnvoll, Spalten umzubenennen und ihnen vernünftige Namen zu geben. (Generell ist es sinnvoll, den Spalten von Anfang an sprechende Namen zu geben, also Namen, die zweifelsfrei beschreiben, was in der Spalte zu finden ist -- dies ist nicht trivial!)

Im Data Frame `asp` sind fast alle Spaltennamen Abkürzungen:

```{r}
asp %>% colnames()
```

Jetzt benennen wir die Spalten um und speichern das Ergebnis mittels der Doppelpipe direkt im Data Frame `asp` ab. Hierzu benutzen wir **`rename()`**. Als Argumente bekommt die Funktion zuerst den gewünschten Spaltennamen, dann ein `=`, und dann den alten Spaltennamen. Sie brauchen die Spaltennamen nicht in Anführungszeichen zu setzen. Wenn Sie gleich mehrere Spalten umbenennen wollen, können Sie das einfach mit Komma getrennt in der Funktion angeben.

```{r}
asp %<>% rename(Dauer = d, 
                Versuchsperson = Vpn, 
                Konsonant = Kons, 
                Betonung = Bet)
asp %>% colnames()
```

## Weitere Beispiele für komplexe Pipes

Wie Sie bereits gesehen haben, lassen sich viele Funktionen mit Pipes aneinanderhängen. Es ist dabei sehr wichtig, sich immer wieder vor Augen zu führen, dass jede Funktion auf das Ergebnis der vorherigen Funktion angewendet wird. Bei langen Pipes sollten Sie außerdem nach jeder Pipe einen Zeilenumbruch einfügen, weil dies die Lesbarkeit erhöht.

Die beiden folgenden Schreibweisen haben dasselbe Ergebnis und werfen auch keinen Fehler, aber sie gehen unterschiedlich vor. Im ersten Beispiel wird zuerst die Spalte `Versuchsperson` ausgewählt, dann wird die erste Zeile ausgewählt, beim zweiten Beispiel genau umgekehrt.

```{r}
asp %>% 
  select(Versuchsperson) %>% 
  slice(1)
asp %>% 
  slice(1) %>% 
  select(Versuchsperson)
```

Das kann unter Umständen zu Fehlern führen, wenn Sie nicht genau aufpassen, in welcher Reihenfolge Sie Funktionen auf einen Data Frame anwenden. Sie möchten zum Beispiel aus dem Data Frame `vdata` die Spalte `X` auswählen, aber auch in `Alter` umbenennen. Dann wird der folgende Code einen Fehler werfen, weil sich die Funktion `select()` nicht mehr auf die Spalte `X` anwenden lässt, *nachdem* die Spalte bereits in `Alter` umbenannt wurde:

```{r, error=TRUE}
vdata %>% 
  rename(Alter = X) %>% 
  select(X)
```

Der Fehler hier sagt Ihnen zum Glück genau, was falsch gelaufen ist. Richtig geht es also so (wir benutzen zusätzlich `slice(1:10)`, damit der Output nicht so lang ist):

```{r}
vdata %>% 
  select(X) %>% 
  rename(Alter = X) %>% 
  slice(1:10)
```

Ein weiteres Beispiel. Sie möchten aus dem Data Frame `int` die Dauerwerte erfahren, wenn F1 unter 270 Hz liegt.

```{r, error=TRUE}
int %>% 
  pull(Dauer) %>% 
  filter(F1 < 270)
```

Dieser Fehler ist schon etwas kryptischer. Rekonstruieren wir also, was schief gelaufen ist. Aus dem Data Frame `int` haben wir die Spalte `Dauer` gezogen, die auch existiert. Dafür haben wir aber `pull()` verwendet, und `pull()` gibt Spalten in Form eines Vektors aus. Wir können das nochmal überprüfen wie folgt:

```{r}
int %>% pull(Dauer)
int %>% pull(Dauer) %>% class()
```

Ja, dies ist ein Vektor mit *integers*. Oben haben wir dann versucht, auf diesen numerischen Vektor eine Funktion anzuwenden, die für Data Frames gedacht ist -- daher der Fehler. Die Lösung ist in diesem Fall also, *zuerst* zu filtern, und dann die Werte ausgeben zu lassen:

```{r}
int %>% 
  filter(F1 < 270) %>% 
  pull(Dauer)
```

Dies sind die Dauerwerte für die drei Zeilen, bei denen F1 unter 270 Hz liegt.

Zuletzt wollen wir hier noch ein Beispiel für eine komplexe Pipe mit der Doppelpipe am Anfang zeigen. Was wir also jetzt tun, wird sofort in den Data Frame geschrieben, und nicht einfach in der Konsole ausgegeben. Wir möchten die Spalte `noise` jetzt dauerhaft im Data Frame `int` anlegen, dann alle Zeilen auswählen, wo die Versuchsperson "S1" ist und die Dauer zwischen 100 und 200 ms liegt, und zuletzt die Spalten `noise` und `Dauer` sowie die ersten fünf Zeilen auswählen.

```{r}
int %<>% 
  mutate(noise = case_when(dB < 25 ~ "leise",
                           dB > 25 & dB < 35 ~ "mittel",
                           dB > 35 ~ "laut")) %>% 
  filter(Vpn == "S1" & Dauer > 100 & Dauer < 200) %>% 
  select(Dauer, noise) %>% 
  slice_head(n = 5)
int
```

Der Data Frame `int` besteht jetzt nur noch aus zwei Spalten und fünf Zeilen, und diese Aktion kann auch nicht rückgängig gemacht werden. Seien Sie also vorsichtig und überlegen Sie genau, ob Sie einen Data Frame mit dem Ergebnis einer Pipe überschreiben wollen.

# Packages und Daten laden

Starten Sie das R Projekt, das Sie für diesen Kurs angelegt haben. Öffnen Sie hierfür RStudio und benutzen Sie die Schaltfläche oben rechts oder navigieren Sie zu Ihrem Kursverzeichnis und klicken Sie auf die `.Rproj` Datei. 

Laden Sie dann die folgenden Packages:

```{r}
library(tidyverse)
library(magrittr)
```

Wir werden diese Woche mit verschiedenen Data Frames arbeiten, die wir vom IPS-Server laden. Die URL legen wir als Variable an und benutzen diese dann in der Funktion `file.path()`.

```{r}
url <- "http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf"
asp <- read.table(file.path(url, "asp.txt"))
int <- read.table(file.path(url, "intdauer.txt"))
coronal <- read.table(file.path(url, "coronal.txt"))
vdata <- read.table(file.path(url, "vdata.txt"))
```

# Summary Statistics

Wenn man sich einen Überblick über Daten verschaffen möchte, sind sogenannte *summary statitics* (deskriptive Statistiken) häufig hilfreich. Zu solchen deskriptiven Werten zählen z.B. das arithmetische Mittel (Mittelwert), Median, Varianz, Standardabweichung, Minimum, Maximum, usw. Hier zeigen wir zunächst wie man solche Werte ohne Funktionen aus dem *tidyverse* berechnen kann. Da R eine Statistik-Software ist, sind solche Basis-Funktionen wie zur Berechnung des Mittelwerts immer verfügbar. Im Folgenden demonstrieren wir die wichtigsten Funktionen zur Berechnung von *summary statistics* anhand der F1-Werte in `vdata`:

```{r}
mean(vdata$F1)             # arithmetisches Mittel
median(vdata$F1)           # Median
var(vdata$F1)              # Varianz
sd(vdata$F1)               # Standardabweichung
min(vdata$F1)              # Minimum
max(vdata$F1)              # Maximum
range(vdata$F1)            # Minimum & Maximum
quantile(vdata$F1, 0.25)   # 1. Quartil
quantile(vdata$F1, 0.75)   # 3. Quartil
IQR(vdata$F1)              # interquartiler Bereich
```

## Mittelwert & Median

Das arithmetische Mittel wird berechnet, indem man die Summe einer Anzahl $n$ an Zahlen bildet, und diese Summe dann durch die Anzahl $n$ teilt. Hier ist ein sehr einfaches Beispiel:

```{r}
zahlen <- 1:5
summe <- sum(zahlen)
summe
anzahl <- length(zahlen)
anzahl
# Mittelwert:
summe/anzahl
# zum Vergleich:
mean(zahlen)
```

Der Median hingegen ist die mittig liegende Zahl in einer sortierten Zahlenreihe. Nehmen wir wieder obiges Beispiel (die Zahlen sind bereits aufsteigend sortiert):

```{r}
zahlen
median(zahlen)
```

Bei einer geraden Anzahl von Zahlen berechnet man den Median als den Mittelwert der zwei mittig liegenden Werte, zum Beispiel:

```{r}
zahlen <- 1:6
median(zahlen)
mean(c(3, 4))
```

Der Median ist robuster gegen sogenannte Ausreißer (engl. *outlier*) als der Mittelwert. Ausreißer sind Datenpunkte, die deutlich extremer sind als die Mehrheit der Datenpunkte. Hier wieder ein einfaches Beispiel:

```{r}
zahlen <- c(1:5, 100)
zahlen
mean(zahlen)
median(zahlen)
```

Die Zahl 100 ist ganz offensichtlich ein Ausreißer im Vektor `zahlen`. Der Mittelwert ist wegen dieses Ausreißers jetzt um ein Vielfaches höher als vorher, während der Median sich nur leicht verändert hat.

## Varianz & Standardabweichung

Varianz und Standardabweichung sind verwandte Maße für die Streuung von Werten um ihren Mittelwert. Genauer gesagt ist die Varianz die Summe der quadrierten Abweichungen der Messwerte von ihrem Mittelwert geteilt durch die Anzahl der Messwerte minus 1, während die Standardabweichung die Quadratwurzel der Varianz ist. Am folgenden Beispiel können Sie nachvollziehen, wie man die Varianz und Standardabweichung "händisch" berechnet:

```{r}
zahlen <- c(12, 6, 24, 3, 17)
# Mittelwert
m <- mean(zahlen)
m
# quadrierte Abweichungen
quadr_abw <- (zahlen - m)^2
quadr_abw
# Anzahl der Messwerte
n <- length(zahlen)
n
# Summe der quadrierten Abweichungen
summe <- sum(quadr_abw)
summe
# Varianz:
varianz <- summe / (n - 1)
varianz
# Mit der Funktion var():
var(zahlen)
```

Um jetzt die Standardabweichung daraus zu berechnen, die in der Statistik viel häufiger verwendet wird als die Varianz, brauchen wir nur noch die Quadratwurzel aus der Varianz zu ziehen:

```{r}
std_abw <- sqrt(varianz)
std_abw
# oder mit sd()
sd(zahlen)
```

## Quantile

Ein Quantil teilt Datenpunkte so auf, dass ein bestimmter Teil der Datenpunkte unterhalb des Quantils liegen. Quantil ist ein Überbegriff; je nachdem in wie viele Stücke man die Datenpunkte aufteilt, sagt man auch Perzentil (100 Stücke) oder Quartil (4 Stücke). Der Median ist ebenfalls ein Quantil, denn 50% der Daten liegen immer unter dem Median. In R berechnet die Funktion `quantile()` die Quantile. Die Funktion bekommt zuerst die Datenpunkte (also einen numerischen Vektor) und anschließend die Proportion der Datenpunkte, die unter dem zu berechnenden Wert liegen soll. Wichtige Quantile sind das erste und dritte Quartil, also die Schwellwerte, unter denen ein Viertel bzw. drei Viertel aller Datenpunkte liegen. 

```{r}
quantile(vdata$F1, 0.25)   # 1. Quartil
quantile(vdata$F1, 0.75)   # 3. Quartil
IQR(vdata$F1)              # interquartiler Bereich
```

Die Differenz zwischen dem ersten und dritten Quartil wird auch interquartiler Bereich oder Interquartilsabstand (*interquartile range*) genannt und kann mit der Funktion `IQR()` berechnet werden.

## Beispiel Boxplot

Ein Boxplot enthält viele der deskriptiven Informationen, die wir bis jetzt behandelt haben:

- Median: Der Strich innerhalb der Box ist der Median.
- Box: Die Box umfasst die mittleren 50% aller Datenpunkte. Das untere Ende der Box ist das erste Quartil (Q1), das obere Ende ist das dritte Quartil (Q3). Das heißt die Box ist genauso groß wie der Interquartilsabstand.
- Whiskers: Die Whiskers erstrecken sich vom Q1 und vom Q3 aus zu dem niedrigsten/höchsten Datenpunkt, der innerhalb von `1.5 * IQR` liegt. Diese Berechnung der Länge der Whiskers als `1.5 * IQR` gilt für Boxplots, die mit `ggplot2` erstellt wurden, aber nicht jeder Boxplot wird so berechnet.
- Punkte: Ausreißer, also alle restlichen Datenpunkte, die nicht in der Box und den Whiskers enthalten sind.

Hier sehen Sie den Boxplot für `F1` aus dem Data Frame `vdata`:

![](img/boxplot.png)

Wie man diesen Boxplot erstellt, erfahren Sie später in dieser Vorlesung.

# Daten manipulieren mit `dplyr` (Fortsetzung)

## Grouping & Summarising

Zu Beginn dieser Vorlesung haben wir *summary statistics* für F1-Werte aus dem Data Frame `vdata` berechnet. Natürlich geht das auch innerhalb der *tidyverse*-Syntax, nämlich mit der Funktion `summarise()` aus dem Package `dplyr`. Diese Funktion verändert den Data Frame grundlegend, denn die ursprünglichen Daten werden zu neuen Werten zusammengefasst. Dies betrifft sowohl die Anzahl der Spalten als auch der Anzahl der Zeilen. `summarise()` erstellt neue Spalten und keine der originalen Spalten werden beibehalten. Die Funktion bekommt als Argument also den/die neuen Spaltennamen und wie die Werte in dieser neuen Spalte berechnet werden sollen:

```{r}
vdata %>% summarise(mittelwert = mean(F1))
```

Der Output dieser Pipe ist ein Data Frame mit nur einer Spalte und einer Zeile. Wir können aber auch mehrere deskriptive Werte gleichzeitig berechnen und erhalten dadurch mehr Spalten:

```{r}
vdata %>% summarise(mittelwert = mean(F1),
                    std_abw = sd(F1),
                    summe = sum(F1),
                    maximum = max(F1),
                    Q1 = quantile(F1, 0.25))
```

Die Funktionen `mutate()` und `summarise()` haben also gemein, dass sie neue Spalten erstellen; während in `mutate()` aber alle ursprünglichen Zeilen und Spalten erhalten bleiben, erstellt `summarise()` einen ganz neuen Data Frame mit deutlich weniger Zeilen als ursprünglich vorhanden waren (denn hier wurden Werte *zusammengefasst*).

Was würden Sie jetzt tun, wenn Sie den F1-Mittelwert für nur einen bestimmten Vokal `V` aus dem Data Frame berechnen wollen? Vermutlich würden Sie dies wie folgt lösen (für den Vokal `V == "E"`):

```{r}
vdata %>% 
  filter(V == "E") %>% 
  summarise(mittelwert = mean(F1))
```

Der F1-Mittelwert für "E" ist also ca. 426 Hz. Wenn Sie sich für die vokalspezifischen F1-Mittelwerte interessieren, dann ist es nicht mehr sinnvoll, für jeden einzelnen Vokal den obigen Code zu benutzen. Stattdessen gibt es die Funktion `group_by()`. `group_by()` bekommt als Argumente alle Spalten, nach denen gruppiert werden soll. `summarise()` berechnet die gewünschten *summary statistics* anschließend **pro Gruppe**. In unserem Beispiel gruppieren wir nach Vokal und berechnen dann den Mittelwert pro Vokal:

```{r}
vdata %>% 
  group_by(V) %>% 
  summarise(mittelwert = mean(F1))
```

Es wurden zwei Spalten erstellt: Die eine enthält die sieben verschiedenen Vokale aus dem originalen Data Frame, die andere die vokalspezifischen F1-Mittelwerte. Sie können natürlich auch nach mehr als einer Spalte gruppieren. Es ist zum Beispiel anzunehmen, dass sich der mittlere F1 nicht nur von Vokal zu Vokal unterscheidet, sondern dass auch der Gespanntheitsgrad `Tense` einen Einfluss hat. Deshalb gruppieren wir nach Vokal und Gespanntheitsgrad und berechnen dann den mittleren F1:

```{r}
vdata %>% 
  group_by(V, Tense) %>% 
  summarise(mittelwert = mean(F1))
```

Wir sehen jetzt also den F1-Mittelwert für nicht gespannte "%", gespannte "%" (ignorieren Sie die seltsame Vokal-Kodierung), nicht gespannte "A", gespannte "A", usw.

<div class="gray">
**Weiterführende Infos: `summarise()` warning**

Oben sehen Sie eine Warnmeldung, die von `summarise()` geworfen wurde. Warnmeldungen sind dazu da, Sie auf etwas aufmerksam zu machen -- Sie sollten sie also nicht ignorieren. Diese Warnmeldung zeigt erstmal an, dass das Ergebnis des Codes ein gruppierter Data Frame ist (Objektklasse `grouped_df`) und dass die Gruppierungsvariable `V` ist:

```{r}
vdata %>% 
  group_by(V, Tense) %>% 
  summarise(mittelwert = mean(F1)) %>% 
  class()
```

Die Warnmeldung zeigt außerdem, dass man die Gruppierung des Ergebnisses auch verändern kann, indem man das `summarise()`-Argument `.groups` verwendet. Dieses Argument kann verschiedene Werte annehmen, wie Sie auf der Hilfsseite der Funktion `summarise()` nachlesen können.

Bei den vorherigen Code Snippets, bei denen wir `group_by()` im Zusammenspiel mit `summarise()` verwendet haben, ist die Warnmeldung übrigens deshalb nicht aufgetaucht, weil wir nur nach einer Variable gruppiert haben; im Ergebnis wird diese Gruppierung automatisch aufgehoben.
</div>

Es ist wichtig zu verstehen, dass nur nach kategorialen Spalten gruppiert werden kann. Es ergibt keinen Sinn, nach nicht-kategorialen numerischen Spalten zu gruppieren, denn hier gibt es keine Gruppen (jeder Wert ist vermutlich einzigartig). Der Sinn von `summarise()` ist es aber ja gerade, deskriptive Statistiken für kategoriale Gruppen zu berechnen.

Zuletzt wollen wir noch die Funktionen `n()` und `n_distinct()` vorstellen. `n()` benötigt keine Argumente und wird nach `group_by()` innerhalb `summarise()` verwendet, um die Anzahl an Beobachtungen (Zeilen) pro Gruppe zurückzugeben. `n_distinct()` bekommt als Argument den Namen einer Spalte und findet heraus, wie viele unterschiedliche (einzigartige) Werte einer Variable es pro Gruppe gibt.

```{r}
# Anzahl an Zeilen für jede Kombination von V und Tense
vdata %>% 
  group_by(V, Tense) %>% 
  summarise(count = n())
# Anzahl der einzigartigen Sprecher pro Region und sozialer Klasse
coronal %>% 
  group_by(Region, Socialclass) %>% 
  summarise(count = n_distinct(Vpn))
```

<div class="gray">
**Weiterführende Infos: Funktionen eindeutig beschreiben**

Da die Funktionen aus dem *tidyverse*, insbesonderen aus `dplyr`, sehr gängige Namen haben (`filter()`, `summarise()`, `rename()`), werden sie leicht von Funktionen mit demselben Namen aus anderen Paketen maskiert. Wenn Sie also von einer dieser Funktionen einen Fehler bekommen, laden Sie entweder noch einmal das Paket, aus dem die Funktion stammen soll (z.B. `library(dplyr)`), oder nutzen Sie die folgende Schreibweise: `dplyr::filter()`.

</div>

## Arranging

In der alltäglichen Arbeit mit Data Frames kann es sinnvoll sein, den Data Frame nach Zeilen oder Spalten zu ordnen. Für das Ordnen der Zeilen wird `arrange()` benutzt, für das Ordnen der Spalten `relocate()`. Hier ordnen wir den Data Frame `int` aufsteigend nach Dauer:

```{r}
int %>% arrange(Dauer)
```

`arrange()` kann auch alphabetisch oder nach mehreren Spalten ordnen:

```{r}
int %>% arrange(Vpn, Dauer)
```

Um absteigend zu ordnen, wird `desc()` (*descending*) innerhalb von `arrange()` genutzt:

```{r}
int %>% arrange(Vpn, desc(Dauer))
```

`relocate()` bekommt als Argumente die Namen aller Spalten, die umsortiert werden sollen. Wenn sonst keine weiteren Argumente angegeben werden, werden die Spalten an den Anfang des Data Frames gesetzt. Ansonsten können die Argumente `.before` und `.after` verwendet werden, um anzugeben, vor oder nach welche Spalten die anderen Spalten gesetzt werden sollen:

```{r}
vdata %>% slice(1)
vdata %>% relocate(Subj) %>% slice(1)
vdata %>% relocate(Subj, Cons) %>% slice(1)
vdata %>% relocate(where(is.numeric), .after = Subj) %>% slice(1)
vdata %>% relocate(where(is.character), .before = dur) %>% slice(1)
```

# Daten abbilden mit `ggplot2`

[`ggplot2`](https://ggplot2.tidyverse.org/) ist eine Library aus dem `tidyverse`, die Ihnen sehr viele Möglichkeiten für die Visualisierung von Daten liefert. `gg` steht für *grammar of graphics*. Der Befehl, mit dem Sie eine Abbildung beginnen, ist `ggplot()`; das Hauptargument dieser Funktion ist der gewünschte Data Frame. Dann fügt man das sog. *aesthetic mapping* mittels `aes()`, sowie Funktionen für die Art der Abbildung, die Beschriftungen, die Legende, etc., hinzu. Jede Funktion wird mit `+` verbunden (nicht mit Pipes!).

## Boxplots

Boxplots sind die wohl wichtigsten wissenschaftlich genutzten Abbildungen. In R werden sie mit dem Befehl [`geom_boxplot()`](https://ggplot2.tidyverse.org/reference/geom_boxplot.html) erstellt. Zuerst zeigen wir, wie der oben verwendete Boxplot erstellt wurde. Die Funktion `ggplot()` bekommt den Data Frame `vdata`. In den *aesthetic mappings* `aes()` tragen wir ein, dass F1 auf der y-Achse aufgetragen werden soll. Zuletzt bestimmen wir noch, dass ein Boxplot gezeichnet werden soll.

```{r}
ggplot(vdata) + 
  aes(y = F1) + 
  geom_boxplot()
```

Zugegeben, der Boxplot sieht ein bisschen anders aus als der Boxplot oben. Wie man diesen Plot "verschönert", lernen Sie nächste Woche. Boxplots eignen sich sehr gut zum Vergleichen von Werten für verschiedene kategoriale Gruppen. Dann werden diese Gruppen (üblicherweise) auf der x-Achse aufgetragen und auf der y-Achse wieder die gewünschten Werte. Hier sehen Sie ein Beispiel für die Dauer verschiedener Konsonanten aus dem Data Frame `asp`:

```{r}
ggplot(asp) +
  aes(x = Kons, y = d) +
  geom_boxplot()
```

Boxplots können auch horizontal erstellt werden (wobei das meist weniger übersichtlich ist). Dann werden die Kategorien auf der y-Achse und die Werte auf der x-Achse aufgetragen:

```{r}
ggplot(asp) +
  aes(x = d, y = Kons) +
  geom_boxplot()
```

Manchmal ist ein sogenannter *Notch* gewünscht; dafür nutzen wir das Argument `notch = TRUE` in der Funktion `geom_boxplot()` (und ggf. `notchwidth`, um die Tiefe des Notches anzupassen):

```{r}
ggplot(asp) +
  aes(x = Kons, y = d) +
  geom_boxplot(notch = TRUE)
ggplot(asp) +
  aes(x = Kons, y = d) +
  geom_boxplot(notch = TRUE, notchwidth = 0.3)
```

<div class="gray">
**Weiterführende Infos: Aesthetic mappings & Piping Data Frames**

Streng genommen sind die *aesthetic mappings* immer ein Argument der Funktion, die über die Art des Plots bestimmt (also z.B. `geom_boxplot()`). Später werden Sie feststellen, dass manche Plots bestimmte *aesthetic mappings* benötigen bzw. zulassen, die andere Plots nicht verarbeiten können. Wir lagern die *aesthetic mappings* in den allermeisten Fällen aus der Plot-Funktion aus, weil das übersichtlicher ist. Es steht Ihnen aber frei, die *aesthetic mappings* in die Funktion als Argument reinzuschreiben:

```{r}
ggplot(asp) +
  geom_boxplot(aes(x = Kons, y = d), 
               notch = TRUE, 
               notchwidth = 0.3)
```

Innerhalb eines `ggplot` werden die einzelnen Funktion immer und ausschließlich mit einem Pluszeichen verbunden. Der Data Frame allerdings kann mit einer einfachen Pipe an `ggplot()`übergeben werden:

```{r}
asp %>%
ggplot() +
  aes(x = Kons, y = d) +
  geom_boxplot()
```

Das ist besonders hilfreich, wenn Sie vor dem Plotten erst noch weitere Funktionen auf den Data Frame anwenden wollen, bevor sie die daraus entstehenden Daten plotten. Hier filtern wir zum Beispiel zuerst nach Betonung, bevor wir anschließend nur noch die Dauer der betonten Wörter plotten:

```{r}
asp %>%
  filter(Bet == "be") %>% 
  ggplot() + 
  aes(x = Kons, y = d) +
  geom_boxplot()
```

</div>

## Scatter- & Lineplots

Scatterplots werden mit den Funktionen [`geom_point()`](https://ggplot2.tidyverse.org/reference/geom_point.html) und/oder [`geom_line()`](https://ggplot2.tidyverse.org/reference/geom_path.html) erstellt. Man kann auch beide Funktionen gleichzeitig verwenden. Auf die x- und y-Achse werden üblicherweise nur numerisch-kontinuierliche Daten aufgetragen. Im Folgenden plotten wir zum Beispiel Lautstärke in Dezibel gegen Dauer.

```{r}
# Punkte:
ggplot(int) +
  aes(x = Dauer, y = dB) +
  geom_point() 

# Linie:
ggplot(int) +  
  aes(x = Dauer, y = dB) + 
  geom_line()

# Beides:
ggplot(int) +
  aes(x = Dauer, y = dB) + 
  geom_line() + 
  geom_point()
```

Manchmal ist es hilfreich, vertikale oder horizontale [Referenzlinien](https://ggplot2.tidyverse.org/reference/geom_abline.html) in einem Plot einzuzeichnen. Horizontale Linien werden mit `geom_hline()` erzeugt, vertikale gerade Linien mit `geom_vline()`. Um eine horizontale Linie zu zeichnen, muss bekannt sein, an welcher Stelle die Linie die y-Achse schneidet. Deshalb bekommt `geom_hline()` immer das Argument `yintercept`. Bei `geom_vline()` muss mit `xintercept` die Schnittstelle der vertikalen Linie mit der x-Achse eingetragen werden. Wir fügen zum obigen Scatterplot zwei gerade Linien hinzu:

```{r}
ggplot(int) +
  aes(x = Dauer, y = dB) +
  geom_point() + 
  geom_vline(xintercept = 150) + 
  geom_hline(yintercept = 35)
```

## Barplots

Eine weitere wichtige Abbildungsform sind Barplots, die mit [`geom_bar()`](https://ggplot2.tidyverse.org/reference/geom_bar.html) erzeugt werden. Dabei darf nur entweder `x` oder `y` in den *aesthetic mappings* verwendet werden. Das liegt daran, dass auf die jeweils andere Achse grundsätzlich ein *count* oder eine Proportion aufgetragen wird, die von `ggplot` berechnet wird. Der folgende Plot zeigt zum Beispiel, wie viele Vorkommnisse dreier Regionen im Data Frame `coronal` zu finden sind.

```{r}
ggplot(coronal) +
  aes(x = Region) +
  geom_bar()
```

Die Balken können wir auch horizontal plotten, indem wir in den *aesthetic mappings* `y` statt `x` angeben:

```{r}
ggplot(coronal) +
  aes(y = Region) +
  geom_bar()
```

Die Werte der Balken können Sie ganz einfach nachvollziehen, indem Sie sich die Anzahl der Vorkommnisse der drei Regionen mittels der Funktion `table()` anzeigen lassen:

```{r}
table(coronal$Region)
```

Beim Barplot können Sie aber wie z.B. beim Boxplot noch eine weitere (kategoriale) Variable plotten. Die zweite Variable, die abgebildet werden soll, wird mit dem Argument `fill` angegeben, das die Levels der Variable als Füllfarben darstellt. Sie werden nächste Woche u.a. lernen, wie man Farben selbst bestimmen kann. Im Folgenden sieht man, wie häufig die Frikative `Fr` "s" (rot) und "sh" (blau) jeweils in den drei Regionen produziert wurden.

```{r}
ggplot(coronal) +
  aes(x = Region, fill = Fr) +
  geom_bar()
```

Lassen Sie uns mittels der zuvor gelernten Funktionen für Grouping und Summarising die Werte in diesem Plot nachvollziehen. Dafür gruppieren wir nach Region und Frikativ und lassen uns dann mit `n()` innerhalb von `summarise()` die Anzahl der Zeilen im Data Frame pro Gruppenkombination bestimmen.

```{r}
coronal %>% 
  group_by(Region, Fr) %>% 
  summarise(count = n())
```

Die Funktion `geom_bar()` kann als Argument noch `position` bekommen...

```{r}
# ...um Proportionen anstatt einer absoluten Anzahl darzustellen:
ggplot(coronal) +
  aes(x = Region, fill = Fr) +
  geom_bar(position = "fill")

# ...um die Balken nebeneinander zu stellen:
ggplot(coronal) +
  aes(x = Region, fill = Fr) +
  geom_bar(position = "dodge")
```

## Histogramme & Wahrscheinlichkeitsdichte

Histogramme zeigen die Verteilung von numerisch-kontinuierlichen Datenpunkten, indem sie den Wertebereich in mehrere kleine Bereiche einteilt. Ähnlich wie beim Barplot zeigen dann Balken (*bins*) an, wie viele Werte in einem bestimmten Wertebereich liegen. In `ggplot` werden Histogramme mit [`geom_histogram()`](https://ggplot2.tidyverse.org/reference/geom_histogram.html) erstellt. In den *aesthetic mappings* legen wir mit dem Argument `x` fest, welche Daten wir anschauen wollen, zum Beipspiel die F1-Verteilung:

```{r}
ggplot(vdata) + 
  aes(x = F1) + 
  geom_histogram()
```

Um die einzelnen Balken besser voneinander unterscheiden zu können, lassen wir die Balken weiß umranden, indem wir `geom_histogram()` das Argument `color = "white"` übergeben:

```{r}
ggplot(vdata) + 
  aes(x = F1) + 
  geom_histogram(color = "white")
```

Wir können auch selbst bestimmen, wie breit die Balken sein sollen, nämlich mit `binwidth`. Im Moment umfasst ein Balken ca. 40 Hz. Die folgenden Abbildungen zeigen die exakt selben Daten, aber mit Balken von 10 Hz und Balken von 100 Hz:

```{r}
ggplot(vdata) + 
  aes(x = F1) + 
  geom_histogram(color = "white",
                 binwidth = 10)

ggplot(vdata) + 
  aes(x = F1) + 
  geom_histogram(color = "white",
                 binwidth = 100)
```

Sie sehen, dass dies für die Repräsentation der Daten einen großen Unterschied macht -- gehen Sie also immer mit Bedacht vor, wenn Sie die *binwidth* von Histogrammen verändern.

Mit dem Histogramm verwandt ist die Wahrscheinlichkeitsdichte (engl. *probability density*). Die einzige Änderung, die wir dafür vornehmen müssen, ist `aes()` das Argument `y = ..density..` zu übergeben. Dies verändert die y-Achse so, dass statt der Anzahl an Datenpunkten die Wahrscheinlichkeitsdichte der Datenpunkte angezeigt wird. *Per definitionem* ist die Fläche unter den Balken der Wahrscheinlichkeitsdichte insgesamt 1.

```{r}
ggplot(vdata) + 
  aes(x = F1, y = ..density..) + 
  geom_histogram(color = "white",
                 binwidth = 100)
```

Die Wahrscheinlichkeitsdichte wird berechnet als `count / (n * binwidth)`, wo `n` die Anzahl aller Datenpunkte ist. In dem Histogramm oben (mit `binwidth = 100`) liegen zum Beispiel 285 Datenpunkte (*count*) im Wertebereich zwischen 150 Hz und 250 Hz. Die Wahrscheinlichkeitsdichte für diesen Balken wird also wie folgt berechnet:

```{r}
count <- 285
n <- nrow(vdata)
binwidth <- 100
dens <- count / (n * binwidth)
dens
```

Dieser Wert stimmt mit dem *density*-Wert überein, den wir in der Wahrscheinlichkeitsdichteverteilung für denselben Balken sehen.

Die Fläche dieses Balkens in der Wahrscheinlichkeitsdichteverteilung wird berechnet als `binwidth * binheight`: 

```{r}
area <- binwidth * dens
area
```

Wenn man die Fläche aller Balken berechnet und summiert, ist die Gesamtfläche 1.

Stellen Sie sich nun ein Wahrscheinlichkeitsdichte-Histogramm vor, das aus unendlich vielen Balken besteht (die dementsprechend unendlich schmal sein müssen). Sie erhalten nicht mehr einzelne Balken sondern eine kontinuierliche Funktion, die sich Wahrscheinlichkeitsdichteverteilung (*probability density function*) nennt. Auch dafür kennt `ggplot2` eine Funktion: [`geom_density()`](https://ggplot2.tidyverse.org/reference/geom_density.html).

```{r}
ggplot(vdata) + 
  aes(x = F1) + 
  geom_density()
```

Hier gilt genau wie bei dem Histogramm mit der Wahrscheinlichkeitsdichte, dass das Integral (die Fläche) unter der Kurve 1 ist. Weshalb das wichtig ist, erfahren Sie übernächste Woche.

<div class="gray">
**Weiterführende Infos: Histogramme und Probability Density**

Für weitere Informationen schauen Sie sich gerne Wilke's *Fundamentals of Data Visualization in R*, [Kapitel 7](https://serialmentor.com/dataviz/histograms-density-plots.html) an.
</div>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
```

<style>
div.green {background-color: #ecffeb; border-radius: 5px; padding: 20px;}
div.gray {background-color: #e8e8e8; border-radius: 5px; padding: 20px;}
</style>

# Packages und Daten laden

Starten Sie das R Projekt, das Sie für diesen Kurs angelegt haben. Öffnen Sie hierfür RStudio und benutzen Sie die Schaltfläche oben rechts oder navigieren Sie zu Ihrem Kursverzeichnis und klicken Sie auf die `.Rproj` Datei. 

Laden Sie dann die folgenden Packages:

```{r}
library(tidyverse)
library(magrittr)
library(gridExtra)
```

Wir werden diese Woche wieder mit verschiedenen Data Frames arbeiten, die wir vom IPS-Server laden. Die URL legen wir als Variable an und benutzen diese dann in der Funktion `file.path()`.

```{r}
url <- "http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf"
asp <- read.table(file.path(url, "asp.txt"))
coronal <- read.table(file.path(url, "coronal.txt"))
```

# Daten aufräumen mit `tidyr`

"*tidy datasets are all alike but every messy dataset is messy in its own way*" -- Hadley Wickham

Hadley Wickham ist der Chef-Entwickler des *tidyverse*. Die Funktionen des *tidyverse* sind nicht nur auf Datenmanipulation und -verarbeitung ausgerichtet, sondern auch auf das Aufräumen von Datensätzen. 

Ein aufgeräumter Datensatz folgt drei Prinzipien:

* Jede Spalte enthält eine Variable
* Jede Zeile enthält eine Beobachtung
* Jede Zelle enthält einen Wert

Diese Grundsätze scheinen zunächst offensichtlich, werden Ihnen aber in der täglichen Arbeit mit Daten in R immer wieder begegnen. Die drei Prinzipien sollten Sie vor allem aus zwei Gründen befolgen: Erstens ist jeder Datensatz so auf dieselbe konsistente Weise strukturiert, was Ihnen jede Analyse erleichtern wird. Zweitens sind die Funktionen in R, aber insbesondere die Funktionen des *tidyverse* auf die Arbeit mit Spalten ausgelegt. Das heißt, Sie wenden Funktionen häufig auf einzelne Spalten an. Deshalb ist es sinnvoll, wenn jede Spalte eine Variable enthält. Um dies zu verdeutlichen, zeigen wir Ihnen im Folgenden Beispiele für die zwei häufigsten Arten von *messy data*.

- *Spaltennamen sind nicht Variablen, sondern Werte*: Die Spalten `schnell` und `langsam` sind eigentlich Werte der Variable `Tempo`.

```{r}
avokal <- read.table(file.path(url, "avokal.txt"))
avokal
```

- *Mehrere Variablen sind in einer Spalte gespeichert*: In der Spalte `Context` sind zwei Informationen abgespeichert: der linke und der rechte phonetische Kontext. Besser ist die Aufteilung in zwei Spalten, die sogar auch schon im Data Frame existieren (`Left` und `Right`).

```{r}
vcv <- read.table(file.path(url, "vcvC.txt"))
vcv %>% head()
```

Es ist nicht trivial, einen Data Frame sauber zu strukturieren. Nur ein Beispiel: Sie haben die ersten vier Formanten in Vokalen gemessen. Ist es sinnvoller, die erhobenen Daten in vier Spalten `F1`, `F2`, `F3`, `F4` festzuhalten? Oder besser in zwei Spalten `Hz` (mit den Formantwerten in Hertz) und `Formant` (mit den Werten 1, 2, 3 oder 4)?

Bevor wir Ihnen zeigen, wie Sie die obigen Datensätze so umformen können, dass sie den drei Prinzipien für aufgeräumte Daten folgen, möchten wir Ihnen noch die **tibble** vorstellen.

## Tibbles

Die *tibble* ist eine vereinfachte Form eines Data Frames, die im *tidyverse* häufig verwendet wird. Wir laden hier einen weiteren Data Frame und formen ihn danach mittels `as_tibble()` um:

```{r}
vdata <- read.table(file.path(url, "vdata.txt")) %>% 
  as_tibble()
```

Wenn wir den Namen der *tibble* jetzt in die Konsole eingeben, wird nicht wie beim Data Frame üblich der gesamte Datensatz angezeigt, sondern nur die ersten zehn Zeilen. Wir sehen außerdem, aus wie vielen Zeilen und Spalten die *tibble* besteht und welcher Objektklasse die einzelnen Spalten angehören:

```{r}
vdata
```

Die *tibble* hat vorrangig die Klasse `tbl_df`, aber zusätzlich noch `tbl` und `data.frame`. Wir werden deshalb im Folgenden trotzdem noch von Data Frames sprechen, wenn wir genau genommen eine *tibble* meinen.

```{r}
vdata %>% class()
```

Natürlich können Sie eine *tibble* auch selbst erstellen, indem Sie die Funktion `tibble()` anstelle der Funktion `data.frame()` verwenden, zum Beispiel so:

```{r}
tibble(x = 1:5, y = 6:10)
```

Wenn Sie die Import-Funktion `read_delim()` aus dem *tidyverse*-Package `readr` anstelle der `read.table()` Funktion verwenden, wird der Datensatz automatisch als *tibble* geladen:

```{r}
int <- read_delim(file.path(url, "intdauer.txt"), 
                  delim = " ", 
                  col_names = c("idx", "Vpn", "dB", "Dauer"), 
                  skip = 1)
int
```

`read_delim()` gibt Ihnen auch die Objektklasse für jede Spalte in der *Column specification* aus. Die Import-Funktionen aus `readr` sind etwas empfindlicher als die Standard-Funktionen in R. Hier mussten wir zum Beispiel einige Argumente spezifieren (`delim`, `col_names` und `skip`), um damit umzugehen, dass der Data Frame Zeilenindizes enthält. Die Standard-Funktion `read.table()` kommt hingegen meist damit klar, dass man nur den Pfad zum Datensatz angibt.

<div class="gray">
**Weiterführende Infos: Tibbles und `readr`**

Wenn Sie mehr über die *tibble* erfahren möchten, empfehlen wir [Kapitel 10 aus *R for Data Science*](https://r4ds.had.co.nz/tibbles.html).

Das Paket `readr` stellt noch weitere Funktionen zum Laden und Abspeichern von Datensätzen bereit, je nachdem wie die Spalten getrennt werden:

- `read_csv()`: **c**omma **s**eparated **v**alues, d.h. durch Komma getrennte Spalten
- `read_csv2()`: durch Semikolon getrennte Spalten
- `read_tsv()`: durch Tab getrennte Spalten
- `read_delim()`: für alle Trennzeichen geeignet

All dies und mehr finden Sie auch in [Kapitel 11 aus *R for Data Science*](https://r4ds.had.co.nz/data-import.html).

</div>

## Pivoting

Wenn wir unsere Daten geladen haben, fangen wir mit dem Aufräumen an. Das Ziel ist entweder, dass die Daten so strukturiert sind, dass jede Zeile eine neue Beobachtung enthält und dass die Spalten jeweils eine Variable enthalten. Das Ziel kann auch sein, dass die Daten so strukturiert sind, dass sie unserem Zweck dienen (z.B. dem Erstellen einer Abbildung). `tidyr` nennt diesen Prozess **pivoting** und es gibt eine gut erklärte Vignette zu dem Thema:

```{r, eval = F}
vignette("pivot")
```

Oben haben wir den Data Frame `avokal` geladen und haben festgestellt, dass die Spalten `schnell` und `langsam` eigentlich zwei Ausprägungen der Variable `Tempo` sind. Das heißt, es wäre besser, eine Spalte namens `Tempo` (Werte: "schnell", "langsam") und eine namens `Dauer` (Werte aus `schnell` und `langsam` in Millisekunden) zu haben:

```{r}
avokal %>% 
  pivot_longer(cols = c(schnell, langsam), 
               names_to = "Tempo", 
               values_to = "Dauer")
```

Der Befehl **`pivot_longer()`** wandelt den Data Frame in das so genannte "lange Format" um. Am wichtigsten sind die folgenden drei Argumente:

- `cols`: alle Spalten, die umwandelt werden sollen
- `values_to`: der Name der neuen Spalte mit den Werten
- `names_to`: der Name der neuen Spalte mit den ehemaligen Spaltennamen

Die Pivoting-Funktionen von `tidyr` sind sehr mächtig und können auch deutlich kompliziertere Operationen durchführen. Nehmen wir den [Data Frame `billboard`](https://tidyr.tidyverse.org/reference/billboard.html), der automatisch mit dem `tidyverse` geladen wird und die Rankings der Billboard Charts aus dem Jahr 2000 enthält:

```{r}
billboard
```

Auch hier ist es der Fall, dass die Spalten `wk1`, `wk2`, `wk3` usw. eigentlich Levels oder Ausprägungen der Variable `week` sind. Wir möchten also eine Spalte erstellen, die uns die Woche angibt, und eine Spalte, die den Rank in den Billboard Charts enthält. Um dies zu erreichen, nehmen wir alle Spalten, die mit "wk" beginnen, packen die Spaltennamen in eine neue Spalte namens `week` und die Werte aus den Spalten in die neue Spalte `rank`. Das Präfix "wk" aus den alten Spaltennamen können wir durch das Argument `names_prefix` ablegen. Und zuletzt löschen wir alle NA-Werte (NA steht für *not available*) -- es gibt z.B. keine Zeile für Woche 8 für 2Pac's "Baby Don't Cry", weil der Song in der achten Kalenderwoche nicht in den Top 100 gerankt wurde.

```{r}
billboard %>% 
  pivot_longer(cols = starts_with("wk"), 
               names_to = "week", 
               values_to = "rank",
               names_prefix = "wk",
               values_drop_na = TRUE)
```

Es gibt noch ein Gegenstück zu `pivot_longer()`, nämlich **`pivot_wider()`**. Diese Funktion wird deutlich seltener gebraucht, und bekommt vor allem die Argumente:

- `names_from`: die Spalte, deren einzigartige Werte als neue Spalten benutzt werden sollen
- `values_from`: die Spalte, deren Werte die neuen Spalten füllen sollen

Ein Beispiel dafür kann aus dem [Data Frame `us_rent_income`](https://tidyr.tidyverse.org/reference/us_rent_income.html) abgeleitet werden, der ebenfalls mit dem *tidyverse* geladen wird:

```{r}
us_rent_income
```

Wir möchten eine Spalte `income` und eine Spalte `rent` aus den Levels der Spalte `variable` erstellen und die neuen Spalten dann mit den Werten aus `estimate` füllen.

```{r}
us_rent_income %>% 
  pivot_wider(names_from = variable,
              values_from = estimate)
```

Im Ergebnis sehen wir jetzt aber einige `NA`-Werte. Diese können wir zum Beispiel durch Nullen ersetzen, indem wir zusätzlich das Argument `values_fill` verwenden.

```{r}
us_rent_income %>% 
  pivot_wider(names_from = variable,
              values_from = estimate,
              values_fill = 0)
```

Auch hier gibt es wieder komplexere Operationen, die `pivot_wider()` durchführen kann. Die beiden Hauptargumente `names_from` und `values_from` können jeweils auch mehr als eine Spalte angegeben bekommen. `pivot_wider()` erstellt dann so viele neue Spalten, wie es Kombinationen zwischen den Levels aus den originalen Spalten bzw. Spaltennamen gibt. Hier geben wir zum Beispiel die Spalten `estimate` und `moe` bei `values_from` an und erhalten so in Kombination mit den zwei Levels aus `variable` vier neue Spalten:

```{r}
us_rent_income %>% 
  pivot_wider(names_from = variable,
              values_from = c(estimate, moe))
```

Zuletzt noch ein phonetisches Beispiel: Wir möchten aus den Levels der Spalte `Bet` (Betonung) des Data Frames `asp` neue Spalten machen und diese mit den Dauerwerten `d` füllen. Der Code wirft aber eine Warnmeldung, weil es mehrere Werte pro Zelle gibt in den neuen Spalten gibt, wie Sie an der seltsamen Schreibweise sehen können:

```{r}
asp %>%
  pivot_wider(names_from = Bet,
              values_from = d)
```

Netterweise gibt uns die Warnmeldung auch sofort drei Lösungswege an: Wir können das Argument `values_fn` benutzen, um die Warnung zu unterdrücken, uns anzeigen zu lassen wie viele Werte pro Zelle es gibt, oder mit einer zusammenfassenden Funktion die Werte umformen. Letzteres scheint hier sinnvoll zu sein: Dort, wo es mehrere Werte pro Zelle gibt, lassen wir mit `mean()` den Durchschnitt berechnen:

```{r}
asp %>%
  pivot_wider(names_from = Bet,
              values_from = d,
              values_fn = mean)
```

In keinem unserer *pivoting*-Beispiele haben wir die Data Frames in ihrer veränderten Form überschrieben (z.B. mittels der Doppelpipe). Die Funktionen `pivot_longer()` und `pivot_wider()` kann man auch für temporäre Änderungen benutzen, z.B. wenn man den Data Frame in einer bestimmten Form nur für eine Abbildung braucht:

```{r}
avokal %>% 
  pivot_longer(cols = c(schnell, langsam), names_to = "Tempo", values_to = "Dauer") %>% 
  ggplot() +
  aes(x = Tempo, y = Dauer) + 
  geom_boxplot()
```

## Separating

Unser zweites Beispiel für *messy data* ist der Data Frame `vcv`, bei dem zwei Informationen in der Spalte `Context` vorhanden sind:

```{r}
vcv %>% head
```

Wir möchten den linken und rechten Kontext, die hier durch einen Punkt getrennt werden, in eigenen Spalten haben (wir entfernen zu Demonstrationszwecken die Spalten `Left` und `Right`, denn die enthalten schon die gewünschte Lösung).

```{r}
vcv %<>% 
  select(RT:Lang, Context) %>% 
  as_tibble()
```

Um unser Ziel zu erreichen, nutzen wir die Funktion **`separate()`**, die obligatorisch folgende Argumente bekommt:

- `col`: die Spalte, deren Inhalt aufgeteilt werden soll
- `into`: die neuen Spaltennamen
- `sep`: wie die *strings* in der gewählten Spalte getrennt werden sollen

Die ersten zwei Argumente sind eigentlich klar: `col` ist die Spalte `Context`, und bei `into` geben wir die zwei gewünschten Spaltennamen `Left` und `Right` an. Für `sep` gibt es zwei Optionen: Die erste Möglichkeit ist mittels einer Zahl, die den Index angibt, ab dem getrennt werden soll: wenn z.B. die ersten zwei Buchstaben immer in die eine und der Rest in die zweite Spalte geschrieben werden soll, nutzen wir `sep = 2`. Wenn wir uns aber die unterschiedlichen Werte in `Context` anschauen, ist das hier nicht sinnvoll:

```{r}
levels(vcv$Context)
```

Der linke Kontext kann aus einem oder zwei Buchstaben bestehen; außerdem ist da noch ein Punkt, der dann entweder in den linken oder rechten Kontext übernommen werden würde, wie man hier sieht:

```{r}
vcv %>% 
  separate(col = Context, 
           into = c("Left", "Right"), 
           sep = 1)
vcv %>% 
  separate(col = Context, 
           into = c("Left", "Right"), 
           sep = 2)
```

Die andere Möglichkeit ist eine **regular expression** (auch: regulärer Ausdruck oder RegEx). Dafür gibt man ein Muster (*pattern*) an, nach dem die Trennung in zwei Spalten erfolgen soll. Das würde für unser Beispiel gut passen, weil wir immer am Punkt trennen wollen. Der Punkt steht aber leider bei den regulären Ausdrücken für ein (und zwar egal welches) Schriftzeichen. Wenn wir den Punkt als Punkt verwenden wollen müssen wir ihn durch das Escape-Zeichen, den doppelten Backslash, schützen.

```{r}
vcv %>% 
  separate(col = Context, 
           into = c("Left", "Right"), 
           sep = "\\.")
```

So erreichen wir also das Ziel, dass jede Spalte nur eine Variable enthält!

<div class="gray">
**Weiterführende Infos: reguläre Ausdrücke**

RegExps sind ein komplexes Thema, in das wir hier nicht tiefer einsteigen werden. Wenn Sie sich einlesen wollen, empfehlen wir [Kapitel 14 aus *R for Data Science*](https://r4ds.had.co.nz/strings.html#matching-patterns-with-regular-expressions).
</div>

# Joining mit `dplyr`

Vielleicht kennen Sie von der Arbeit mit relationalen Daten(banken) und/oder SQL den sogenannten *join*. Das Prinzip relationaler Daten beruht darauf, alle Informationen und Messwerte, die man gesammelt und erhoben hat, in thematisch sinnvoll aufgeteilten Tabellen abzuspeichern. So könnte man nach einem Experiment zum Beispiel eine Tabelle mit den Messwerten (Formantwerte, Grundfrequenz, etc.) und eine Tabelle mit den Metadaten (Alter, Herkunft, Bildungsstand, etc.) zu den Sprechern erstellen. Zusätzlich bietet es sich vielleicht an, eine Tabelle mit Informationen über das erhobene Material (Wörter, Betonung, Phoneme, Stimuli, etc.) und über die getesteten Bedingungen (Sprechtempo, welcher Gesprächspartner, etc.) zu haben. Wenn nötig, kann man jederzeit (und ggf. nur temporär) zwei Tabellen über einen **key**, d.h. eine Identifikationsspalte, miteinander verbinden. Bei `dplyr` unterscheiden wir zwischen *mutating joins* und *filtering joins*.

## Mutating Joins

Wie auch bei `mutate()` werden einem Data Frame `x` durch einen *mutating join* weitere Spalten hinzugefügt. Im Gegensatz zu `mutate()` stammen die neuen Spalten aber aus einem anderen Data Frame `y`. In beiden Data Frames `x` und `y` muss es mindestens eine Spalte geben, die einen *identifier* oder *key* enthält, über den die Tabellen verbunden werden können.

### Inner Join

Die einfachste Form des *mutating join* ist der sogenannte *inner join*, der mittels der Funktion `inner_join()` durchgeführt wird. Die Funktion bekommt als Argumente zwei Data Frames `x` und `y` und den Identifier im Argument `by`. Das Ergebnis eines *inner join* enthält alle Spalten von `x` und `y` sowie alle Zeilen, für die es einen Match in beiden Data Frames gibt. Fehlende Werte (`NA`) tauchen **nicht** im Ergebnis eines *inner join* auf, daher ist hier immer Vorsicht angebracht.

Als einfaches Beispiel verwenden wir hier eine Tabelle mit Grundfrequenzwerten von zehn Sprechern und eine Tabelle mit Metadaten über die zehn Sprecher:

```{r}
measures <- tibble(subject = rep(paste0("s", 1:10), each = 10),
                   F0 = rnorm(100, 120, 15))
measures
meta <- tibble(subject = paste0("s", 1:10),
               age = rep(c("old", "young"), each = 5))
meta
```

Beide *tibbles* haben die Spalte `subject`, die wir als *key* bei unserem *inner join* benutzen werden:

```{r}
inner_join(x = measures, y = meta, by = "subject")
```

Es kann vorkommen, dass die *key*-Spalte in den zwei Data Frames unterschiedlich benannt ist. In diesem Fall sagen wir der Funktion mittels `by = c("a"="b")`, dass die Spalte `a` aus dem Data Frame `x` mit der Spalte `b` aus dem Data Frame `y` ge*matched* werden soll (das gilt für alle *join* Funktionen):

```{r}
measures %<>% rename(Vpn = subject)
inner_join(x = measures, y = meta, by = c("Vpn"="subject"))
measures %<>% rename(subject = Vpn)
```

In diesem Beispiel werden bislang immer alle Zeilen von `measures` zurückgegeben und alle Spalten beider Data Frames. Das liegt daran, dass es sowohl in `measures` als auch in `meta` Informationen zu denselben zehn Probanden gibt. Wenn wir in einem der Data Frames mehr oder weniger Informationen haben, werden plötzlich Zeilen weggelassen.

```{r}
# Messwerte von 20 statt 10 Sprechern:
measures <- tibble(subject = rep(paste0("s", 1:20), each = 10),
                   F0 = rnorm(200, 120, 15))
inner_join(x = measures, y = meta, by = "subject")
```

In diesem Ergebnis gibt es keine Zeilen für die Sprecher 11 bis 20, weil es zu diesen Sprechern keine Informationen im Data Frame `meta` gibt!

### Outer Join

Im Unterschied zum *inner join* werden beim *outer join* auch Zeilen mit fehlenden Werten beibehalten. Diese fehlenden Werte werden als `NA` gekennzeichnet. Die einfachste Version eines *outer join* ist der *full join*, bei dem alle Zeilen und alle Spalten aus beiden Data Frames beibehalten werden. Die entsprechende Funktion dazu heißt `full_join()` (wir frisieren das Ergebnis hier mit `slice()`, um einen interessanten Teil des Ergebnisses hervorzuheben):

```{r}
full_join(x = measures, y = meta, by = "subject") %>% 
  slice(95:105)
```

Dieses Ergebnis besteht aus 200 Zeilen (wenn wir nicht `slice()` darauf anwenden) -- als wir oben die Funktion `inner_join()` auf die exakt selben *tibbles* ausgeführt haben, hatte das Ergebnis nur 100 Zeilen. Das liegt daran, dass `full_join()` die 100 Zeilen mit den Messwerten der Sprecher 11 bis 20 beibehalten hat, während `inner_join()` diese Zeilen gelöscht hat, weil es für diese Sprecher keine Informationen aus `meta` zu holen gab. Beim Ergebnis des `full_join()` finden sich deshalb `NA`-Werte in der angehängten Spalte `age` für die Sprecher 11 bis 20.

Wenn Sie nicht alle Zeilen aus beiden Data Frames übernehmen wollen, stehen Ihnen die Funktionen `left_join()` und `right_join()` zur Verfügung. Beim `left_join()` werden alle Zeilen aus Data Frame `x` und beim `right_join()` alle Zeilen aus dem Data Frame `y` übernommen. In unserem Beispiel wollen wir alle Zeilen aus `measures` erhalten und nur die Altersinformation aus `meta` hinzufügen:

```{r}
left_join(x = measures, y = meta, by = "subject")
right_join(x = meta, y = measures, by = "subject")
```

## Filtering Joins

Die zweite Art von *joins* in R sind die sogenannten *filtering joins*, die keine neuen Spalten hinzufügen, sondern nur ausgewählte Zeilen zurückgeben. Hierfür gibt es im *tidyverse* zwei Funktionen:

- `semi_join()`: Gibt alle Beobachtungen des Data Frames `x` zurück, für die es einen Match im Data Frame `y` gibt
- `anti_join()`: Gibt alle Beobachtungen des Data Frames `x` zurück, für die es keinen Match im Data Frame `y` gibt

Wir werden diese beiden Funktionen anhand der folgenden Data Frames demonstrieren:

```{r}
vcv %<>% as_tibble()
vcv
vcv_summary <- vcv %>% 
  group_by(Subject, Vowel) %>% 
  summarise(mean_rt = mean(RT)) %>% 
  ungroup() %>%
  slice_max(mean_rt, n = 5)
vcv_summary
```

Der Data Frame `vcv_summary` enthält also die fünf höchsten durchschnittlichen Reaktionszeiten und welchem Sprecher und Vokal diese Werte zugeordnet sind. Wenn wir jetzt herausfinden wollen, aus welchen Beobachtungen in `vcv` diese Mittelwerte berechnet wurden, nutzen wir den *semi join*. Genau genommen möchten wir alle Zeilen aus `vcv` zurückbekommen, für die es in `vcv_summary` einen Match bezüglich der Spalten `Subject` und `Vowel` gibt.

```{r}
semi_join(x = vcv, y = vcv_summary, by = c("Subject", "Vowel"))
```

Das Ergebnis enthält jetzt also alle Beobachtungen, aus denen die gemittelten Reaktionszeiten in `vcv_summary` berechnet wurden. Wir können das nochmal verdeutlichen, indem wir uns ausgeben lassen, welche einzigartigen Kombinationen von `Subject` und `Vowel` es im Ergebnis des *semi joins* gibt (es sind dieselben fünf Kombinationen wie in `vcv_summary`):

```{r}
semi_join(x = vcv, y = vcv_summary, by = c("Subject", "Vowel")) %>% 
  select(Subject, Vowel) %>% 
  unique()
```

Mit dem *anti join* erhalten wir hingegen alle Beobachtungen, aus denen *nicht* die gemittelten Reaktionszeiten berechnet wurden, oder mit anderen Worten: alle Beobachtungen aus `vcv`, für die es bzgl. `Subject` und `Vowel` keinen Match in `vcv_summary` gibt.

```{r}
anti_join(x = vcv, y = vcv_summary, by = c("Subject", "Vowel"))
```

*Anti joins* eignen sich unter anderem zur Fehlersuche in den eigenen Daten. Nehmen wir nochmal unsere Beispieldaten von vorhin, `measures` mit Messwerten für 20 Sprecher, und `meta` mit Metadaten für nur 10 dieser Sprecher. Wenn wir den *anti join* hier anwenden, finden wir sofort heraus, für welche Zeilen in `measures` es keinen Match in `meta` gibt, für welche Sprecher es also keine Metadaten gibt.

```{r}
anti_join(x = measures, y = meta, by = "subject")
```

Das Ergebnis ist das Gegenstück zum *inner join* von oben: Wir erhalten hier die 100 Beobachtungen für die Sprecher 11 bis 20, für die es *keine* Metadaten gibt.

<div class="gray">
**Weiterführende Infos: `dplyr`**

Die *joins* waren die letzten Funktionen aus dem *tidyverse*, die wir Ihnen hier vorstellen wollen. Eine Übersicht über alle bisher gelernten und weitere Funktionen erhalten Sie in diesem [Cheatsheet](https://raw.githubusercontent.com/rstudio/cheatsheets/master/data-transformation.pdf).
</div>

# Pretty Plots

`ggplot2` bietet Ihnen vielfältige Möglichkeiten, Ihre Abbildungen zu verschönern und zu individualisieren. Hier stellen wir Ihnen die wichtigsten Spezifikationen vor.

## Achsenbeschriftungen

Die Beschriftungen der Achsen wird mit `xlab()` bzw. `ylab()` erstellt. Einen Titel können Sie mit `ggtitle()` hinzufügen:

```{r}
ggplot(asp) + 
  aes(x = Kons, y = d) + 
  geom_boxplot() + 
  xlab("Artikulationsstelle") + 
  ylab("Dauer (ms)") + 
  ggtitle("Boxplot-Daten")
```

Ansonsten können Sie auch `labs()` nutzen für alle Labels zusammen:

```{r}
ggplot(coronal) + 
  aes(x = Region, fill = Fr) + 
  geom_bar(position = "fill") + 
  labs(x = "Region", 
       y = "Proportion", 
       title = "Proportionale Aufteilung von Frikativen",
       subtitle = "Aufgeteilt nach Region")
```

## Koordinatensystem begrenzen

Um den sichtbaren Bereich eines Plots zu begrenzen oder zu erweitern, können Sie die folgenden Funktionen verwenden. Diese haben jedoch unterschiedliche "Nebenwirkungen" (s. auch [hier](https://ggplot2.tidyverse.org/reference/coord_cartesian.html)):

* `xlim()` und/oder `ylim()` bzw. `scale_x_continuous(limits = c())` und/oder `scale_y_continuous(limits = c())`: Eliminiert Datenpunkte beim Heranzoomen und wirft eine Warnmeldung. Dies beeinflusst ggf. Regressionslinien und andere überlagerte Abbildungskomponenten.
* `coord_cartesian(xlim = c(), ylim = c())`: Blendet die Datenpunkte nur aus und wirft daher keine Warnmeldung. Dies beeinflusst Regressionslinien und andere überlagerte Abbildungskomponenten nicht.

```{r}
# ohne Achsenbeschränkung:
ggplot(int) + 
  aes(x = dB, y = Dauer) + 
  geom_point()

# mit coord_cartesian()
ggplot(int) + 
  aes(x = dB, y = Dauer) + 
  geom_point() + 
  coord_cartesian(xlim = c(10,40), 
                  ylim = c(30,280))

# mit xlim() und ylim()
ggplot(int) + 
  aes(x = dB, y = Dauer) + 
  geom_point() + 
  xlim(10, 40) + 
  ylim(30, 280)
```

## Farben

`ggplot2` verwendet standardmäßig immer dieselbe Farbpalette. Ihnen stehen aber deutlich mehr Farben zur Verfügung, wie Ihnen diese [Farbauswahl](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf) zeigt. Die Farbnamen bekommen Sie auch mit:

```{r, eval = F}
colors()
```

```{r}
# die ersten 10 anzeigen:
colors()[1:10]
```

Ihre Farbauswahl teilen Sie mit den Argumenten `col` (Umriss- bzw. Linienfarbe) bzw. `fill` (Füllfarbe) im *aesthetic mapping* mit. Wenn wir die Variable `Kons` in folgendem Boxplot farb-kodieren wollen, sieht das so aus:

```{r}
# mit "fill" (empfohlen für Boxplots!)
ggplot(asp) + 
  aes(x = Kons, y = d, fill = Kons) + 
  geom_boxplot()

# mit "col"
ggplot(asp) + 
  aes(x = Kons, y = d, col = Kons) + 
  geom_boxplot()
```

Wie Sie sehen, wurde diesen zwei Plots automatisch eine Legende hinzugefügt, die aufschlüsselt, welche Farben für welche Werte der Variable `Kons` verwendet wurden. Jetzt wollen wir unsere Farben selbst auswählen:

```{r}
# "fill" mit eigenen Farben
farben <- c("darkgoldenrod1", "navy")
ggplot(asp) + 
  aes(y = d, x = Kons, fill = Kons) + 
  geom_boxplot(fill = farben)
```

Hier gibt es keine Legende. Wenn wir bei selbst gewählten Farben eine Legende haben wollen, benutzen wir eine Funktion namens `scale_color_manual()` bzw. `scale_fill_manual()`:

```{r}
# "fill" mit eigenen Farben
farben <- c("darkgoldenrod1", "navy")
ggplot(asp) + 
  aes(y = d, x = Kons, fill = Kons) + 
  geom_boxplot() + 
  scale_fill_manual(values = farben)

# dasselbe für den Barplot
ggplot(coronal) + 
  aes(x = Region, fill = Fr) + 
  geom_bar() + 
  scale_fill_manual(values = farben)
```

Für Boxplots gibt es übrigens die Möglichkeit, die Outlier uabhängig von der Box zu gestalten:

```{r}
farben <- c("darkgoldenrod1", "navy")
ggplot(asp) + 
  aes(y = d, x = Kons, fill = Bet) + 
  geom_boxplot(outlier.color = "red", 
               outlier.shape = 4, 
               outlier.size = 3) + 
  scale_fill_manual(values = farben)
```

## Weitere Spezifikationen

Es gibt natürlich noch deutlich mehr Spezifikationen für die einzelnen Abbildungstypen als die Farbe, z.B. die Zeichengröße, den Linientyp, die Punktform, die Schriftart... 

* `col`: Umriss- bzw. Linienfarbe
* `fill`: Füllfarbe
* `shape`: Form
* `size`: Größe
* `lty`: Linientyp
* `stroke`: Dicke für Text

Dazu gibt es eine Vignette:

```{r, eval = F}
vignette("ggplot2-specs")
```

Einige dieser Spezifikationen wenden wir hier an:

```{r}
ggplot(int) + 
  aes(x = dB, y = Dauer) +
  geom_point(col = "purple", 
             size = 3, 
             shape = 0) + 
  geom_line(col = "orange", 
            size = 1.5, 
            lty = "twodash")
```

Sie sollten sich aber natürlich immer sorgfältig überlegen, ob eine Spezifikation notwendig ist, um die Abbildung klarer zu gestalten.

## Schriftzeichengröße

Die Default-Schriftzeichengröße der Achsenbeschriftung und Titel ist 11pt oder kleiner. Vor allem wenn Sie Ihre Plots präsentieren, ist es SEHR wichtig, dass Sie die Schriftzeichengröße verändern. Wir empfehlen **mind. 16-24pt**. Hierfür müssen wir das `theme()` ändern.

```{r}
ggplot(asp) + 
  aes(x = Kons, y = d) + 
  geom_boxplot() + 
  xlab("Artikulationsstelle") + 
  ylab("Dauer (ms)") + 
  ggtitle("Boxplot-Daten") + 
  theme(text = element_text(size = 24),            # Beschriftungen & Titel
        axis.text = element_text(size = 18))       # Achsenbeschriftungen
```

<div class="gray">
**Weiterführende Infos: theme()**

Das `theme()` ist z.B. für die Hintergrundfarbe der Plots, die Achsen, und vieles weitere zuständig. Diese [Übersicht an Argumenten](https://ggplot2.tidyverse.org/reference/theme.html) der Funktion `theme()` zeigt Ihnen, wie viel Sie damit einstellen können.
</div>

## Plots unterteilen und anordnen

### Plots unterteilen

`ggplot2` bietet zwei Möglichkeiten, einen Plot in mehrere zu unterteilen: `facet_wrap()` und `facet_grid()`. Hauptargument dieser Funktionen ist/sind die üblicherweise kategoriale(n) Variable(n), deren Werte in getrennten Panels dargestellt werden sollen. Zum Beispiel können wir die Datenpunkte, die mit unterschiedlichen Phonemen assoziiert sind, in getrennten Panels darstellen.

Die Formeln, um die Variablen anzugeben, sehen wie folgt aus:

* `.~Var1` bzw. `~Var1`
* `Var1~.` (Punkt muss da sein!)
* `Var1~Var2`
* `Var1+Var2~Var3`
* `Var1~Var2+Var3`

Es bietet sich nicht an, mehr als drei Variablen in `facet_wrap()` oder `facet_grid()` zu verwenden, da dies die Übersichtlichkeit des Plots deutlich einschränkt.

`facet_wrap()` ordnet die Panels eines Plots in Reihen *und* Spalten an.

```{r}
# aufteilen nach Versuchsperson
ggplot(vdata) + 
  aes(x = F1, y = F2) + 
  geom_point() + 
  facet_wrap(~Subj)

# aufteilen nach Versuchsperson und Gespanntheit
ggplot(vdata) + 
  aes(x = F1, y = F2) + 
  geom_point() + 
  facet_wrap(Subj~Tense)
```

`facet_grid()` hingegen ordnet in Zeilen *oder* Spalten. Die Reihenfolge für die Formel ist `facet_grid(Zeilen~Spalten)`

```{r}
# aufteilen nach Versuchsperson in Zeilen
ggplot(vdata) + 
  aes(x = F1, y = F2) + 
  geom_point() + 
  facet_grid(Subj~.)

# aufteilen nach Versuchsperson in Spalten
ggplot(vdata) + 
  aes(x = F1, y = F2) + 
  geom_point() + 
  facet_grid(~Subj)

# aufteilen nach Versuchsperson und Tense
ggplot(vdata) + 
  aes(x = F1, y = F2) + 
  geom_point() + 
  facet_grid(Subj~Tense)
```

### Plots anordnen

Des Weiteren gibt es die Möglichkeit, mehrere Plots neben- oder untereinander anzuordnen. Hierfür verwenden wir die Funktion `grid.arrange()` die oben geladene Library `gridExtra`.

```{r}
plot1 <- ggplot(asp) +
  aes(x = Kons, y = d) +
  geom_boxplot()

plot2 <- ggplot(coronal) + 
  aes(x = Region, fill = Fr) + 
  geom_bar()

plot3 <- ggplot(int) +
  aes(x = dB, y = Dauer) +
  geom_line() + geom_point()

# in drei Spalten und einer Zeile anordnen
grid.arrange(plot1, plot2, plot3, ncol = 3, nrow = 1)

# in einer Spalte und drei Zeilen anordnen
grid.arrange(plot1, plot2, plot3, ncol = 1, nrow = 3)
```

# Hilfe zur Selbsthilfe

`ggplot2` ist nicht nur bekannt, sondern auch beliebt! Dementsprechend viel Hilfe bekommen Sie von der R Community. Hier ein paar gute Quellen für Hilfe bei der Erstellung von Abbildungen:

- Kapitel [Data Visualisation](https://r4ds.had.co.nz/data-visualisation.html) in Hadley Wickham's "R for Data Science"

- [Cookbook for R](http://www.cookbook-r.com/Graphs/)

- [Cheatsheet ggplot2](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)

- [Stack Overflow](https://stackoverflow.com/questions/tagged/ggplot2?sort=faq)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
```

<style>
div.green {background-color: #ecffeb; border-radius: 5px; padding: 20px;}
div.gray {background-color: #e8e8e8; border-radius: 5px; padding: 20px;}
</style>

# Packages updaten

Es ist Zeit, die installierten Packages zu updaten! Stellen Sie zuerst sicher, dass die Session neu und noch kein Paket geladen ist, indem Sie in der Werkzeugleiste auf `Session > Restart R` klicken. Warten Sie kurz, bis die Session neu gestartet wurde. Gehen Sie dann auf `Tools > Check for Package Updates`. Wählen Sie in dem erscheinenden Pop-up Fenster zuerst `Select All` und anschließend `Install Updates` aus. Es erscheint ein weiteres Pop-up, das fragt, ob Sie R vor der Installation neu starten wollen; dies können Sie mit Klick auf `No` verneinen. (Sollte es anschließend allerdings Probleme mit den Updates geben, versuchen Sie in dem Dialogfenster `Yes` auszuwählen, dann wird die Session ebenfalls nochmal neu gestartet). Das Updaten kann eine Weile dauern.

# Packages und Daten laden

Starten Sie das R Projekt, das Sie für diesen Kurs angelegt haben. Öffnen Sie hierfür RStudio und benutzen Sie die Schaltfläche oben rechts oder navigieren Sie zu Ihrem Kursverzeichnis und klicken Sie auf die `.Rproj` Datei. 

Laden Sie dann das *tidyverse* und den folgenden Data Frame:

```{r}
library(tidyverse)
url <- "http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf"
df <- read.table(file.path(url, "vdata.txt")) %>% as_tibble() %>% 
  rename(vokal = V, spannung = Tense, konsonant = Cons, tempo = Rate, subject = Subj) %>% 
  mutate(dauer = log(dur)) %>% 
  select(-c(X, Y))
```

# Einführung in die Inferenzstatistik

*Statistical analysis [...] is a strikingly subjective process* -- Bodo Winter

Eine **Population** oder Grundgesamtheit ist im statistischen Sinne die Menge aller Einheiten (d.h. Personen, Wörter, etc.), die in bestimmten Identifikationskriterien (z.B. Geschlecht, Herkunft, grammatikalische Funktion, etc.) übereinstimmen. Stellen Sie sich z.B. vor, Sie möchten die durchschnittliche Grundfrequenz (F0) aller Frauen in Deutschland erfassen. Dann ist Ihre Population die Menge aller Frauen in Deutschland, also ca. 40 Mio. Menschen. Den Populationsmittelwert $\mu$ (sprich: /myː/, geschrieben oft: mu) können Sie in diesem Beispiel nur ermitteln, indem Sie zu jeder einzelnen Frau in Deutschland fahren und deren Grundfrequenz messen, was natürlich schon rein wirtschaftlich unmöglich ist. 

Stattdessen erhebt man in der Wissenschaft meist **Stichproben** (*samples*), also z.B. nur einen Teil der weiblichen Bevölkerung, und geht davon aus, dass der so erhaltene Stichprobenmittelwert $m$ nicht allzu weit weg ist vom tatsächlichen Populationsmittelwert $\mu$. Je größer die Stichprobe ist, desto mehr wird sich deren Mittelwert $m$ und Standardabweichung $s$ dem tatsächlichen Populationsmittelwert $\mu$ und der Populationsstandardabweichung $\sigma$ (sigma) annähern. Für die Merkmale einer Population werden im Normalfall griechische Symbole verwendet, für die Merkmale einer empirisch erhobenen Verteilung (d.h. einer Stichprobe) werden römische Buchstaben benutzt.

Die Methoden der **Inferenzstatistik** ermöglichen uns Rückschlüsse von der Stichprobe auf die Population. Genauer gesagt: die Inferenzstatistik hilft uns dabei, die Parameter der Population zu **schätzen**. Es gibt auch Maße wie z.B. den Standard Error, die beschreiben wie gut oder schlecht die Schätzung ist. Bevor ein statistischer Test durchgeführt wird, muss klar sein, was getestet wird: die sogenannte Null-Hypothese (H0). Wir handeln dabei nach dem Prinzip der Falsifizierung, d.h. wir versuchen H0 mit einem statistischen Test zu widerlegen. Zusätzlich spricht man häufig von der Alternativ-Hypothese (H1), die genau das Gegenteil der Null-Hypothese ist. Hier ein Beispiel:

Wir haben ein Experiment durchgeführt, weil wir herausfinden wollen, ob die Spannung einen Einfluss auf die Vokaldauer hat. Die gesammelten Daten sind im Data Frame `df` festgehalten. Dann stellen wir folgende Hypothesen auf:

- H0: *Die Vokaldauer wird nicht von der Spannung beeinflusst.*
- H1: *Die Vokaldauer wird von der Spannung beeinflusst.*

Wir führen dann einen statistischen Test aus, um H0 zu falsifizieren. Aber Achtung: Wenn wir H0 falsifiziert haben, heißt das *nicht*, dass wir H1 verifiziert haben!

Zusätzlich müssen wir *vor* unserem statistischen Test das sog. **Signifikanzniveau**, auch **$\alpha$-Level** (alpha Level) genannt, festlegen. Das ist der Wahrscheinlichkeitswert, ab dem wir unsere Null-Hypothese verwerfen bzw. als falsifiziert betrachten. In der Wissenschaft gibt es drei (arbiträr festgelegte!) $\alpha$-Level: **0.05, 0.01, und 0.001** (wir verwenden hier immer die englische Schreibweise und nutzen Punkte als Dezimalzeichen). Wenn das Ergebnis unseres statistischen Tests das festgelegte Signifikanzniveau *unter*schreitet, betrachten wir H0 als widerlegt und bezeichnen das Ergebnis als "signifikant". Im Falle der hier vorgestellten statistischen Tests wird jeweils der **p-value** (p-Wert) berechnet, der uns Auskunft über das Signifikanzniveau gibt. Bei einem *p-value* von `p < 0.05` ist das Ergebnis des Tests statistisch signifikant.

# Normalverteilung

Oben haben wir auf den Unterschied zwischen einer Population und einer Stichprobe hingewiesen. Parallel dazu unterscheidet man auch zwischen theoretischen und empirischen Verteilungen. **Theoretische Verteilungen** haben häufig feststehende Namen (Normalverteilung, Poisson-Verteilung, Student-t-Verteilung, etc.) und es wird davon ausgegangen, dass Messwerte, die für eine gesamte Population erhoben wurden, einer bestimmten theoretischen Verteilung folgen. Wie wir aber festgestellt haben, können wir so gut wie nie eine Population vermessen und nutzen stattdessen eine Stichprobe. Die Messwerte einer Stichprobe stellen eine **empirische Verteilung** dar, weil sie empirisch erhoben wurden. Wir werden häufig testen müssen, welcher theoretischen Verteilung die erhobene empirische Verteilung am ehesten entspricht.

In vielen empirischen Experimenten entsprechen die Daten einer Normalverteilung (auch Gauss-Verteilung genannt). Diese Verteilung lässt sich durch die zwei Parameter Mittelwert und Standardabweichung vollständig beschreiben. Hier sehen Sie drei verschiedene Normalverteilungen: 

- schwarz: Mittelwert $\mu = 0$ und Standardabweichung $\sigma = 1$
- blau: Mittelwert $\mu = 3$ und Standardabweichung $\sigma = 2$
- grün: Mittelwert $\mu = -2$ und Standardabweichung $\sigma = 1.5$

![](img/normalverteilung.png)

Wie Sie sehen, verschiebt der Mittelwert die Normalverteilung entlang der x-Achse, während die Standardabweichung zu Veränderungen in der Breite der Verteilung führt: je größer die Standardabweichung, desto breiter die Normalverteilung. Außerdem zeigt die Abbildung, dass die Verteilung *kontinuierlich* ist, d.h. sie deckt den Wertebereich von minus bis plus unendlich ab.

## Auf Normalverteilung testen

Im Data Frame `df` wurde die Dauer von Vokalen festgehalten. Hier wollen wir testen, ob die empirische Verteilung der Dauer der Normalverteilung entspricht. Hier ist zunächst die empirisch gemessene Dauer in einer Wahrscheinlichkeitsdichteverteilung:

```{r}
ggplot(df) + 
  aes(x = dauer) + 
  geom_density() +
  xlim(3.0, 7.0)
```

<div class="green">
**Weiterführende Infos: Logarithmierung von Daten**

In der Abbildung sehen Sie die Vokaldauer nicht in Millisekunden, sondern in logarithmierter Form, d.h. wir haben den natürlichen Logarithmus auf die Daten angewendet (s. Code Snippet, in dem `df` geladen wurde). Bei bestimmten Messwerten kann es sinnvoll sein, die Daten zu logarithmieren, nämlich dann, wenn die empirische Verteilung ge-*skewed* ist. Bei Reaktionszeiten ist es z.B. nicht möglich, dass es Werte unter Null gibt; auch niedrige Werte (unter 100ms) sind sehr unwahrscheinlich, aber sehr lange Reaktionszeiten kommen durchaus vor. Bei Dauerwerten ist es ähnlich. Die tatsächlichen Dauerwerte in Millisekunden sehen so aus:

```{r}
ggplot(df) + 
  aes(x = dur) + 
  geom_density() + 
  xlab("Dauer (ms)")
```

Diese Verteilung hat einen starken rechtsseitigen *Skew*, eben weil lange Dauerwerte vorkommen, während sehr kurze Dauerwerte selten sind.

</div>

### Überlagerung der Normalverteilung

Um zu testen, ob Daten normalverteilt sind, sind visuelle Methoden unter angewandten Statistikern und Statistikerinnen beliebter als statistische Tests. Die erste Möglichkeit ist die Überlagerung der Normalverteilung über die empirische Verteilung. Dafür nutzen wir die Funktion `dnorm()`, die als Argumente den Mittelwert `mean` und die Standardabweichung `sd` bekommt. In `ggplot2` können wir nicht einfach "fremde" Funktionen wie `dnorm()` an unseren `ggplot()`-Code anhängen. Wir benutzen stattdessen die `ggplot2`-eigene Funktion `stat_function()`. Diese Funktion hat folgende Argumente:

- `fun`: Die Funktion, mittels derer eine neue Kurve (in unserem Fall: die Kurve der Normalverteilung) erzeugt werden soll.

- `args`: Eine Liste von Argumenten der unter `fun` angegebenen Funktion. In unserem Fall braucht die Funktion `dnorm()` die Argumente `mean` und `sd`.

```{r}
ggplot(df) + 
  aes(x = dauer) + 
  geom_density() + 
  xlim(3.0, 7.0) + 
  stat_function(fun = dnorm, 
                args = list(mean = mean(df$dauer), sd = sd(df$dauer)),
                color = "blue")
```

Es gibt leichte Unterschiede zwischen der blauen Normalverteilung und der schwarzen empirischen Verteilung. Wahrscheinlich sind unsere Daten also nicht perfekt normalverteilt, aber zumindest annähernd.

### Q-Q-Plot

Neben der Überlagerung der Normalverteilung auf die empirische Verteilung werden häufig sogenannte **Q-Q-Plots** benutzt, wobei Q für **Quantil** steht. Schauen Sie folgendes [YouTube-Video](https://www.youtube.com/watch?v=X9_ISJ0YpGw), um zu verstehen, wie ein Q-Q-Plot berechnet wird. Und lassen Sie sich nicht verwirren: Obwohl im Video auf der y-Achse "*sample quantiles*" steht, sind das einfach nur die aufsteigend geordneten Datenpunkte!

```{r}
ggplot(df) + 
  aes(sample = dauer) + 
  stat_qq() + 
  stat_qq_line() + 
  ylab("samples") + 
  xlab("theoretical quantiles")
```

In `ggplot2` kann diese Abbildung mittels `stat_qq()` erstellt werden. Zusätzlich plotten wir mit `stat_qq_line()` eine gerade Linie, die wir zur Orientierung nutzen können. Wenn die Punkte von der Linie abweichen, sind die Daten nicht normalverteilt (wobei leichte Abweichungen am oberen und unteren Ende der Linie recht häufig sind). In diesem Fall ist ebenfalls eine leichte Abweichung von der Normalverteilung zu erkennen.

### Interpretation trainieren

Anfänglich kann es schwierig sein, anhand einer oder zwei Abbildungen festzulegen, ob die geplotteten Daten normalverteilt sind. Hier zeigen wir deshalb vier Beispiele für eindeutig nicht normalverteilte Daten, damit Sie Ihren Blick dafür schärfen können, wie Q-Q-Plots und überlagerte Wahrscheinlichkeitsverteilungen *nicht* aussehen sollten, wenn Ihre Daten normalverteilt sind.

Im folgenden sehen Sie vier Wahrscheinlichkeitsverteilungen: für bimodale Daten (es gibt zwei Peaks), für links und rechts ge-*skew*-te Daten, sowie für uniform verteilte Daten (wo jeder Wert theoretisch gleich häufig vorkommt).

![](img/dists.png)

Hier sind dieselben Wahrscheinlichkeitsverteilungen mit der jeweils parametrisch angepassten Normalverteilung (d.h. für jede Normalverteilung wurden der der abgebildeten Verteilung entsprechende Mittelwert und die entsprechende Standardabweichung verwendet):

![](img/dists_norm.png)

Zuletzt erstellen wir noch für alle vier Verteilungen die Q-Q-Plots, die deutlich von der geraden Linie abweichen:

![](img/dists_qq.png)

### Shapiro-Wilk Test

Zuletzt schauen wir uns noch den **Shapiro-Wilk Test** an, weil das der am häufigsten verwendete Test für Normalverteilungen ist. In R ist dazu die Funktion `shapiro.test()` gedacht, die als einziges Argument die Daten als Vektor bekommt:

```{r}
shapiro.test(df$dauer)
```

Die Null-Hypothese dieses Tests ist, dass die Daten tatsächlich normalverteilt sind. Wenn der p-Wert unter dem allgemein anerkannten Signifikanzniveau von $\alpha = 0.05$ liegt, müssen wir diese Hypothese ablehnen und daher davon ausgehen, dass die Daten nicht normalverteilt sind. Unsere logarithmierten Dauerwerte sind laut diesem Test also nicht normalverteilt. Für den Shapiro-Wilk Test sollte Ihr Datensatz aus weniger als 5000 Beobachtungen bestehen und nicht zu viele Ausreißer oder identische Werte besitzen, da dies die Aussagekraft des Ergebnisses stark beeinflussen kann. Trotz dieses Testergebnisses würde ich wegen der zuvor erstellten Abbildungen davon ausgehen, dass unsere Daten annähernd normalverteilt sind.

<div class="green">
**Weiterführende Infos: Wissenschaftliche Notation von Zahlen**

Sehr große und sehr kleine Zahlen werden in R häufig in der wissenschaftlichen Notation dargestellt, also z.B. 1e+02 anstatt 100. "e" steht hierbei für Basis 10 und die Zahlen danach sind der Exponent:

```{r}
1e+02 == 1 * 10^2
1e-02 == 1 * 10^-2
```

Bei Zahlen wie 100 oder 0.01 kann man die wissenschaftliche Notation noch ganz gut im Kopf umformen. Aber natürlich kann Ihnen R dies auch abnehmen, und zwar mit der Funktion `format()`, die das Argument `scientific = FALSE` bekommt:

```{r}
format(1e+02, scientific = F)
format(1e-02, scientific = F)
# und für unseren p-value aus dem Shapiro-Wilk Test oben:
format(2.062e-11, scientific = F)
```

Achtung, die Funktion gibt nur Schriftzeichenobjekte zurück, sie müssen also (falls Sie mit der umgewandelten Zahl arbeiten wollen) `as.numeric()` verwenden, um das Ergebnis von `format()` wieder in eine Zahl umzuwandeln:

```{r}
class(format(1e+02, scientific = F))
as.numeric(format(1e+02, scientific = F))
```

</div>

## 68–95–99.7 Regel & Konfidenzintervalle

### Theoretisch

Für Normalverteilungen (und in Annäherung auch für quasi-normalverteilte Daten) gilt die sogenannte **68–95–99.7 Regel**. Wir illustrieren dies an einer Normalverteilung mit Mittelwert $\mu = 0$ und Standardabweichung $\sigma = 1$:

![](img/area_norm.png)

Die Gesamtfläche unter der Normalverteilung ist 1, d.h. wir können mithilfe der Fläche bestimmen, mit welcher Wahrscheinlichkeit Datenpunkte in einen Wertebereich fallen. Bei der Normalverteilung fallen 68% der Daten in den blauen Bereich, 95% der Daten in den blauen + roten Bereich und 99.7% aller Daten in den gesamten eingefärbten Bereich. Wie Sie anhand der x-Achse dieser Abbildung sehen können, entspricht dies $\mu\pm1\sigma$ (blau), $\mu\pm2\sigma$ (blau+rot) und $\mu\pm3\sigma$ (blau+rot+gelb). Die 68–95–99.7 Regel ist also eine Art Eselsbrücke, mit der man sich merken kann, dass bei (annähernd) normalverteilten Daten 68% der Datenpunkte in einem Wertebereich von $\mu\pm1\sigma$ liegen, usw.

Die Fläche unter der Normalverteilung können wir mit **`pnorm()`** berechnen. Diese Funktion bekommt einen x-Wert und den Mittelwert und die Standardabweichung, die die Normalverteilung beschreiben (der *default* ist Null und Eins, deshalb lasse ich hier diese beiden Argumente weg). Dann berechnet `pnorm()` die Fläche von minus unendlich bis zu dem genannten x-Wert. Wenn wir einen ganz hohen x-Wert eintragen, sollte klar werden, dass die Fläche unter der Normalverteilung tatsächlich 1 ist:

```{r}
pnorm(100)
```

Für `x = 0` (also unseren Mittelwert) sollte die Fläche 0.5 betragen:

```{r}
pnorm(0)
```

Das heißt 50% aller Datenpunkte fallen in den Bereich von minus unendlich bis null. Nun können wir auch die oben eingefärbten Flächen berechnen:

```{r}
# blaue Flächen:
pnorm(0) - pnorm(-1)
pnorm(1) - pnorm(0)
# rote Flächen:
pnorm(-1) - pnorm(-2)
pnorm(2) - pnorm(1)
# gelbe Flächen:
pnorm(-2) - pnorm(-3)
pnorm(3) - pnorm(2)
```

In der Statistik interessieren wir uns häufig dafür, ob Daten in das **Konfidenzintervall** fallen, meist wird vom 95%-Konfidenzintervall gesprochen. Das heißt bei normalverteilten Daten prüfen wir, ob ein Datenpunkt in den Wertebereich $\mu \pm 2\sigma$ fällt. Um wiederum herauszufinden, welcher Wertebereich das genau ist, nutzen wir die **`qnorm()`** Funktion, die wieder Mittelwert und Standardabweichung als Argumente bekommt und zusätzlich die gewünschte Fläche unter der Verteilung. Wenn wir symmetrisch um den Mittelwert eine Fläche von insgesamt 0.95 haben wollen, ist die Fläche am linken und rechten Rand der Verteilung jeweils $(1 - 0.95) / 2 = 0.025$. Für unsere Normalverteilung oben können wir dann berechnen:

```{r}
qnorm(0.025)
qnorm(1-0.025)
```

Das heißt für die Normalverteilung mit Mittelwert $\mu = 0$ und Standardabweichung $\sigma = 1$ liegt eine Fläche von 0.95 (d.h. 95% der Datenpunkte) zwischen -1.96 und 1.96. Oben hatten wir gesagt, dass die Fläche von 0.95 $\mu \pm2\sigma$ entspricht -- genauer hätten wir schreiben müssen $\mu \pm 1.96\sigma$.

### Empirisch

Nehmen wir an, dass wir festgestellt haben, dass die Dauerwerte in `df` normalverteilt sind. Was wir nun mit unserem Wissen über die 68–95–99.7 Regel und Konfidenzintervalle anstellen können, ist zum Beispiel herauszufinden, mit welcher Wahrscheinlichkeit ein neuer Messpunkt aus demselben Experiment in einen gewissen Wertebereich fällt. Die folgenden Abbildungen zeigen die Normalverteilung mit Mittelwert `mean(df$dauer)` und Standardabweichung `sd(df$dauer)` und nicht die empirische Datenverteilung.

```{r}
mittel <- mean(df$dauer)
stdabw <- sd(df$dauer)
```


Wenn wir also dasselbe Experiment, aus dem `df` entstanden ist, nochmal durchführen, mit welcher Wahrscheinlichkeit würde ein neuer Datenpunkt in einem Wertebereich unter 4.5 (logarithmierte Dauer) liegen?

![](img/norm_area1.png)

Das ist äquivalent zu der Frage, wie groß die blaue Fläche ist, denn die gesamte Fläche unter der Normalverteilung ist immer gleich 1. Hierfür benutzen wir die Funktion `pnorm()`: 

```{r}
pnorm(4.5, mean = mittel, sd = stdabw)
```

Die Wahrscheinlichkeit, dass ein neuer Datenpunkt in einen Wertebereich von minus unendlich bis 4.5 fällt, liegt also bei 7.4%. 

Nun dasselbe für die folgende blaue Fläche:

![](img/norm_area2.png)

Wir fragen uns also, wie groß diese Fläche ist bzw. mit welcher Wahrscheinlichkeit ein neuer Datenpunkt in den Bereich über 5.1 fällt. Bei der Berechnung müssen wir nun aber zwei Dinge beachten: Erstens berechnet `pnorm()` immer die Fläche zwischen minus unendlich und einem Wert, zweitens ist die Fläche unter der Normalverteilung gleich 1. Deshalb:

```{r}
1 - pnorm(5.1, mean = mittel, sd = stdabw)
```

Die Fläche hat also eine Größe von 0.254, bzw. die Wahrscheinlichkeit liegt bei 25.4%.

Wenn es uns nun um die Wahrscheinlichkeit geht, mit der Werte dem Experiment in einen Wertebereich von 4.9 bis 5.5 fallen, müssen wir `pnorm()` wie folgt austricksen:

![](img/norm_area3.png)

```{r}
pnorm(5.5, mean = mittel, sd = stdabw) - pnorm(4.9, mean = mittel, sd = stdabw)
```

Hier liegt die Wahrscheinlichkeit, dass ein neuer Datenpunkt in den abgefragten Wertebereich fällt, bei 49.7%.

Nun wollen wir für unsere Verteilung von Dauerwerten noch das 95%-Konfidenzintervall bestimmen, d.h. den Wertebereich, in dem 95% der Daten liegen. Das bedeutet nichts anderes, als dass wir den Wertebereich zu errechnen versuchen, für den die Fläche unter der Normalverteilung 0.95 ist. Da die Fläche symmetrisch um den Mittelwert verteilt sein soll, muss auf jeder Seite der Randbereich berechnet werden, für den die Fläche jeweils $(1 - 0.95) / 2 = 0.025$ ist.

![](img/norm_area4.png)

Hierfür verwenden wir wieder `qnorm()`:

```{r}
qnorm(0.025, mean = mittel, sd = stdabw)
qnorm(0.975, mean = mittel, sd = stdabw)
```

Das bedeutet, dass ein neuer Wert aus dem Experiment mit einer Wahrscheinlichkeit von 95% in den Wertebereich von 4.35 bis 5.47 fallen wird bzw. dass 95% der Daten aus dem aktuellen Experiment in diesem Bereich liegen (unter der Annahme, dass die Daten normalverteilt sind).

```{r}
anzahl <- df %>% 
  filter(dauer > 4.35 & dauer < 5.47) %>% 
  nrow()
anzahl
anzahl / nrow(df)
```

Tatsächlich liegen 96.5% der Daten in dem berechneten Wertebereich. Hier zeigt sich noch einmal der Unterschied zwischen theoretischer und empirischer Verteilung: Wir haben geprüft, dass unsere empirischen Daten annähernd normalverteilt sind und daraufhin mit dem Mittelwert und der Standardabweichung der Daten eine Normalverteilung erstellt. Es ist eine *theoretische* Berechnung, dass 95% der Daten im Wertebereich zwischen 4.35 und 5.47 liegen (und dass 7.4% der Daten in einem Bereich unter 4.5 liegen, etc.), denn dies beruht auf der theoretischen Normalverteilung. Wenn wir uns die tatsächliche Proportion der Werte in den Bereichen und im Konfidenzintervall anschauen, kann sie sich von den theoretisch berechneten Proportionen unterscheiden.

# Statistik in R: Literatur

Wenn Sie mehr Informationen zu benötigen, seien Ihnen folgende Werke ans Herz gelegt:

- Bodo Winter's "Statistics for Linguists: An Introduction using R": Ein frisch erschienenes Buch voller hervorragender Erklärungen zu allen wichtigen Themen der Inferenzstatistik. Ist über die Uni-Bib digital verfügbar.

- Stefan Gries' "Statistics for Linguistics with R: A Practical Introduction": Nützlich für die Entscheidungsfindung, welches statistische Modell zu den eigenen Daten und der eigenen Fragestellung passt. Da das Buch von 2009 ist, ist der Code z.T. veraltet, aber aus statistischer Sicht ist der Inhalt noch aktuell. Ist über die Uni-Bib digital verfügbar.

- Harald Baayen's "Analyzing Linguistic Data: A Practical Introduction to Statistics": Einführung für eher Fortgeschrittene. Hier ist der R Code ebenfalls oft veraltet, aber die Erklärungen und Beispiele zu den Statistikgrundlagen sind hilfreich. Als physisches Exemplar in der Unibib verfügbar.


# Packages und Daten laden

Starten Sie das R Projekt, das Sie für diesen Kurs angelegt haben. Öffnen Sie hierfür RStudio und benutzen Sie die Schaltfläche oben rechts oder navigieren Sie zu Ihrem Kursverzeichnis und klicken Sie auf die `.Rproj` Datei.

Installieren Sie das Paket `broom`:

```{r, eval = F}
install.packages("broom")
```

Laden Sie dann die folgenden Packages und Data Frame:

```{r}
library(broom)
library(tidyverse)
url <- "http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf"
queen <- read.table(file.path(url, "queen.txt")) %>% as_tibble()
```

# Einfache lineare Regression

Bisher haben wir uns mithilfe der deskriptiven Statistik bestimmte Messwerte (Variablen) genauer angeschaut und einiges über empirische und theoretische Verteilungen erfahren. Häufig stehen solche Variablen jedoch in Abhängigkeit zu anderen Variablen. Zum Beispiel ist durch viele Studien belegt worden, dass unsere Reaktionsfähigkeit mit steigendem Schlafmangel abnimmt. Das heißt, dass die Variable Reaktionszeit von der Variable Schlafmangel abhängig ist. Wir sprechen deshalb auch von abhängigen und unabhängigen Variablen. Mit der einfachen linearen Regression lassen sich diese Abhängigkeiten beschreiben. Oft wird auch davon gesprochen, dass man den Wert der abhängigen Variable $y$ durch die unabhängige Variable $x$ vorhersagt. Bevor wir eine lineare Regression durchführen, sprechen wir über Regressionslinien und Korrelation.

## Korrelation

Die Korrelation, auch *Pearson's correlation* $r$, ist ein Maß für die Assoziation zwischen zwei Variablen und kann mit der Funktion `cor()` berechnet werden. Wir werden hier mit den Data Frame `queen` arbeiten:

```{r}
queen %>% head()
```

Darin sind die durchschnittlichen Grundfrequenzwerte von Queen Elizabeth II. bei ihren jährlichen Weihnachtsansprachen festgehalten. Uns interessiert, ob das Alter der Queen einen Einfluss auf ihre Grundfrequenz hat. Erstmal machen wir uns ein Bild von der Lage. Es ist wichtig, dass wir bei Abbildungen für Daten, bei denen wir eine Korrelation vermuten, die unabhängige Variable (hier: Alter) immer auf die x-Achse und die abhängige Variable (hier: f0) immer auf die y-Achse packen.

```{r}
ggplot(queen) + 
  aes(x = Alter, y = f0) + 
  geom_point()
```

Es sieht so aus, als ob es da einen Zusammenhang geben könnte: Je älter die Queen wird, desto mehr sinkt ihre Grundfrequenz! Unseren visuellen Eindruck können wir anhand der Korrelation $r$ überprüfen:

```{r}
cor(queen$Alter, queen$f0)
```

Die Korrelation $r$ nimmt ausschließlich Werte zwischen -1 und 1 an. Je näher der Wert an Null ist, desto schwächer ist die Abhängigkeit zwischen den beiden Variablen. Mit -0.84 liegt eine starke negative Korrelation vor, d.h. unser visueller Eindruck scheint zu stimmen.

## Die Regressionslinie

### Theoretische Informationen

Die Regressionslinie der einfachen linearen Regression lässt sich mit folgender Formel beschreiben:

$y = k + bx$

Hierbei ist $k$ der y-Achsenabschnitt (engl. *intercept*) und $b$ die Steigung (engl. *slope*). Weil Intercept und Slope eine Regressionslinie eindeutig beschreiben, nennt man die beiden Parameter auch **Regressionskoeffizienten**. Durch die oben gegebene Formel lassen sich bei bekanntem Intercept $k$ und bekannter Slope $b$ also für alle möglichen $x$-Werte auch die entsprechenden $y$-Werte vorhersagen. Die Regressionslinie ist immer eine unendliche, exakt gerade Linie und verläuft außerdem durch den Mittelwert der Verteilung. 

In der folgenden Abbildung sehen Sie drei Regressionslinien: blau und grün haben dasselbe Intercept, aber entgegengesetzte Slopes; blau und orange haben unterschiedliche Intercepts, aber die gleiche Slope. Der genaue Wert der Steigung gibt an, um wie viel der $y$-Wert steigt oder sinkt, wenn man um eine $x$-Einheit erhöht. Für $x = 0$ in der Abbildung ist $y = 1$ (für blau und grün). Für $x = 1$ ist $y = 1 + b$, also für blau $y = 1 + 0.5 = 1.5$ und für grün $y = 1 + (-0.5) = 0.5$. Für die orange Linie gilt bei $x = 0$ ist $y = 2$, bei $x = 1$ ist $y = 2 + 0.5 = 2.5$.

```{r}
ggplot() + 
  xlim(-0.5, 1) + ylim(-0.5, 3) + xlab("x") + ylab("y") +
  geom_abline(slope = 0.5, intercept = 1, color = "blue", size = 1.2) + 
  geom_abline(slope = -0.5, intercept = 1, color = "darkgreen", size = 1.2) + 
  geom_abline(slope = 0.5, intercept = 2, color = "orange", size = 1.2) + 
  geom_vline(xintercept = 0, lty = "dashed")
```

Zusammengefasst beschreiben die blaue und orange Linie also eine positive Korrelation zwischen $x$ und $y$ (je größer $x$, desto größer $y$), die grüne Linie beschreibt eine negative Korrelation (je größer $x$, desto kleiner $y$).

**Achtung: Correlation is not causation!** Die lineare Regression kann nur die Korrelation zwischen zwei Variablen beschreiben, nicht aber die Kausalität. Die Kausalität bringen wir mit unserem Wissen ins Spiel. Wir wissen also zum Beispiel, dass es der Schlafmangel ist, der eine langsamere Reaktionszeit verursacht. Die lineare Regression kann nur zeigen, ob eine Beziehung zwischen Reaktionszeit und Schlafmangel besteht, aber genauso gut könnte das aus Sicht der Regression bedeuten, dass eine langsamere Reaktionszeit Schlafmangel verursacht.

### Regressionslinien mit `ggplot2`

Um eine Regressionslinie durch eine `ggplot2`-Abbildung zu legen, können wir entweder `geom_abline()` (s.o.) oder `geom_smooth()` benutzen. Die erste Funktion bekommt die Argumente `slope` und `intercept`, wie Sie in der Abbildung oben sehen können. Die Funktion `geom_smooth()` hingegen bekommt das Argument `method = "lm"`. "lm" steht für **linear model**, d.h. damit berechnet die Funktion *slope* und *intercept* für uns unter der Annahme, dass die Daten in einer linearen Beziehung stehen. Zusätzlich geben wir das Argument `se = F` an, weil wir hier keine Konfidenzintervalle angezeigt bekommen wollen. So sieht die Regressionslinie im Fall der Queen aus:

```{r}
ggplot(queen) + 
  aes(x = Alter, y = f0) + 
  geom_point() + 
  geom_smooth(method = "lm", se = F, color = "blue")
```

Der Unterschied zwischen `geom_abline()` und `geom_smooth()` ist, dass `geom_abline()` eine theoretisch unendlich lange, gerade Linie zeichnet (aber wir sehen natürlich nur einen Ausschnitt davon), während sich `geom_smooth()` nach dem Wertebereich der Daten richtet. `geom_smooth()` kann zusätzlich auch andere Arten von Regressionslinien zeichnen.

## Lineare Regression mit `lm()`

Nun sind wir bereit, eine lineare Regression mittels der Funktion `lm()` durchzuführen. Diese Funktion bekommt als Argumente nur eine Formel und den Data Frame. Die Formel lautet `y ~ x`, d.h. wir wollen die $y$-Werte (die Grundfrequenz) in Abhängigkeit von den $x$-Werten (dem Alter) vorhersagen. Die lineare Regression schätzt Intercept und Slope so ein, dass eine Regressionslinie durch die Datenpunkte gelegt werden kann, die den kleinstmöglichen Abstand zu allen Punkten hat (dieses Verfahren heißt auch **least squares**).

```{r}
queen.lm <- lm(f0 ~ Alter, data = queen)
queen.lm
```

Die Koeffizienten lassen sich separat auch mit `coef()` ausgeben:

```{r}
queen.lm %>% coef()
```

Wir sehen also, dass das geschätzte Intercept bei 288.2 liegt und die Steigung bei -1.07. Die Steigung wird leider verwirrenderweise immer wie die $x$-Variable genannt, in diesem Fall also "Alter". Die Koeffizienten bedeuten folgendes: Bei einem Alter von Null Jahren ($x = 0$) liegt die mittlere Grundfrequenz bei ca. 288 Hz, wenn es einen perfekten linearen Zusammenhang zwischen dem Alter und der Grundfrequenz der Queen gibt. Mit jeden weiteren Jahr ($x$ wird um 1 erhöht) sinkt die Grundfrequenz um 1.07 Hz. Indem wir Intercept und Slope in unsere Formel von vorhin einsetzen, können wir nun für alle möglichen Alter die entsprechende Grundfrequenz vorhersagen:

```{r}
x <- c(0, 40, 50)
f0_fitted <- coef(queen.lm)[1] + coef(queen.lm)[2] * x
f0_fitted
```

Bei einem Alter von Null Jahren lag die geschätzte Grundfrequenz der Queen wie schon gesagt bei 288 Hz. Bei einem Alter von 40 Jahren waren es vermutlich schon nur noch 245 Hz, bei 50 Jahren 234.5 Hz. Wie Sie sehen, lassen sich anhand des von uns "gefitteten Modells" auch $y$-Werte vorhersagen bzw. schätzen, die nicht im originalen Datensatz enthalten waren (wie z.B. den f0-Wert für die 50-jährige Queen). Alle diese Punkte liegen aber genau auf der Regressionslinie und da die Regressionslinie unendlich lang ist, ergibt die Schätzung nicht zwangsläufig für alle Werte Sinn. Halten Sie es beispielsweise für wahrscheinlich, dass die Grundfrequenz der Queen bei ihrer Geburt bei 288 Hz lag? Normalerweise haben Kinder eine Grundfrequenz von 300 bis 400 Hz. Sie müssen für Ihre Daten immer wissen, ob die Schätzungen sinnvoll sind oder nicht. In R können Sie die Schätzungen mit der Funktion `predict()` durchführen, die als Argumente das Modell `queen.lm` und einen Data Frame mit den $x$-Werten bekommt, für die $y$ geschätzt werden soll. Dabei muss die $x$-Variable genauso heißen wie in dem ursprünglichen Data Frame, also hier "Alter":

```{r}
predict(queen.lm, data.frame(Alter = seq(0, 100, by = 10)))
```

## Residuals

Wir sehen oben den geschätzten $y$-Wert für $x = 40$, also den Grundfrequenzwert für die Queen mit 40 Jahren, nämlich ca. 245 Hz. Der tatsächlich gemessene Wert liegt aber weit darunter:

```{r}
queen %>% filter(Alter == 40) %>% pull(f0)
```

Die Differenzen zwischen den geschätzten und gemessenen $y$-Werten werden **Residuals** genannt. Die folgende Abbildung zeigt einen Ausschnitt aus dem vorherigen Plot für die Altersspanne zwischen 30 und 40 Jahren. Die schwarzen Punkte sind die tatsächlich gemessenen f0-Werte, die roten Punkte hingegen sind die geschätzten Werte. Sie liegen genau auf der blauen Regressionslinie. Die vertikalen gestrichelten Linien stellen die Residuals dar. Diese Abbildung verdeutlicht, weshalb Residuals auch als **Error** bezeichnet werden.

![](img/resid.png)

Der Abstand zwischen den tatsächlichen und geschätzten Werten wird berechnet als die Summe über die quadrierten Residuals und heißt deshalb auch **sum of squares of error** (SSE). Das Verfahren, mit dem die Parameter der Regressionslinie geschätzt werden, heißt **least squares**, weil es versucht, die SSE so klein wie möglich zuhalten. Das führt dazu, dass die Regressionslinie so durch die Daten gelegt wird, dass alle Datenpunkte so nah wie möglich an der Linie sind.

Die Residuals können mit `resid()` ausgegeben werden:

```{r}
queen.lm %>% resid()
```

SSE kann mit `deviance()` berechnet werden:

```{r}
queen.lm %>% deviance()
```

## Annahmen überprüfen

Statistische Modelle wie die lineare Regression basieren auf Annahmen bezüglich der Daten, die erfüllt sein müssen, damit das Ergebnis des Modells überhaupt aussagekräftig ist. Im Falle der linearen Regression beziehen sich die Annahmen auf die Residuals.

### Normalität der Residuals

Die erste Annahme ist, dass die Residuals normalverteilt sind. Wir überprüfen das mit einem Q-Q-Plot:

```{r}
ggplot(augment(queen.lm)) + 
  aes(sample = .resid) + 
  stat_qq() + 
  stat_qq_line() + 
  ylab("samples") + 
  xlab("theoretical quantiles")
```

Das sieht okay, aber nicht perfekt aus. Wir erstellen noch eine Wahrscheinlichkeitsverteilung mit überlagerter Normalverteilung:

```{r}
ggplot(augment(queen.lm)) + 
  aes(x = .resid) + 
  geom_density() + 
  xlim(-40, 40) + 
  xlab("residuals") +
  stat_function(fun = dnorm, 
                args = list(mean = mean(augment(queen.lm)$.resid), sd = sd(augment(queen.lm)$.resid)), 
                inherit.aes = F, 
                color = "blue")
```

Wenn wir uns jetzt immer noch unschlüssig sind, können wir noch einen Shapiro-Wilk Test durchführen:

```{r}
shapiro.test(augment(queen.lm)$.resid)
```

Da der p-Wert hier höher ist als $\alpha = 0.05$, scheinen die Residuals normalverteilt zu sein.

### Konstante Varianz der Residuals

Die zweite Annahme besagt, dass die Varianz der Residuals für alle geschätzten Werte ähnlich sein sollte. Diese Annahme hat auch den schönen Namen **Homoskedastizität**. Wenn die Annahme nicht erfüllt wird, spricht man von **Heteroskedastizität**. Um die Varianz visuell darzustellen, plotten wir die Residuals gegen die geschätzten Werte. Da der Mittelwert der Residuals immer bei ungefähr Null liegt (gestrichelte Linie in der Abbildung bei $y = 0$), können wir die konstante Varianz daran erkennen, dass sich die Punkte gleichmäßig um diesen Mittelwert verteilen.

```{r}
ggplot(augment(queen.lm)) + 
  aes(x = .fitted, y = .resid) + 
  geom_point() +
  xlab("geschätzte f0-Werte") + 
  ylab("Residuals") + 
  geom_hline(yintercept = 0, lty = "dashed")
```

Da wir hier nur sehr wenige Datenpunkte haben, ist es schwierig einzuschätzen, ob es in dem Plot ein erkennbares Muster gibt, das darauf hinweisen würde, dass die Errors keine konstante Varianz hätten. Die zwei Ausreißer oben rechts sind jedenfalls kein gutes Zeichen, der Rest sieht okay aus. Um Ihre Intuition dafür zu schulen, was gute und schlechte Residual Plots sind, empfehle ich die Abbildungen 6.2 und 6.3 in Winter (2020, S. 111f.).

## Alle Ergebnisse von `lm()` verstehen

Die in diesem Abschnitt genannten Details zur Berechnung der verschiedenen Werte finden sich extrem selten in Büchern über Statistik, Statistik-Blogs, R-Vignetten oder sonstigen Informationsquellen. Sie müssen diese Details nicht auswendig lernen, es geht darum, dass Sie die Ergebnisse von `lm()` nachvollziehen können -- und dazu gehört mehr als nur der p-Wert.

### Geschätzte y-Werte und Residuals

Wie Sie vielleicht bemerkt haben, habe ich die Funktion `augment()` benutzt, um die Ergebnisse des linearen Modells in `ggplot2` nutzen zu können. Diese Funktion stammt aus dem Paket [`broom`](https://cran.r-project.org/web/packages/broom/vignettes/broom.html), das wir anfangs geladen haben und das noch zwei weitere hilfreiche Funktionen bereitstellt: `tidy()` und `glance()`. Das Ergebnis dieser Funktionen ist immer eine `tibble`, also ein Objekt, das wir easy weiterverarbeiten können (im Gegensatz zu den seltsamen Regressionsobjekten):

```{r}
queen.lm %>% class
queen.lm %>% augment() %>% class()
```

Schauen wir uns zuerst an, was `augment()` macht:

```{r}
queen.lm %>% augment()
```

Diese Funktion hängt an die originalen Daten (Spalten `f0` und `Alter`) weitere Spalten an, nämlich die gefitteten (also die vom Modell geschätzten) f0-Werte `.fitted`, die Residuals `.resid`, und weitere, die erstmal nicht interessant für uns sind.

### Regressionskoeffizienten und t-Statistik

Die Funktion `tidy()` gibt eine Tabelle für die geschätzten Regressionskoeffizienten zurück:

```{r}
queen.lm %>% tidy()
```

Die Spalte `estimate` enthält die Schätzungen für Intercept (erste Zeile) und Slope (zweite Zeile). Die Spalte `std.error` enthält den sogenannten **Standard Error**, ein Maß für die Genauigkeit der Schätzung. Hier wollen wir möglichst kleine Werte (in Relation zum Estimate), weil das bedeutet, dass die Schätzungen des Modells für die Regressionskoeffizienten präzise sind. Es folgt eine Spalte mit der Teststatistik `statistic`. Bisher haben wir noch nicht über die statistische Signifikanz der Regression gesprochen. Hier wird ein **t-Test** durchgeführt, um herauszufinden, ob sich die Schätzungen der Regressionskoeffizienten signifikant von Null unterscheiden. Wenn die Regressionskoeffizienten nahe Null sind, tragen sie nichts dazu bei, den $y$-Wert vorherzusagen (erinnern Sie sich an die Formel für die Regressionslinie: Wenn $k$ oder $b$ gleich Null sind, haben sie keinen Einfluss auf die Regressionslinie). Der Wert in der Spalte `statistic`, der in diesem Fall auch **t-Wert** genannt wird, wird berechnet als `estimate / std.error`, so z.B. für die Slope:

```{r}
as.numeric(tidy(queen.lm)[2, "estimate"] / tidy(queen.lm)[2, "std.error"])
```

Die $t$-Statistik hat ihre eigene Wahrscheinlichkeitsdichteverteilung, genannt Student-$t$-Verteilung oder einfach $t$-Verteilung, die wir (analog zur Funktion `dnorm()` für Normalverteilungen) mit der Funktion `dt()` in `ggplot2` zeichnen lassen können. Diese Funktion bekommt ein Argument namens `df`, das steht hier für **degrees of freedom** (Freiheitsgrade). Die Freiheitsgrade sind üblicherweise die Größe der Stichprobe minus die Anzahl der Koeffizienten, also hier `nrow(queen) - 2`.

```{r}
ggplot() +
  xlim(-5, 5) +
  stat_function(fun = dnorm) +
  stat_function(fun = dt, args = list(df = 28), col = "orange") +
  stat_function(fun = dt, args = list(df = 5), col = "darkgreen") +
  xlab("") + ylab("density")
```

Hier sehen Sie die $t$-Verteilung in orange gegen die schwarze Normalverteilung mit Mittelwert Null und Standardabweichung Eins; die beiden Verteilungen sind sich sehr ähnlich. Mit abnehmenden Freiheitsgeraden (zum Beispiel 5 Freiheitsgrade bei der dunkelgrünen Verteilung) werden sich die Normal- und $t$-Verteilung unähnlicher. Wie Sie wissen, ist die Fläche unter diesen Verteilungen immer 1, d.h. auch bei der $t$-Verteilung können wir mittels einer Funktion berechnen, wie groß die Wahrscheinlichkeit ist, dass ein Wert in einen bestimmten Wertebereich fällt. Der t-Wert für die Slope ist in unserem Beispiel ca. -8.02. Unter der orangen $t$-Verteilung, die zu unseren Daten passt, ist nur ein sehr sehr kleine Fläche unter der Kurve für den Wertebereich von minus unendlich bis -8.02 (um dies nachzuvollziehen, brauchen Sie ein bisschen Fantasie, da der Wertebereich in der Abbildung oben erst bei -5 beginnt). In Anlehnung an `pnorm()` heißt die Funktion für die Berechnung der Fläche unter der $t$-Verteilung `pt()`. So können wir unter Einsatz des t-Werts und der Freiheitsgrade den p-Wert berechnen:

```{r}
2 * pt(-8.016248, df = 28)
```

<div class="green">
**Weiterführende Infos: two-tailed t-Test**

Wir müssen den Wahrscheinlichkeitswert, den `pt()` berechnet, mit 2 multiplizieren, weil es sich bei dem berechneten t-Test nicht um einen one-tailed, sondern um einen two-tailed t-Test handelt. Als *tail* werden die extremen Enden der Verteilungen bezeichnet; für Normal- und $t$-Verteilung gilt, dass sehr hohe und sehr niedrige Werte unwahrscheinlich sind (d.h. es ist nur sehr wenig Fläche unter den Verteilungen von minus unendlich bis zu einem sehr niedrigen x-Wert bzw. von einem sehr hohen x-Wert bis plus unendlich).

Wenn wir hier mit `pt()` eine Wahrscheinlichkeit (bzw. Fläche) berechnen, gilt das nur für den *lower tail* von minus unendlich bis zu dem angegebenen x-Wert. Dieselbe Wahrscheinlichkeit gilt aber für die Fläche von dem ins Positive verkehrten x-Wert (hier: `abs(-8.016248)`) bis plus unendlich (den *upper tail*). Daher machen wir es uns hier einfach, indem wir das Ergebnis oben mit 2 multiplizieren. Wir hätten auch schreiben können:

```{r}
pt(-8.016248, df = 28) + pt(abs(-8.016248), df = 28, lower.tail = F)
```

Etwas Ähnliches haben Sie übrigens schon bei der Berechnung des 95%-Konfidenzintervalls für Normalverteilungen gesehen: Da ging es uns darum, die Fläche von 0.05 gleichmäßig auf beide symmetrischen Hälften der Verteilung aufzuteilen, d.h. wir haben auch da beide *tails* berücksichtigt, und nicht nur einen von beiden.

</div>

Der p-Wert kann hier wie folgt interpretiert werden: Wenn wir davon ausgehen, dass die tatsächliche Steigung Null ist (das ist die Null-Hypothese dieses t-Tests), dann ist die hier beobachtete Steigung von -1.07 höchst unerwartet.

### Gütekriterien für das Modell und F-Statistik

Nun, da wir die Ausgabe von `tidy()` verstehen, bleibt nur noch `glance()`. Die Funktion `glance()` zeigt auf einen Blick ein paar Kriterien, mit denen man die **Güte des Modells** einschätzen kann (für bessere Lesbarkeit verwandeln wir den Data Frame ins lange Format):

```{r}
queen.lm %>% glance() %>% pivot_longer(cols = c(r.squared:nobs))
```

Der Wert `r.squared` ist genau das, was der Name sagt: der quadrierte Korrelationswert $r$, den wir oben mit `cor()` berechnet hatten:

```{r}
cor(queen$Alter, queen$f0)^2
```

**$R^2$** beschreibt die Proportion der Varianz in den Daten, die von dem berechneten Modell beschrieben wird. Hier wird also ca. 70% der Varianz in den Daten durch das Modell mit einem Prädiktor (d.h. einer unabhängigen Variable) beschrieben. In der Linguistik sind viel niedrigere $R^2$-Werte üblicher, weil unser Untersuchungsgegenstand häufig von vielen zufälligen Faktoren beeinflusst wird, die wir nicht erfassen können. Der Wert `adj.r.squared` ist eine Form des $R^2$, die für die Anzahl der unabhängigen Variablen normalisiert. Das ist wichtig, denn bei einer höheren Zahl von unabhängigen Variablen wird $R^2$ automatisch steigen, selbst wenn eine oder mehrere der Variablen aus statistischer Sicht nichts dazu beitragen, die y-Werte zu erklären bzw. zu schätzen. Der *adjusted* $R^2$ hingegen bezieht die Anzahl der unabhängigen Variablen in die Berechnung von $r^2$ ein und ist daher verlässlicher als der einfache $R^2$. Da es hier nur eine Variable gibt, sind sich $R^2$ und *adjusted* $R^2$ sehr ähnlich.

Die Spalte `sigma` enthält den **Residual Standard Error**, das ist eine Schätzung für die Standardabweichung der Fehlerverteilung. Diesen Wert können wir mit `sigma()` berechnen (wir werden gleich nochmal auf diesen Wert zurückkommen):

```{r}
queen.lm %>% sigma()
```

Dann sehen wir wieder eine `statistic` samt `p.value`. Dieses Mal handelt es sich um die **$F$-Statistik**, d.h. diesmal lesen wir den p-Wert aus der F-Verteilung ab, die mit `df()` in `ggplot2` gezeichnet werden kann. Diese Verteilung ist von zwei Parametern abhängig, nämlich `glance(queen.lm)$df` und `glance(queen.lm)$df.residual`. Dies sind die Freiheitsgrade für das Modell und für die Residuals. In orange sehen Sie wieder die F-Verteilung, die zu unseren Daten passt (nämlich mit einem Freiheitsgrad für das Modell und 28 Freiheitsgraden für die Residuals). Die Verteilung kann nur Werte größer als Null annehmen. In dunkelgrün sehen Sie eine $F$-Verteilung, bei der die Freiheitsgrade für das Modell verändert wurden, und in schwarz zum Vergleich die Normalverteilung.

```{r}
ggplot(data.frame(x = 0:5)) +
  aes(x) +
  stat_function(fun = dnorm) +
  stat_function(fun = df, args = list(df1 = 1, df2 = 28), col = "orange") +
  stat_function(fun = df, args = list(df1 = 7, df2 = 28), col = "darkgreen") +
  xlab("") + ylab("density")
```

Wenn wir uns hier die orange Verteilung anschauen, sehen wir schon, dass ein $F$-Wert von 64.3 (s. `statistic`) extrem unwahrscheinlich wäre, daher ist der p-Wert auch sehr klein. Die Null-Hypothese im F-Test, der hier ausgeführt wurde, lautet, dass ein Modell ohne Prädiktoren die Daten genauso gut oder besser erklärt, als das Modell mit dem Prädiktor `Alter`. Das heißt, wenn der p-Wert, der aus dem F-Test resultiert, sehr klein ist, können wir daraus schließen, dass unser Modell mit der unabhängigen Variable `Alter` die Daten besser erklärt, als ein Modell ohne Prädiktoren.

Wir versuchen nun die Werte in den Spalten `statistic`, `df`, `df.residual` und `deviance` nachzuvollziehen. Letzteren kennen Sie schon unter dem Namen **SSE** (*sum of squared error*): 

```{r}
SSE <- queen.lm %>% deviance()
SSE
```

`df` ist wie bereits angedeutet die Anzahl der Freiheitsgrade des Modells und wird berechnet als die Anzahl der Regressionskoeffizienten $k$ minus 1:

```{r}
k <- length(queen.lm$coefficients)
df.SSR <- k-1
df.SSR
```

`df.residual` ist die Anzahl der Freiheitsgrade der Residuals, die als die Anzahl der Beobachtungen $N$ minus die Anzahl der Regressionskoeffizienten $k$ berechnet wird.

```{r}
N <- queen %>% nrow()
df.SSE <- N-k
df.SSE
```

Um zuletzt noch den $F$-Wert in der Spalte `statistic` nachvollziehen zu können, schauen wir uns die Formel dafür an: $F = \frac{MSR}{MSE}$. Hierbei steht **MSR** für *mean squares of regression* und **MSE** für *mean squares of error*. Diese beiden Werte beschreiben die Varianz der geschätzten y-Werte und die Varianz der Residuals. Um MSE und MSR berechnen zu können, müssen wir zuerst zwei andere Werte berechnen: *sum of squares of Y* (**SSY**), die den Abstand der Datenpunkte zum Mittelwert von $y$ beschreibt, und *sum of squares of regression* (**SSR**), die den Unterschied zwischen SSY und SSR beschreibt. Zuerst SSY:

```{r}
SSY <- sum((queen$f0 - mean(queen$f0))^2)
SSY
```

In der Praxis ist SSY die Summe aller Abstände zwischen den schwarzen (tatsächlich gemessenen) Datenpunkten und der orangen Linie, die die y-Achse bei `mean(queen$f0)` schneidet und eine Steigung von Null hat (diese Abbildung zeigt wieder nur einen Ausschnitt aus dem gesamten Wertebereich):

![](img/resid2.png)

Sie können hier schon rein visuell sehen, dass die orange Linie die Daten viel schlechter beschreibt als die blaue Regressionslinie, deshalb ist SSY auch viel größer als SSE.

Für SSR müssen wir nun nur noch die Differenz von SSY und SSE bilden:

```{r}
SSR <- SSY - SSE
SSR
```

Nun können wir endlich MSE und MSR berechnen:

```{r}
MSE <- SSE/df.SSE
MSE
MSR <- SSR/df.SSR
MSR
```

...Und aus der Division von MSE und MSR entsteht der $F$-Wert:

```{r}
F_wert <- MSR / MSE
F_wert
```

Zwischen der $t$-Statistik und unserer $F$-Statistik besteht übrigens ein quadratischer Zusammenhang: $F = t^2$ bzw. $t = \sqrt{F}$

```{r}
sqrt(F_wert)
```

Deshalb ist auch der p-Wert bei beiden Statistiken genau derselbe.

Zuletzt kommen wir nochmal auf den *residual standard error* zurück, den wir oben als eine Schätzung für die Standardabweichung der Residuen bezeichnet hatten. Dieser berechnet sich als die Quadratwurzel aus MSE:

```{r}
sqrt(MSE)
```

Wenn wir die Standardabweichung der Residuen ermitteln, sollte das sehr nah an dem *residual standard error* liegen (allerdings ist letzterer nur eine Schätzung, daher, müssen die Werte nicht gleich sein):

```{r}
queen.lm %>% augment() %>% pull(.resid) %>% sd()
```

Wenn der *residual standard error* genau Null ist, dann liegen alle Datenpunkte exakt auf der Regressionslinie, d.h. dann kann jeder y-Wert aus dem Dataset genau durch den dazugehörigen x-Wert mittels eines linearen Modells berechnet werden.

## Ergebnis berichten

Nun haben wir alle (für uns relevanten) Ergebnisse aus `lm()` nachvollzogen. Wir haben dies mittels der `broom` Funktionen gemacht. Eine traditionellere Art der Übersicht über die Ergebnisse der linearen Regression bietet `summary()`:

```{r}
queen.lm %>% summary()
```

Sie sollten hier alle Zahlen wiedererkennen.

Es ist außerordentlich wichtig, dass wir unser Ergebnis korrekt berichten. Dafür benötigen wir 

- $R^2$ (oder, weil stabiler: *adjusted* $R^2$): 0.69
- den $F$-Wert: 64.3
- die Freiheitsgrade für das Modell und die Residuals: 1 und 28
- den p-Wert, bzw. das nächst höhere Signifikanzniveau: p < 0.001

Wir berichten: **Es besteht eine signifikante lineare Beziehung zwischen dem Alter der Queen und ihrer Grundfrequenz ($R^2$ = 0.69, $F$[1, 28] = 64.3, $p$ < 0.001).**

# Zusammenfassung

- Bei der linearen Regression werden die Werte der abhängigen Variable $y$ durch die Werte der unabhängigen Variable $x$ geschätzt unter der Annahme, dass zwischen beiden eine lineare Beziehung besteht.
- Die Regressionslinie ist die gerade Linie, zu der der Abstand der Datenpunkte möglichst gering ist (*least squares* Verfahren).
- Die Funktion `lm()` schätzt die Regressionskoeffizienten y-Achsenabschnitt (*intercept*) und Steigung (*slope*).
- Es wird ein t-Test ausgeführt, um zu testen, ob sich die Regressionskoeffizienten von Null unterscheiden. Wenn $p < 0.05$ im t-Test für $x$, dann unterscheidet sich die Steigung signifikant von Null, d.h. $x$ ist ein guter Prädiktor für $y$.
- Die Differenzen zwischen den tatsächlichen und den geschätzten y-Werten heißen *Residuals* oder *Error*; der *residual standard error* ist eine Schätzung für die Standardabweichung der Fehlerverteilung.
- $R^2$ sind das Quadrat des Korrelationswertes $r$ und beschreibt die Proportion der Varianz in der abhängigen Variable $y$, die von dem linearen Modell beschrieben wird.
- Es wird außerdem ein $F$-Test ausgeführt, um zu prüfen, ob das lineare Modell erfolgreich einen signifikanten Anteil der Varianz in der abhängigen Variable erklärt. Wenn $p < 0.05$ im $F$-Test, dann beschreibt das Modell mit dem gewählten Prädiktor besser die empirischen Daten als ein Modell ohne Prädiktoren (Mittelwertmodell).
- *SSE* ist die *sum of squares of error* und beschreibt den Abstand der Datenpunkte zur Regressionslinie; im *least squares* Verfahren wird versucht, SSE zu minimieren
- *SSR* ist die *sum of squares of regression* und beschreibt, wie gut das Regressionsmodell im Vergleich zum Mittelwertmodell ist (SSY)


# Packages und Daten laden

Laden Sie die Pakete und Data Frames:

```{r}
library(broom)
library(emmeans)
library(tidyverse)
library(magrittr)
url <- "http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf"
faux <- read.table(file.path(url, "faux.txt"), stringsAsFactors = T, header = T) %>% 
  as_tibble()
int <- read.table(file.path(url, "dbwort.df.txt"), stringsAsFactors = T) %>% 
  as_tibble() %>% 
  rename(vowel = V, gender = G)
vlax <- read.table(file.path(url, "vlax.txt"), stringsAsFactors = T) %>%
  as_tibble() %>%
  filter(V %in% c("O", "I") & f0 != 0) %>%
  rename(vowel = V)
```

# Einführung

Bislang haben wir mit der einfachen linearen Regression gearbeitet, bei der eine Variable (z.B. Grundfrequenz) durch eine weitere Variable (z.B. Alter in Jahren) vorhergesagt wurde. Viel häufiger haben wir aber mehrere Variablen, von denen wir vermuten, dass sie einen Einfluss auf unsere Messwerte haben. Hierfür brauchen wir dann die multiple lineare Regression. Die Formel für die lineare Regression wird so angepasst, dass es für jede unabhängige Variable $x_1$, $x_2$, usw. eine eigene Slope gibt:

$y = k + b_1x_1 + b_2x_2 + ...$

Auch hier steht $k$ wieder für den y-Achsenabschnitt, $b_1$, $b_2$ usw. sind Slopes. Im Folgenden zeigen wir drei Beispiele für multiple lineare Regressionen, je nach der Art der unabhängigen Variablen (kategorial oder kontinuierlich). Aus Gründen der Übersichtlichkeit enthalten alle Beispiele nur zwei unabhängige Variablen; es können aber natürlich generell auch mehr als zwei Prädiktoren in der Regression vorkommen.

# Kontinuierliche unabhängige Variablen

Zuerst wollen wir herausfinden, ob die Grundfrequenz `f0` im (artifiziell erstellten) Data Frame `faux` von den beiden kontinuierlichen Variablen `dB` (Lautstärke) und `dur` (Wortdauer) abhängig ist. Dafür erstellen wir eine Abbildung von den Daten:

```{r}
ggplot(faux) +
  aes(x = dB, y = f0, col = dur) +
  geom_point()
```

Es ist nicht ganz einfach, diesen Plot zu interpretieren. Wenn wir uns erst einmal nur auf die Abhängigkeit der Grundfrequenz von der Lautstärke konzentrieren, sehen wir, dass eine klare positive Korrelation vorliegt, d.h. je lauter die Versuchspersonen sprachen, desto höher wurde ihre Grundfrequenz. Da die Dauer ebenfalls eine kontinuierliche Variable ist, bleibt uns nur, ein Farbkontinuum für die Visualisierung der Variable zu nutzen (`ggplot()` macht das automatisch durch das Argument `col`). Es scheint so, als ob die dunkleren Punkte (= niedrige Dauerwerte) mit eher hohen f0-Werten assoziiert sind und die helleren Datenpunkte (= hohe Dauerwerte) eher mit niedrigen f0-Werten. Hier könnte also eine negative Korrelation zwischen f0 und Dauer vorliegen. Mittels `cor()` können wir diesen Eindruck überprüfen:

```{r}
cor(faux$f0, faux$dB)
cor(faux$f0, faux$dur)
```

Da wir es hier mit zwei unabhängigen Variablen zu tun haben, müssen wir auch überlegen, ob eine **Interaktion** zwischen ihnen vorliegen könnte. Eine Interaktion liegt vor, wenn sich der Effekt einer unabhängigen auf die abhängige Variable für verschiedene (extreme) Werte der zweiten unabhängigen Variable unterscheidet. In unserem Beispiel ist der Effekt von Lautstärke auf f0 im Allgemeinen ein positiver, d.h. je lauter gesprochen wurde, desto höher f0. Dieser Effekt ist jedoch (visuell) stärker ausgeprägt für niedrige als für hohe Dauerwerte. Stellen Sie sich vor, Sie würden zwei Regressionslinien durch die obige Abbildung legen: eine dunkelblaue für einen beispielhaft niedrigen Dauerwert (z.B. 150 ms) und eine hellblaue für eine beispielhaft hohe Dauer (z.B. 450 ms). Wenn keine Interaktion zwischen den Variablen besteht, sind die zwei beispielhaften Regressionslinien parallel zueinander; andernfalls sind sie es nicht. Für die Daten im Data Frame `faux` habe ich die zwei beispielhaften Regressionslinien eingezeichnet: im linken Plot unter der Annahme, dass keine Interaktion besteht, im rechten unter der Annahme, dass eine Interaktion besteht:

![](img/interaction.png)

Die Regressionslinien im rechten Plot scheinen die Daten besser zu beschreiben als die Daten im linken Plot; d.h. wir gehen davon aus, dass zwischen Lautstärke und Dauer in dem Datensatz eine Interaktion besteht. Wir werden später sehen, wie genau die Regressionslinien für die beiden Plots berechnet wurden.

<div class="green">
**Weiterführende Infos: Regressionslinien bei zwei kontinuierlichen Prädiktoren**

Es erfordert ein bisschen Rechenarbeit, Regressionslinien für zwei kontinuierliche Variablen zu zeichnen. Wenn Sie diese Arbeit irgendwann vermeiden wollen, können Sie sich aber die Libraries `ggiraph` und `ggiraphExtra` installieren. In dieser [Vignette](https://cran.r-project.org/web/packages/ggiraphExtra/vignettes/ggPredict.html) finden Sie Informationen über die Funktion `ggPredict()` aus `ggiraphExtra`, die Regressionslinien für multiple Prädiktoren zeichnen kann.
</div>

Wir werden im Folgenden für viele Fragestellungen zuerst eine multiple Regression ohne und dann mit Interaktion berechnen, damit Sie sich mit Interaktionen vertraut machen können. Wenn Sie selbst vor der Entscheidung stehen, ob Sie in Ihrem Datensatz eine Interaktion zwischen unabhängigen Variablen vorliegen könnte, schauen Sie sich Ihren Datensatz in Abbildungen an und überlegen Sie genau, wie eine Interaktion zu interpretieren wäre -- *bevor* Sie die Regression durchführen.

## Ohne Interaktion

Wenn es in einem Modell keine Interaktionen gibt, werden die unabhängigen Variablen in der `lm()`-Formel mit einem Plus verbunden.

```{r}
lm1 <- lm(f0 ~ dB + dur, data = faux)
lm1 %>% 
  tidy() %>% 
  select(term, estimate)
```

Hier versuchen wir zu verstehen, was genau die Schätzungen in der Spalte `estimate` bedeuten. Die Schätzung für das Intercept ist auch in der multiplen Regression das geschätzte arithmetische Mittel der abhängigen Variable für $x_1 = 0$ und $x_2 = 0$ (und alle möglichen weiteren $x$, wenn es noch mehr unabhängige Variablen gibt). Das heißt, wenn die Lautstärke 0 dB beträgt und die Dauer ebenfalls 0 ms ist, sollte die Grundfrequenz bei 168.6 Hz liegen. Die Slopes messen nun nicht mehr den Effekt jeder einzelnen unabhängigen Variable, sondern den Effekt einer unabhängigen Variable, während alle anderen Variablen konstant bei Null gehalten werden. Jede Erhöhung der Lautstärke um ein Dezibel (bei konstanter Dauer) führt zu einer Erhöhung der Grundfrequenz um 2.9 Hz, jede Erhöhung der Dauer um eine Millisekunde (bei konstanter Lautstärke) führt zu einer Reduktion der Grundfrequenz um 0.4 Hz. Das heißt hier finden wir die erwarteten Zusammenhänge wieder: Lautstärke beeinflusst die Grundfrequenz positiv, Dauer negativ.

Rufen wir uns nochmal die Formel für die Regressionslinie bei zwei unabhängigen Variablen in Erinnerung:

$y = k + b_1x_1 + b_2x_2$

Die Werte für den y-Achsenabschnitt $k$ und die beiden Slopes $b_1$ und $b_2$ können wir uns aus dem Data Frame holen, den `tidy()` zurückgibt:

```{r}
k <- lm1 %>% tidy() %>% filter(term == "(Intercept)") %>% pull(estimate)
k
b_1 <- lm1 %>% tidy() %>% filter(term == "dB") %>% pull(estimate)
b_1
b_2 <- lm1 %>% tidy() %>% filter(term == "dur") %>% pull(estimate)
b_2
```

Nun können wir uns nach Belieben Werte für $x_1$ (Lautstärke in Dezibel) und $x_2$ (Dauer in Millisekunden) ausdenken, zusammen mit den Regressionskoeffizienten in die Formel einsetzen, und so y-Werte schätzen. Wenn wir beide $x$-Werte auf Null setzen, kommt genau das Intercept $k$ als geschätzter y-Wert heraus:

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot dur \\ 
 &= 168.64 + 2.89 \cdot 0 + (-0.37 \cdot 0) \\
 &= 168.64
\end{aligned}
$$

Wenn Sie diese Werte nicht manuell berechnen möchten, steht Ihnen natürlich die Funktion `predict()` zur Verfügung. Bei der multiplen linearen Regression muss der Funktion ein Data Frame übergeben werden, der so viele Spalten hat wie es unabhängige Variablen in der Regression gab; die Spalten müssen auch genau wie die unabhängigen Variablen benannt sein. Zu Demonstrationszwecken zeigen wir hier, dass die Grundfrequenz `f0` laut dem berechneten linearen Modell wirklich bei ca. 168 Hz liegt, wenn die Lautstärke bei 0 dB und die Dauer bei 0 ms liegt. Wir zeigen außerdem den geschätzten f0-Wert für `db = 1` und `dur = 0` (der 2.9 Hz höher ist als das Intercept) und für `db = 0` und `dur = 1` (der 0.4 Hz niedriger ist als das Intercept). Zuletzt schätzen wir mittels `predict()` den f0-Wert für die durchschnittliche Lautstärke und Dauer.

```{r}
d1 <- data.frame(dB  = c(0, 1, 0, mean(faux$dB)), 
                 dur = c(0, 0, 1, mean(faux$dur)))
d1 %<>% mutate(estimated_f0 = predict(lm1, d1))
d1
```

Jetzt können wir auch nachvollziehen, wie die zwei beispielhaften Regressionslinien im linken Plot oben berechnet wurden. Wir wählen 450 ms als beispielhaft hohen und 150 ms als beispielhaft niedrigen Dauerwert (in der Literatur wird als "extremer Wert" für eine unabhängige Variable auch gerne ihr Mittelwert plus/minus 1.5 Standardabweichungen genommen). Anschließend setzen wir diese Werte zusammen mit den Regressionskoeffizienten in unsere Formel für die Regressionslinie ein (hier zuerst für eine Dauer von 450 ms):

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot dur \\ 
 &= 168.64 + 2.89 \cdot dB + (-0.37 \cdot 450) \\
 &= 168.64 + 2.89 \cdot dB + (-167.43) \\
 &= 1.21 + 2.89 \cdot dB
\end{aligned}
$$

Dabei kommt heraus, dass das Intercept einer Regressionslinie für eine Dauer von 450 ms bei 1.21 liegen sollte und die Steigung bei 2.89.

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot dur \\ 
 &= 168.64 + 2.89 \cdot dB + (-0.37 \cdot 150) \\
 &= 168.64 + 2.89 \cdot dB + (-55.81) \\
 &= 112.83 + 2.89 \cdot dB
\end{aligned}
$$

Bei der Regressionslinie für eine Dauer von 150 ms hingegen liegt das Intercept bei 112.83 und die Steigung bei 2.89. Die Slope ist also für die beiden Regressionslinien dieselbe, ergo sind diese Linien parallel zueinander. Die beiden berechneten Intercepts und die Slope können nun für `geom_abline()` benutzt werden:

```{r}
high_dur <- 450
low_dur <- 150
slope <- b_1
intercept_high_dur <- k + b_1 * 0 + b_2 * high_dur
intercept_low_dur <- k + b_1 * 0 + b_2 * low_dur

ggplot(faux) + 
  aes(x = dB, y = f0, col = dur) + 
  geom_point() + 
  xlim(0, 95) +
  ylim(0, 500) +
  geom_abline(slope = slope, intercept = intercept_high_dur, color = "#56B1F7", size = 1.2) +
  geom_abline(slope = slope, intercept = intercept_low_dur, color = "#132B43", size = 1.2) +
  geom_vline(xintercept = 0, lty = "dashed")
```

Wir hätten natürlich auch `dur` auf die x-Achse setzen und `dB` durch ein Farbkontinuum darstellen können. Dann müssten wir zwei beispielhaft extreme *Lautstärke*werte wählen, wieder alle Werte (Intercept, Slopes, gewählte Beispielwerte für `dB`) in die Formel einsetzen und kämen auf folgendes Ergebnis:

```{r}
high_dB <- 75
low_dB <- 45
intercept_high_dB <- k + b_1 * high_dB + b_2 * 0
intercept_low_dB <- k + b_1 * low_dB + b_2 * 0
slope <- b_2

ggplot(faux) + 
  aes(x = dur, y = f0, col = dB) + 
  geom_point() + 
  xlim(0, 600) +
  ylim(0, 500) +
  geom_abline(slope = slope, intercept = intercept_high_dB, color = "#56B1F7", size = 1.2) +
  geom_abline(slope = slope, intercept = intercept_low_dB, color = "#132B43", size = 1.2) +
  geom_vline(xintercept = 0, lty = "dashed")
```

Wenn Sie sich ausreichend Zeit genommen haben, um die Regressionskoeffizienten zu verstehen und zu interpretieren, können wir uns jetzt den Statistiken und Gütekriterien des Modells zuwenden. Mit `tidy()` schauen wir uns die $t$-Statistik samt $p$-Wert an, die aussagen, ob sich die Regressionskoeffizienten signifikant von Null unterscheiden:

```{r}
lm1 %>% tidy()
```

Die beiden Regressionskoeffizienten für die unabhängigen Variablen unterscheiden sich laut den Tests signifikant von Null, d.h. sowohl die Lautstärke als auch die Dauer scheinen gute Prädiktoren für die Grundfrequenz zu sein. Die Gütekriterien sind wieder die $F$-Statistik sowie der $R^2$-Wert:

```{r}
lm1 %>% glance() %>% pivot_longer(cols = c(r.squared:nobs))
```

Die beiden unabhängigen Variablen beschreiben 66% der Varianz in den Grundfrequenzwerten. Allgemein betrachtet ist das ein relativ hoher Wert. Noch mehr Bedeutung bekommt der Wert jedoch im Vergleich zu den $R^2$-Werten aus den zwei einfachen linearen Regressionen, die wir mittels der unabhängigen Variablen erstellen können:

```{r}
lm(f0 ~ dB, data = faux) %>% glance() %>% pull(r.squared)
lm(f0 ~ dur, data = faux) %>% glance() %>% pull(r.squared)
```

Das heißt, ein Modell mit der Lautstärke als einziger unabhängiger Variable beschreibt nur knapp 10% der Varianz in den Grundfrequenzwerten, ein Modell mit der Dauer beschreibt ca. 28% dieser Varianz. Wenn man jedoch beide Prädiktoren in einem Modell (ohne Interaktion) vereint, steigt dieser Anteil auf 66%.

Zuletzt sollten wir nicht vergessen, das Ergebnis der Regression zu berichten: **Eine multiple lineare Regression mit Lautstärke und Dauer als unabhängige Variablen zeigte einen signifikanten Effekt von Lautstärke ($t$ = 45.7, $p$ < 0.001) und Dauer ($t$ = 55.7, $p$ < 0.001) auf die Grundfrequenz. Das gewählte Modell beschreibt die Daten besser als ein Modell ohne Prädiktoren ($R^2$ = 0.66, $F$[2, 1866] = 1815, $p$ < 0.001).** In einer wissenschaftlichen Veröffentlichung sollten Sie natürlich noch ein paar Zeilen darauf verwenden zu beschreiben, in welche Richtung die signifikanten Effekte gehen (d.h. ob sie die abhängige Variable positiv oder negativ, stark oder schwach beeinflussen) und ob das den Erwartungen (Hypothesen) entspricht, die Sie idealerweise vor der Datenerhebung aufgestellt haben.

*(Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!)*

## Mit Interaktion

Da wir in unserer ersten Abbildung der Daten in `faux` festgestellt haben, dass es eine Interaktion zwischen Lautstärke und Dauer geben könnte, wollen wir das Modell jetzt *mit* Interaktion berechnen. Interaktionen in einem linearen Modell können auf zwei Arten geschrieben werden: entweder mit einem Asterisk `dB * dur` oder in der ausformulierten Variante `dB + dur + dB:dur`. Bei drei unabhängigen Variablen `A`, `B` und `C` können sowohl Interaktionen zwischen zwei als auch zwischen allen drei Faktoren vorliegen, also: `A * B * C` oder `A + B + C + A:B + A:C + B:C + A:B:C`. Es kann auch sinnvoll sein, eine Interaktion nur zwischen zwei von drei Faktoren zu berechnen, z.B. `A * B + C` oder `A + B + C + A:B`.

Hier wählen wir die Kurzschreibweise und schauen uns zuerst wieder die Schätzungen der Regressionskoeffizienten an:

```{r}
lm2 <- lm(f0 ~ dB * dur, data = faux)
lm2 %>% 
  tidy() %>% 
  select(term, estimate)
```

Der y-Achsenabschnitt für `dB = 0` und `dur = 0` liegt jetzt bei 86 Hz. Die Slopes beschreiben nun den Einfluss einer unabhängigen Variable, wenn alle anderen unabhängigen Variablen konstant bei Null gehalten werden. Die Lautstärke `dB` hat auch in diesem Modell einen positiven Einfluss auf die Grundfrequenz mit einer Steigung von 4.3 Hz, d.h. die Grundfrequenz steigt mit der Lautstärke (wenn die Dauer konstant bei Null gehalten wird). Die Steigung für `dur` hingegen ist mit -0.09 Hz negativ, d.h. für höhere Dauerwerte sinkt die Grundfrequenz (wenn die Lautstärke konstant bei Null gehalten wird). 

Nun gibt es allerdings noch eine weitere Slope, nämlich für die Interaktion `dB:dur`. Diese Slope findet sich auch in der für Interaktionen angepassten Formel für die Regressionslinie:

$y = k + b_1x_1 + b_2x_2 + b_3(x_1 \cdot x_2)$

Die Slope $b_3(x_1 \cdot x_2)$ deutet schon an, dass die Interaktion nur wichtig für die Regressionslinie ist, wenn sowohl $x_1$ als auch $x_2$ ungleich Null ist: denn wenn eine von beiden Null ist, ergibt die Multiplikation $x_1 \cdot x_2$ Null, d.h. die Slope $b_3$ muss mit Null multipliziert werden und fällt somit aus der Formel raus. 

In unserem Modell ist die Steigung für die Interaktion der beiden Prädiktoren negativ. Das lässt sich so interpretieren, dass die Grundfrequenz sinkt, wenn sowohl die Lautstärke als auch die Dauer steigen. Um diese Zusammenhänge genau nachvollziehen zu können, holen wir uns aus dem obigen Data Frame das Intercept $k$, die beiden Slopes $b_1$ (für die Lautstärke) und $b_2$ (für die Dauer), sowie die Interaktion $b_3$:

```{r}
k <- lm2 %>% tidy() %>% filter(term == "(Intercept)") %>% pull(estimate)
k
b_1 <- lm2 %>% tidy() %>% filter(term == "dB") %>% pull(estimate)
b_1
b_2 <- lm2 %>% tidy() %>% filter(term == "dur") %>% pull(estimate)
b_2
b_3 <- lm2 %>% tidy() %>% filter(term == "dB:dur") %>% pull(estimate)
b_3
```

Nun können Sie diese Werte zusammen mit frei gewählten Werten für $x_1$ (Lautstärke) und $x_2$ (Dauer) in die obige Formel einsetzen, um herauszufinden, wie hoch die Grundfrequenz für die gewählten Werte der unabhängigen Variablen sein sollte. Nehmen wir an $x_1$ und $x_2$ sind beide Null, dann gilt wieder, dass der geschätzte y-Wert genau dem Intercept $k$ entspricht:

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot dur + b_3(dB \cdot dur) \\ 
 &= 86.02 + 4.30 \cdot 0 + (-0.092 \cdot 0) + (-0.0047 \cdot 0 \cdot 0) \\
 &= 86.02
\end{aligned}
$$

Genauso kann man anstatt der beiden Nullen verschiedene andere Werte für $x_1$ und $x_2$ einsetzen. Als Beispiel wählen wir hier $x_1 = 1$ und $x_2 = 1$, um zu zeigen, dass dann auch die Slope für die Interaktion der beiden Variablen zum Tragen kommt:

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot dur + b_3(dB \cdot dur) \\ 
 &= 86.02 + 4.30 \cdot 1 + (-0.092 \cdot 1) + (-0.0047 \cdot 1 \cdot 1) \\
 &= 90.22
\end{aligned}
$$

Oder, indem wir die extrahierten Regressionskoeffizienten nutzen:

```{r}
k + b_1 + b_2 + b_3
```

Auch hier können wir wieder `predict()` einsetzen, um uns für verschiedene Kombinationen von `dB`- und `dur`-Werten den `f0`-Wert schätzen zu lassen:

```{r}
d2 <- data.frame(dB  = c(0, 1, 0, 1, mean(faux$dB)), 
                 dur = c(0, 0, 1, 1, mean(faux$dur)))
d2 %<>% mutate(estimated_f0 = predict(lm2, d2))
d2
```

Nun können wir wieder die beispielhaften Regressionslinien nachvollziehen, die wir oben im rechten Plot gesehen haben. Wir wählen 450 ms und 150 ms als beispielhafte Dauerwerte:

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot dur + b_3(dB \cdot dur) \\ 
 &= 86.02 + 4.30 \cdot dB + (-0.092 \cdot 450) + (-0.0047 \cdot dB \cdot 450) \\
 &= 86.02 + 4.30 \cdot dB + (-41.45) + (-2.10 \cdot dB) \\
 &= 44.57 + 2.20 \cdot dB
\end{aligned}
$$

Und für 150 ms:

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot dur + b_3(dB \cdot dur) \\ 
 &= 86.02 + 4.30 \cdot dB + (-0.092 \cdot 150) + (-0.0047 \cdot dB \cdot 150) \\
 &= 86.02 + 4.30 \cdot dB + (-13.82) + (-0.70 \cdot dB) \\
 &= 72.20 + 3.60 \cdot dB
\end{aligned}
$$

Dieses Mal unterscheiden sich die Steigungen für die beiden extremen gewählten Dauerwerte, eben weil die Interaktion mit einbezogen wurde. Nun legen wir die berechneten Intercepts und Slopes als Variablen an und setzen diese wieder bei `geom_abline()` ein, um die Regressionslinien zu zeichnen:

```{r}
high_dur <- 450
low_dur <- 150

intercept_low_dur <- k + b_1 * 0 + b_2 * low_dur + b_3 * 0 * low_dur
intercept_low_dur
intercept_high_dur <- k + b_1 * 0 + b_2 * high_dur + b_3 * 0 * high_dur
intercept_high_dur

slope_low_dur <- b_1 + b_3 * low_dur
slope_low_dur
slope_high_dur <- b_1 + b_3 * high_dur
slope_high_dur

ggplot(faux) + 
  aes(x = dB, y = f0, col = dur) + 
  geom_point() + 
  xlim(0, 95) +
  ylim(0, 500) +
  geom_abline(slope = slope_low_dur, intercept = intercept_low_dur, color = "#132B43", size = 1.2) +
  geom_abline(slope = slope_high_dur, intercept = intercept_high_dur, color = "#56B1F7", size = 1.2) +
  geom_vline(xintercept = 0, lty = "dashed")
```


Nachdem wir die Regressionskoeffizienten verstanden haben, schauen wir uns die $t$-Statistik an und stellen fest, dass sowohl die Steigung für `dB` ($t$ = 22.6, $p$ < 0.001) als auch die für `dur` ($t$ = 2.5, $p$ < 0.05) sich signifikant von Null unterscheiden. Zusätzlich unterscheidet sich auch die Steigung für die Interaktion signifikant von Null ($t$ = 7.8, $p$ < 0.001).

```{r}
lm2 %>% tidy()
```

Das bedeutet, dass auch das gesamte Modell voraussichtlich besser ist als ein Modell ohne Prädiktoren. Wir schauen uns mit `glance()` wieder die $F$-Statistik und $R^2$ an:

```{r}
lm2 %>% glance() %>% pivot_longer(cols = c(r.squared:nobs))
```

Somit können wir berichten: **Eine multiple lineare Regression mit Lautstärke und Dauer als unabhängige Variablen mitsamt ihrer Interaktion zeigte einen signifikanten Effekt von Lautstärke ($t$ = 22.6, $p$ < 0.001) und Dauer ($t$ = 2.5, $p$ < 0.05) auf die Grundfrequenz. Zusätzlich war auch die Interaktion signifikant ($t$ = 7.8, $p$ < 0.001). Das gewählte Modell beschreibt die Daten besser als ein Modell ohne Prädiktoren ($R^2$ = 0.67, $F$[3, 1865] = 1269.6, $p$ < 0.001).**

*(Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!)*

# Kategorische unabhängige Variablen

Nun wollen wir anhand der Daten in `int` herausfinden, ob die Lautstärke in Dezibel `db` vom Vokaltyp `vowel` und dem Geschlecht der Versuchsperson `gender` beeinflusst wird.

```{r}
int
```

Erstmal schauen wir uns in einem Boxplot an, ob überhaupt eine Abhängigkeit zwischen den Variablen vorliegen könnte:

```{r}
ggplot(int) + 
  aes(x = vowel, y = db, fill = gender) + 
  geom_boxplot()
```

Der Plot zeigt, allgemein gesprochen, dass Frauen leiser sprechen als Männer. Dieser Effekt unterscheidet sich aber für die beiden Vokale: Bei /a/ ist der Effekt von Geschlecht viel ausgeprägter als bei /i/. Es könnte also eine Interaktion zwischen Vokal und Geschlecht vorliegen. Genauso können wir die Interaktion der unabhängigen Variablen für den Effekt des Vokals betrachten: Im Vokal /i/ ist die Lautstärke generell niedriger als in /a/ -- aber für Männer ist der Effekt stärker als für Frauen (d.h. der Unterschied zwischen den zwei blauen Boxen ist größer als der zwischen den zwei roten Boxen).

Obwohl hier wahrscheinlich eine Interaktion zwischen Vokal und Geschlecht vorliegt, schauen wir uns aus didaktischen Gründen erstmal an, wie eine multiple Regression mit zwei kategorialen Prädiktoren (unabhängigen Variablen) ohne Interaktion aussieht.

## Ohne Interaktion

```{r}
lm3 <- lm(db ~ vowel + gender, data = int)
lm3 %>% 
  tidy() %>% 
  select(term, estimate)
```

Wenn wir es mit *kontinuierlichen* unabhängigen Variablen zu tun haben, ist das Intercept der geschätzte y-Wert für Null in allen unabhängigen Variablen. Was genau ist aber "Null" für *kategoriale* Variablen? Dies bestimmt R durch das sogenannte **Treatment Coding** (auch oft mit dem Überbegriff *Dummy Coding* bezeichnet). Die Variable `gender` kann als Werte entweder `f` (*female*) oder `m` (*male*) annehmen. R geht dann alphabetisch vor und bestimmt, dass `f` als Null und `m` als Eins interpretiert werden soll. Genauso läuft es bei den Vokalen `vowel`, wo `a` als Null und `i` als Eins interpretiert wird. Dementsprechend ist das Intercept in dieser Regression der geschätzte Dezibel-Mittelwert für das `a` von Frauen. Dies können wir rechnerisch leicht nachvollziehen:

```{r}
k <- lm3 %>% 
  augment() %>% 
  filter(vowel == "a" & gender == "f") %>% 
  summarise(m = mean(.fitted)) %>% 
  pull(m)
k
```

Wie Sie sehen, heißen die beiden Slopes in unserem Modell `genderm` und `voweli`. Hierbei wird immer zuerst der Faktor (der Name der kategorialen Variable) genannt und dann das Level des Faktors, das *nicht* im Intercept verarbeitet wurde (also das Level `m` für `gender` und das Level `i` für `vowel`). Die Steigung `genderm` beschreibt die Lautstärkeveränderung von Frauen zu Männern für den Vokal /a/. Diese ist mit 30.1 dB positiv, d.h. Männer produzieren das /a/ deutlich lauter als Frauen. Die Steigung `voweli`beschreibt die Lautstärkeveränderung von /a/ zu /i/ für Frauen und ist mit -20.12 dB negativ, d.h. Frauen produzieren das /i/ leiser als das /a/. Dies stimmt mit unserem Eindruck aus den Boxplots überein.

Wenn wir die Lautstärke-Mittelwerte für alle vier Kombinationen für die Levels von `vowel` und `gender` schätzen wollen, brauchen wir wieder die beiden Slopes $b_1$ (für den Vokal /i/) und $b_2$ (für Männer) aus dem Ergebnis von `tidy()`, sowie das Intercept $k$, das wir oben schon berechnet haben. Erinnern Sie sich außerdem, dass durch das Treatment Coding festgelegt wurde, dass `a = 0`, `i = 1`, `female = 0` und `male = 1`.

```{r}
b_1 <- lm3 %>% tidy() %>% filter(term == "voweli") %>% pull(estimate)
b_1
b_2 <- lm3 %>% tidy() %>% filter(term == "genderm") %>% pull(estimate)
b_2
```

Geschätzte Lautstärke für das /a/ ($x_1 = 0$) von Frauen ($x_2 = 0$):

$$
\begin{aligned}
 y &= k + b_1 \cdot x_1 + b_2 \cdot x_2 \\ 
 &= 53.23 + (-20.12 \cdot 0) + 30.09 \cdot 0 \\
 &= 53.23
\end{aligned}
$$

... für das /a/ ($x_1 = 0$) von Männern ($x_2 = 1$):

$$
\begin{aligned}
 y &= k + b_1 \cdot x_1 + b_2 \cdot x_2 \\ 
 &= 53.23 + (-20.12 \cdot 0) + 30.09 \cdot 1 \\
 &= 83.31
\end{aligned}
$$

... für das /i/ ($x_1 = 1$) von Frauen ($x_2 = 0$):

$$
\begin{aligned}
 y &= k + b_1 \cdot x_1 + b_2 \cdot x_2 \\ 
 &= 53.23 + (-20.12 \cdot 1) + 30.09 \cdot 0 \\
 &= 33.11
\end{aligned}
$$

... und für das /i/ ($x_1 = 1$) von Männern ($x_2 = 1$):

$$
\begin{aligned}
 y &= k + b_1 \cdot x_1 + b_2 \cdot x_2 \\ 
 &= 53.23 + (-20.12 \cdot 1) + 30.09 \cdot 1 \\
 &= 63.19
\end{aligned}
$$

Die `predict()`-Funktion kann uns diese Rechenarbeit aber auch abnehmen:

```{r}
d3 <- data.frame(gender = c("f", "f", "m", "m"), 
                 vowel = c("a", "i", "a", "i"))
d3 %<>% mutate(estimated_mean_dB = predict(lm3, d3))
d3
```

Vergleichen Sie diese geschätzen Mittelwerte auch mit den Boxplots, die wir zuvor gemacht haben oder mit den tatsächlichen Mittelwerten (die Sie mit *tidyverse*-Funktionen berechnen können). Wir streben natürlich danach, dass die Schätzungen möglichst genau sind, denn das bedeutet, dass das von uns gewählte Modell sehr gut zu den Daten passt. In diesem Fall scheinen die Schätzungen schon recht gut zu sein:

```{r}
int %>%
  group_by(gender, vowel) %>% 
  summarise(m = mean(db))
```

<div class="green">
**Weiterführende Infos: Dummy Coding**

Wenn eine der unabhängigen Variablen mehr als zwei Stufen gehabt hätte (also wenn z.B. noch ein dritter Vokal /o/ im Datensatz vorkäme), dann gäbe es eine weitere Slope für diese Stufe (also z.B. `vowelo`). Es gibt auch noch andere Arten des Dummy Coding, z.B. *Sum Coding*, wie Sie im Kapitel 7 in Winter (2020) nachlesen können.
</div>

Jetzt schauen wir uns erst noch die statistischen Ergebnisse unserer multiplen Regression an:

```{r}
lm3 %>% tidy()
```

Diese Übersicht zeigt, dass es einen signifikanten Einfluss von Geschlecht ($t$ = 8.0, $p$ < 0.001) und von Vokal ($t$ = 5.3, $p$ < 0.001) auf die Lautstärke gab, oder anders gesagt: die Steigungen für Vokal und Gender unterscheiden sich signifikant von Null und sind daher gute Prädiktoren für die Lautstärke. Nun schauen wir uns noch die Gütekriterien an:

```{r}
lm3 %>% glance() %>% pivot_longer(cols = c(r.squared:nobs))
```

Hier sehen wir, dass durch die beiden unabhängigen Variablen Geschlecht und Vokal ca. 43% der Varianz in den Lautstärkemessungen beschrieben werden kann. Der $F$-Test zeigt, dass unser Regressionsmodell die Daten erfolgreicher beschreibt als ein Modell ohne Prädiktoren ($R^2$ = 0.43, $F$[2, 117] = 45.8, $p$ < 0.001). 

*(Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!)*

## Mit Interaktion

Da wir auch in diesem Datensatz eine Interaktion zwischen Vokal und Geschlecht vermuten, binden wir sie in unsere Regression ein:

```{r}
lm4 <- lm(db ~ vowel * gender, data = int)
lm4 %>% 
  tidy() %>% 
  select(term, estimate)
```

Das Intercept beschreibt hier immer noch den geschätzten Dezibel-Mittelwert für /a/ von Frauen, der bei ca. 48 dB liegt. Der Einfluss des Vokals auf die Lautstärke ist wieder negativ, der Einfluss des Geschlechts ist positiv. Nun gibt es zusätzlich die Slope für die Interaktion `voweli:genderm`, die mit -20.2 dB negativ ist. Wenn der Vokal also /i/ ist und das Geschlecht männlich, dann sinkt die Lautstärke. Im folgenden sehen Sie, wie Sie nun unter Einbeziehung der Interaktion die Lautstärke für die vier Kombinationen aus $vowel \times gender$ schätzen können (all dies leitet sich wieder aus der Formel für die Regressionslinie mit Interaktion ab):

```{r}
k <- lm4 %>% tidy() %>% filter(term == "(Intercept)") %>% pull(estimate)
b_1 <- lm4 %>% tidy() %>% filter(term == "genderm") %>% pull(estimate)
b_2 <- lm4 %>% tidy() %>% filter(term == "voweli") %>% pull(estimate)
b_3 <- lm4 %>% tidy() %>% filter(term == "voweli:genderm") %>% pull(estimate)

# female-a
k
# male-a
k + b_1
# female-i
k + b_2
# male-i
k + b_1 + b_2 + b_3
```

Die `predict()`-Funktion kann Ihnen hier aber auch wieder die Rechenarbeit abnehmen:

```{r}
d4 <- data.frame(gender = c("f", "f", "m", "m"), 
                 vowel = c("a", "i", "a", "i"))
d4 %<>% mutate(estimated_mean_dB = predict(lm4, d4))
d4
```

Durch die Einbindung der Funktion in das Modell sind die Lautstärke-Schätzungen im Vergleich zum Modell vorher präziser geworden! Tatsächlich stimmen die Schätzungen perfekt mit den tatsächlichen Mittelwerten überein (das wird im "echten Leben" so gut wie niemals vorkommen):

```{r}
int %>% 
  group_by(gender, vowel) %>% 
  summarise(m = mean(db))
```

Nun schauen wir uns die $t$-Statistik an, die zeigt, ob unsere Regressionskoeffizienten dazu beitragen, die Varianz in den Dezibelwerten zu erklären:

```{r}
lm4 %>% tidy()
```

Offensichtlich unterscheiden sich die Slope für Geschlecht ($t$ = 7.7, $p$ < 0.001) und (fast) die Slope für Vokal ($t$ = 1.9, $p$ = 0.06) (fast) signifikant von Null. Zusätzlich gab es eine signifikante Interaktion zwischen den Faktoren ($t$ = 2.8, $p$ < 0.01).

Oben haben wir festgestellt, dass unser Modell mit Interaktion die Daten besser beschreibt als das vorherige Modell ohne Interaktion. Dies zeigt sich auch in der $F$-Statistik:

```{r}
lm4 %>% glance() %>% pivot_longer(cols = c(r.squared:nobs))
```

Nun werden ca. 46% der Varianz in den Daten durch die beiden Faktoren und ihre Interaktion beschrieben. Das Modell mit der Interaktion beschreibt also mehr Varianz als das Modell ohne Interaktion; d.h. es liefert präzisere Schätzungen der Regressionskoeffizienten. Der $F$-Test zeigt außerdem, dass das Regressionsmodell die Daten besser beschreibt als das Intercept-only Modell ($R^2$ = 0.46, $F$[3, 116] = 34.7, $p$ < 0.001).

*(Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!)*

### post-hoc Tests mit emmeans

Wenn in einer multiplen Regression mit mindestens zwei kategorialen unabhängigen Variablen die Interaktion einen signifikanten Effekt auf die gemessene Variable hat, dann sollte man sogenannte **post-hoc Tests** ausführen. Dafür eignet sich das Paket [`emmeans`](https://cran.r-project.org/web/packages/emmeans/vignettes/basics.html) mit der gleichnamigen Funktion außerordentlich gut. Für dieses Paket gibt es mehrere ausführliche Vignetten. 

Die Funktion `emmeans()` führt $t$-Tests auf alle Kombinationen der Werte der kategorialen unabhängigen Variablen aus. So finden wir heraus, welche Kombination(en) dafür sorgen, dass die Interaktion in der Regression signifikant war. Als Argumente bekommt die Funktion das Ergebnis des Regressionsmodells, und dann die Formel `pairwise ~ vowel | gender` (sprich: *pairwise comparisons for vowel given gender*). Die Formel bedeutet hier, dass pro Geschlecht der Unterschied zwischen den Vokalen und dessen Signifikanz ermittelt werden soll.

```{r}
emmeans(lm4, pairwise ~ vowel | gender)
```

Das Ergebnis dieser Funktion sind zwei Tabellen, eine namens `emmeans` und eine namens `contrasts`. In der Tabelle `emmeans`, Spalte `emmean` finden Sie unsere Schätzungen für die Mittelwerte wieder, zuerst für Frauen (`gender = f`), dann für Männer (`gender = m`). `emmeans` steht für *estimated marginal means*. Sie finden hier zusätzlich den Standard Error `SE`, die Freiheitsgrade `df`, sowie das 95%-Konfindenzinterval `lower.CL` und `upper.CL`.

In der Tabelle `contrasts` finden Sie die Differenzen zwischen /a/ und /i/, zuerst für Frauen, dann für Männer. Zuerst wird Ihnen hierfür wieder ein `estimate` angezeigt. Dieser ist ganz einfach die Differenz aus den `emmeans` für /a/ und /i/ pro Geschlecht. Es folgen Standard Error `SE` und dann die Freiheitsgrade `df`, der $t$-Wert `t.ratio` und der $p$-Wert `p.value`. Hier sehen wir im Einklang mit den Boxplots, die wir ganz zu Anfang dieses Kapitels erstellt haben, dass es keinen signifikanten Lautstärke-Unterschied zwischen /a/ und /i/ für Frauen gibt ($t$[116] = 1.9, $p$ = 0.06), sehr wohl aber für Männer ($t$[116] = 5.8, $p$ < 0.001). Nun sollten wir zusätzlich schauen, ob es pro Vokaltyp signifikante Unterschiede zwischen Frauen und Männern gab. Dafür verändern wir einfach die Formel zu `pairwise ~ gender | vowel`:

```{r}
emmeans(lm4, pairwise ~ gender | vowel)
```

Hier können wir berichten, dass sowohl für /a/ ($t$[116] = 7.7, $p$ < 0.001) als auch für /i/ ($t$[116] = 3.8, $p$ < 0.001) ein signifikanter Unterschied zwischen Männern und Frauen vorliegt. Das stimmt ebenfalls mit unserem visuellen Eindruck aus den anfänglichen Boxplots überein.

# Mix aus kontinuierlichen und kategorischen unabhängigen Variablen

Zuletzt beschäftigen wir uns mit der Frage, ob die Grundfrequenzwerte im Data Frame `vlax` vom Vokal `vowel` und von der Lautstärke `dB` beeinflusst werden. Hier haben wir also ein Regressionsmodell mit einer kategorischen und einer kontinuierlichen unabhängigen Variable. Bei einer Abbildung bietet es sich an, die kontinuierliche unabhängige Variable auf die x-Achse zu packen und die Levels der kategorialen Variable durch Farben darzustellen. Hier zeigen wir zusätzlich die Regressionslinien, die `geom_smooth()` für uns berechnen kann.

```{r}
ggplot(vlax) + 
  aes(x = dB, y = f0, col = vowel) + 
  geom_point() + 
  geom_smooth(method = "lm", se = F)
```

Im Allgemeinen scheint es auch hier den bekannten positiven linearen Zusammenhang zwischen `f0` und `dB` zu geben. Dieser Zusammenhang scheint allerdings nur für den Vokal /I/, nicht aber für den Vokal /O/ zu existieren. Auch hier liegt also vermutlich eine Interaktion zwischen den beiden unabhängigen Variablen vor, wir berechnen aber zuerst zu Demonstrationszwecken ein Modell ohne Interaktion. 

## Ohne Interaktion

```{r}
lm5 <- lm(f0 ~ dB + vowel, data = vlax)
lm5 %>% 
  tidy() %>% 
  select(term, estimate)
```

Auch hier wird die kategoriale Variable `vowel` mittels Dummy Coding hinter den Kulissen umgewandelt. Wir sehen, dass die zweite Slope `vowelO` heißt, das bedeutet, dass der Vokal "I" schon im Intercept verarbeitet wurde. Wenn also der Vokal "I" ist und `dB = 0`, dann sollte die Grundfrequenz bei -258.9 Hz liegen. Der Einfluss von Lautstärke auf Grundfrequenz ist wie erwartet ein positiver, d.h. mit jeder Steigerung der Lautstärke um 1 dB steigt die Grundfrequenz um 6.2 Hz (dies sehen Sie z.B. im Vergleich der ersten beiden Zeilen in `d5`). Wenn der Vokal "O" anstatt "I" ist, sinkt die Grundfrequenz um 22.9 Hz (vgl. erste und dritte Zeile in `d5`). Bei durchschnittlicher Lautstärke liegt die Grundfrequenz bei der Produktion von "I" bei 200 Hz und bei der Prodution von "O" bei 177.1 Hz (s. Zeilen vier und fünf in `d5`).

```{r}
d5 <- data.frame(dB = c(0, 1, 0, mean(vlax$dB), mean(vlax$dB)), 
                 vowel = c("I", "I", "O", "I", "O"))
d5 %<>% mutate(estimated_f0 = predict(lm5, d5))
d5
```

Wir können für "I" und "O" jeweils eine Regressionslinie mittels des linearen Models berechnen, indem wir uns die Regressionskoeffizienten holen und wie wie zuvor in die Formel einsetzen:

```{r}
k <- lm5 %>% tidy() %>% filter(term == "(Intercept)") %>% pull(estimate)
k
b_1 <- lm5 %>% tidy() %>% filter(term == "dB") %>% pull(estimate)
b_1
b_2 <- lm5 %>% tidy() %>% filter(term == "vowelO") %>% pull(estimate)
b_2
```

Für den Vokal "I" ($x_2 = 0$) liegt der y-Achsenabschnitt bei -258.9 Hz und die Steigung bei 6.17 Hz.

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot x_2 \\ 
 &= -258.87 + (6.17 \cdot dB) + (-22.93 \cdot 0) \\
 &= -258.87 + (6.17 \cdot dB)
\end{aligned}
$$

Für den Vokal "O" ($x_2 = 1$) liegt der y-Achsenabschnitt bei -281.8 Hz, die Steigung aber ebenfalls bei 6.17 Hz.

$$
\begin{aligned}
 y &= k + b_1 \cdot dB + b_2 \cdot x_2 \\ 
 &= -258.87 + (6.17 \cdot dB) + (-22.93 \cdot 1) \\
 &= -281.79 + (6.17 \cdot dB)
\end{aligned}
$$

Die hier berechneten Regressionslinien können nicht denen in der Abbildung der Daten entsprechen, denn die hier berechneten Linien haben dieselbe Steigung und verlaufen dementsprechend parallel zueinander, während die von `geom_smooth()` gezeichneten Linien einander schneiden. Daraus können wir schließen, dass dieses Modell ohne Interaktion nicht besonders gut zu den Daten passt.

Trotzdem zeigt die $t$-Statistik, dass sich die Regressionskoeffizienten signifikant von Null unterscheiden (Slope für Lautstärke: $t$ = 8.1, $p$ < 0.001; Slope für Vokal: $t$ = 2.6, $p$ < 0.01). Das heißt, beide Koeffizienten sind gute Prädiktoren für die Grundfrequenz in diesem Modell.

```{r}
lm5 %>% tidy()
```

Wie immer werfen wir auch noch einen Blick auf die Gütekriterien für das Modell und stellen fest, dass durch die beiden unabhängigen Variablen Vokal und Lautstärke ca. 26.6% der Varianz in den Grundfrequenzwerten beschrieben werden kann. Der $F$-Test zeigt, dass unser Regressionsmodell die Daten erfolgreicher beschreibt als ein Modell ohne Prädiktoren ($R^2$ = 0.27, $F$[2, 175] = 33.0, $p$ < 0.001).  

```{r}
lm5 %>% glance() %>% pivot_longer(cols = c(r.squared:nobs))
```

*(Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!)*

## Mit Interaktion

Nun berechnen wir das Modell mitsamt der Interaktion zwischen den unabhängigen Variablen:

```{r}
lm6 <- lm(f0 ~ dB * vowel, data = vlax)
lm6 %>% 
  tidy() %>% 
  select(term, estimate)
```

Das Dummy Coding bleibt gleich, d.h. der Vokal "I" wird als Null und der Vokal "O" als Eins gewertet. Das Intercept liegt bei -318.5 Hz. Der Einfluss von Lautstärke auf Grundfrequenz ist immer noch positiv mit ca. 7 Hz. Auffällig ist, dass sich der Koeffizient `vowelO` im Vergleich zum Modell ohne Interaktion stark verändert hat, von -22.9 zu +615.2 Hz. Das ergibt hier aber auch Sinn, wenn wir uns nochmal die Abbildung anschauen (hier haben wir die x-Achse so verändert, dass sie bei Null beginnt):

```{r}
ggplot(vlax) + 
  aes(x = dB, y = f0, col = vowel) + 
  geom_point() + 
  xlim(0, 90) +
  geom_smooth(method = "lm", se = F)
```

Wenn Sie die blaue Regressionslinie bis zur y-Achse verfolgen, kommen Sie bei knapp unter 300 Hz heraus -- das entspricht dem Intercept (`db = 0` und `vowel = "I"`) plus der Slope für `vowelO`. Dasselbe gilt für die rote Linie, die die y-Achse an einem Punkt schneidet, der hier nicht mehr abgebildet wird, nämlich bei ca. -311 Hz -- das entspricht dem Intercept plus der Slope für `dB`. Wir können diese Berechnungen wie folgt nachvollziehen (oder Sie setzen die Regressionskoeffizienten manuell in die Formel ein, wie oben gezeigt):

```{r}
k <- lm6 %>% tidy() %>% filter(term == "(Intercept)") %>% pull(estimate)
b_1 <- lm6 %>% tidy() %>% filter(term == "dB") %>% pull(estimate)
b_2 <- lm6 %>% tidy() %>% filter(term == "vowelO") %>% pull(estimate)
b_3 <- lm6 %>% tidy() %>% filter(term == "dB:vowelO") %>% pull(estimate)

# "I" bei 0 dB
k
# "I" bei 1 dB
k + b_1
# "O" bei 0 dB
k + b_2
# "O" bei 1 dB
k + b_1 + b_2 + b_3
```

Die `predict()`-Funktion kann Ihnen hier wieder die Rechenarbeit abnehmen:

```{r}
d6 <- data.frame(dB = c(0, 1, 0, 1), 
                 vowel = c("I", "I", "O", "O"))
d6 %<>% mutate(estimated_f0 = predict(lm6, d6))
d6
```

Wenn Sie sich jetzt den Unterschied zwischen Zeile 1 und 2 in `d6` anschauen, sehen Sie, dass die Lautstärke für den Vokal "I" positiv mit der Grundfrequenz korreliert ist, denn mit steigender Lautstäke (von 0 zu 1 dB) steigt auch die geschätzte Grundfrequenz. Für den Vokal "O" hingegen *sinkt* die Grundfrequenz mit steigender Lautstärke (vgl. Zeilen 3 und 4 in `d6`), d.h. die Korrelation zwischen Lautstärke und Grundfrequenz ist negativ für den Vokal "O" -- das entspricht auch den Regressionslinien im obigen Plot sowie den wie folgt berechneten Korrelationswerten $r$:

```{r}
vlax %>% 
  group_by(vowel) %>% 
  summarise(r = cor(dB, f0))
```

Nun schauen wir uns die $t$-Statistik an, die zeigt, ob unsere Regressionskoeffizienten dazu beitragen, die Grundfrequenzwerte zuverlässig zu schätzen:

```{r}
lm6 %>% tidy()
```

Die Slope für Laustärke ($t$ = 9.0, $p$ < 0.001) und die Slope für Vokal ($t$ = 3.2, $p$ < 0.01) unterscheiden sich signifikant von Null und eignen sich dementsprechend gut als Prädiktoren für die Grundfrequenz. Zusätzlich gab es eine signifikante Interaktion zwischen den Faktoren ($t$ = 3.3, $p$ < 0.001).

Die $F$-Statistik zeigt, dass das Regressionsmodell mit den beiden Prädiktoren und ihrer Interaktion die Daten besser beschreibt als ein Mittelwertmodell ($R^2$ = 0.31, $F$[3, 174] = 27.0, $p$ < 0.001). Anhand des $R^2$-Werts sehen wir, dass das Modell nun auch mehr Varianz in den Grundfrequenzwerten beschreibt als im Modell ohne Interaktion, d.h. die Erklärungskraft des Modells ist durch die Hinzunahme der Interaktion gestiegen.

```{r}
lm6 %>% glance() %>% pivot_longer(cols = c(r.squared:nobs))
```

Für dieses Modell berechnen wir trotz der signifikanten Interaktion zwischen den unabhängigen Variablen keine post-hoc Tests, weil eine der unabhängigen Variablen kontinuierlich ist. Die post-hoc Tests mit `emmeans` ergeben nur Sinn für mehrere kategoriale Variablen.

*(Natürlich würden wir unter realistischen Bedingungen hier noch die Annahmen über die Residuals prüfen!)*


# Packages und Daten laden

Installieren Sie die folgenden Pakete:

```{r, eval = F}
install.packages(c("pbkrtest", "MuMIn", "lmerTest"))
```

Laden Sie die Pakete und Data Frames:

```{r}
library(emmeans)
library(lmerTest)
library(MuMIn)
library(tidyverse)
library(magrittr)
url <- "http://www.phonetik.uni-muenchen.de/~jmh/lehre/Rdf"
df <- read.table(file.path(url, "int_new.txt"), stringsAsFactors = T, header = T) %>% 
  mutate(subject = factor(subject, levels = paste0("S", 1:10))) %>% 
  as_tibble()
```

# Mixed Models (LMERs): Einführung

Mit einem Linear Mixed Effect Model (auch: Linear Mixed Effect Regression oder LMER) wird geprüft, ob eine gemessene abhängige Variable von einer oder mehreren unabhängigen Variablen beeinflusst wird. Diese Art von Modellen wird als "gemischt" (*mixed*) bezeichnet, weil sowohl Variablen, deren Einfluss auf die abhängige Variable für die Forschenden interessant ist (**Fixed Effects**), als auch Variablen, deren Einfluss uninteressant ist (**Random Effects**), Teil des Modells sein können. Die Fixed Effects in einem Mixed Model können sowohl kategorial als auch numerisch sein, die Random Effects ausschließlich kategorial.

Damit erweitern wir die einfache und multiple lineare Regression, die nur Fixed Effects als Prädiktoren genutzt haben. In der Phonetik und Linguistik werden häufig die StudienteilnehmerInnen und Items oder Wörter als Random Effects im LMER verwendet. Das liegt daran, dass wir eine *zufällige* Auswahl an Probanden und/oder Wörtern einer bestimmten Sprache auswählen und hoffen, dass sich die Ergebnisse auf andere Probanden und/oder Wörter derselben Sprache übertragen lassen. Dieser zufällige Auswahlprozess hat zur Folge, dass wir *noise* in unser Experiment bringen, das wir dann mittels LMER wieder "herausrechnen" können. Während wir also für Fixed Effects einen vorhersagbaren Einfluss auf die abhängige Variable vermuten, ist der Einfluss der Random Effects nicht vorhersagbar, d.h. *random*.

Schauen wir uns den Data Frame `df` an, den wir bereits aus früheren Kapiteln kennen (für diese Vorlesung wurde er leicht verändert):

```{r}
df
```

Hier haben wir die SprecherInnen in der Spalte `subject` und die produzierten Wörter in der Spalte `word`. Uns interessiert, ob der Vokal `vowel` und das Geschlecht `gender` einen Einfluss auf die Lautstärke `db` haben. Für diese Fragestellung schauen wir uns die Daten in einer Abbildung an:

```{r}
ggplot(df) + 
  aes(x = vowel, y = db, fill = gender) + 
  geom_boxplot()
```

Wir sehen (wie schon in VL 9), dass Frauen die Vokale leiser produzierten als Männer und dass /i/ etwas leiser produziert wurde als /a/. Es ist außerdem zu vermuten, dass eine Interaktion zwischen Geschlecht und Vokal vorliegt, denn der Effekt von Geschlecht ist stärker ausgeprägt für /a/ als für /i/ (bzw. umgekehrt: der Effekt von Vokal ist stärker ausgeprägt für Männer als für Frauen).

Nun wissen wir aber, dass wir mehrere ProbandInnen und verschiedene Wörter aufgenommen haben, um die Fragestellung zu untersuchen. Dabei interessiert uns nicht der genaue Einfluss der Individuen oder Wörter auf die Lautstärke, im Gegenteil: Wir wollen über die ProbandInnen und Wörter eine allgemeine Aussage ableiten, die (theoretisch) für alle möglichen ProbandInnen und Wörter gilt. Sollte es nennenswerte Lautstärke-Unterschiede zwischen den individuellen ProbandInnen und/oder den individuellen Wörtern geben, haben wir durch unsere zufällige Auswahl der ProbandInnen und Wörter *noise* verursacht. Mit Boxplots schauen wir, ob es nennenswerte Variationen der Dezibelwerte zwischen den Sprechern und/oder zwischen den Wörtern gibt:

```{r}
ggplot(df) + 
  aes(x = subject, y = db) + 
  geom_boxplot()
ggplot(df) + 
  aes(x = word, y = db) + 
  geom_boxplot()
```

Da es hier z.T. große Lautstärke-Unterschiede zwischen einzelnen Sprechern und zwischen einzelnen Wörtern gibt, ist es unbedingt notwendig, dass wir in dem Mixed Model, das wir für unsere Fragestellung konstruieren werden, mit Random Effects für diese beiden Variablen arbeiten.

Ein zweiter äußerst wichtiger Grund für Random Effects ist, dass wir häufig sogenannte *repeated measure* Experimente durchführen, bei denen z.B. mehrere Datenpunkte von demselben Probanden stammen und/oder bei denen Wörter mehrfach wiederholt wurden. Das bedeutet, dass die Datenpunkte nicht mehr **unabhängig** von einander sind (zum Beispiel: Datenpunkte von demselben Probanden werden häufig näher zusammenliegen als solche von unterschiedlichen Probanden). Dies ist jedoch eine wichtige Annahme in Regressionsmodellen, über die wir bislang nicht gesprochen haben. Wenn wir also überlegen, welches statistische Modell zu den Daten und unserer Fragestellung passen könnte, muss einbezogen werden, ob die Datenpunkte unabhängig voneinander sind oder nicht (in früheren Vorlesungen haben wir diese Annahme aus didaktischen Gründen z.T. ignoriert). Wenn die Datenpunkte nicht als unabhängig voneinander betrachtet werden können, muss ein Mixed Model verwendet werden, da sich die Random Effects der Abhängigkeiten zwischen Datenpunkten annehmen. Ein Unterschied zwischen der Unabhängigkeitsannahme und den beiden anderen Annahmen, die Sie hier bisher kennengelernt haben, ist, dass sich die Unabhängigkeitsannahme direkt auf die gemessenen Daten bezieht, während die Normalitäts- und Varianz-Annahmen sich auf die Residuals beziehen.

Bei der einfachen und multiplen linearen Regression wird ein Intercept und dann eine Slope pro unabhängiger Variable (pro Fixed Effect) geschätzt. Bei den gemischten Modellen wird das ebenfalls gemacht, aber wir können jetzt erlauben, dass das Intercept und die Slope(s) sich für die Levels der Random Effects unterscheiden. Um die Konzepte **Random Intercept** und **Random Slope** zu erklären, konzentrieren wir uns hier erstmal auf `subject` als Random Effect.

# Random Intercepts vs. Random Slopes

## Random Intercepts

Sie erinnern sich, dass mittels Treatment Coding der Vokal /a/ als Null und der Vokal /i/ als Eins interpretiert wird, genauso wird das Level `f` (*female*) der Variable `gender` als Null und `m` (*male*) als Eins interpretiert. Die Schätzung des Lautstärke-Mittelwerts für /a/ von Frauen ist folglich in unserem Mixed Model das Intercept (genauso wie vorher in der multiplen Regression). Wir haben mittels der Boxplots oben festgestellt, dass sich die Versuchspersonen hinsichtlich der Lautstärke deutlich voneinander unterscheiden. Interessant ist jetzt für uns, ob sich Versuchspersonen desselben Geschlechts auch im Lautstärke-Mittelwert für /a/ (dem im "allgemeinen" Intercept verarbeiteten Level von `vowel`) unterscheiden. Denn wenn dem so ist, dann können wir mittels eines Random Intercepts für die Variable `subject` verlangen, dass das Intercept von Person zu Person variieren darf (deshalb spricht man manchmal auch von *varying intercepts*).

In der folgenden Abbildung sehen Sie die Dezibelwerte für die Vokale /a/ und /i/, getrennt nach Versuchsperson. Die Personen S1-S5 sind männlich, die Personen S6-S10 sind weiblich. Die Kreuze in orange markieren die Dezibel-Mittelwerte pro Person und Vokal. Wenn wir uns jetzt die Mittelwerte für /a/ pro Versuchsperson innerhalb derselben Zeile anschauen (das jeweils linke orange Kreuz pro Panel), gibt es große Unterschiede, z.B. zwischen S1 und S4 oder zwischen S8 und S10. Das ist ein Indikator dafür, dass es sinnvoll ist, pro Sprecher ein Random Intercept zu berechnen.

```{r}
ggplot(df) + 
  aes(x = vowel, y = db) + 
  geom_point() + 
  facet_wrap(~subject, nrow = 2) + 
  geom_point(data = df %>% 
               group_by(subject, vowel) %>% 
               summarise(db = mean(db)),
             color = "orange", size = 3, stroke = 2, shape = 4) + 
  geom_line(data = df %>% 
              group_by(subject, vowel) %>% 
              summarise(db = mean(db)),
            aes(x = vowel, y = db, group = subject),
             lty = "dashed")
```

In der R Notation wird ein Random Intercept so geschrieben: `(1 | subject)`. Hierbei steht `1` für Intercept, ausgesprochen bedeutet die Formel so viel wie "schätze sprecherspezifische Intercepts". In der Praxis berechnet ein Mixed Model mit einem solchen Random Intercept allerdings nicht ein Intercept pro Person, sondern die *Standardabweichung* der Versuchspersonen-Intercepts vom geschätzten "allgemeinen" Intercept über alle Datenpunkte. Das ist wichtig, weil die Berechnung des Mixed Models ewig lang dauern und wahrscheinlich Fehler werfen würde, wenn wirklich pro Versuchsperson ein Intercept geschätzt würde.

## Random Slopes

In der obigen Abbildung sehen Sie neben den Mittelwerten für /a/ auch die für /i/ sowie eine gestrichelte Linie, die beide verbindet. Diese stellt hier sozusagen die ideale Regressionslinie pro Versuchsperson dar. Da die Linien ganz unterschiedlich steil verlaufen, können wir erahnen, dass die einzelnen Versuchspersonen unterschiedliche Slopes haben. Versuchsperson S7 macht z.B. kaum einen Lautstärke-Unterschied zwischen /a/ und /i/ (die Slope ist ca. 0), während Versuchsperson S6 das /a/ deutlich lauter produziert als das /i/ (die Slope ist negativ). Wir vergleichen die Slopes auch hier wieder nur innerhalb einer Geschlechtsgruppe, d.h. für S1-S5 und für S6-S10. Hier scheint es sinnvoll zu sein, auch sprecherspezifische Random Slopes zu berechnen, weil der Effekt des Vokals auf die Lautstärke unterschiedlich für die Versuchspersonen ist.

Allgemein wird davon ausgegangen, dass auch ein Random Intercept geschätzt werden soll, wenn wir Random Slopes schätzen. Daher ist die Formel für Random Slopes `(1 + vowel | subject)`, also "schätze sprecherspezifische Intercepts und sprecherspezifische Slopes in Relation zum Vokal". Häufig wird anstatt dieser Notation allerdings die Kurzform `(vowel | subject)` verwendet, da für die Funktion dann klar ist, dass sowohl Random Slope als auch Random Intercept berechnet werden soll. Auch hier wird wieder die Abweichung von der "allgemeinen" Slope geschätzt, und nicht wirklich eine Slope pro Proband.

Vielleicht fragen Sie sich jetzt, warum der Fixed Effect `gender` im Vergleich zu `vowel` hier eine irgendwie untergeordnete Rolle zu spielen scheint. Warum z.B. interessieren wir uns nicht für `(1 + gender | subject)` oder `(1 + vowel + gender | subject)`? Das liegt daran, dass die Levels der Variable `gender` nicht pro Versuchsperson variieren, denn die Versuchspersonen sind hier *entweder* männlich *oder* weiblich, wie die folgende Tabelle zeigt: 

```{r}
table(df$subject, df$gender)
```

Hier gibt es also gar keine Variation, die wir durch die entsprechende Random Slope herausrechnen könnten. Für die Stufen der Variable `vowel` hingegen liegen jeweils Werte für die Versuchspersonen vor (sie haben sowohl /a/ als auch /i/ produziert). Und das wiederum bedeutet, dass sich der Effekt von Vokal auf die Lautstärke von Versuchsperson zu Versuchsperson unterscheiden kann (der Effekt von Geschlecht auf Lautstärke hingegen kann sich nicht pro Versuchsperson unterscheiden).

```{r}
table(df$subject, df$vowel)
```

## Random Effects Struktur für `word` bestimmen

Da wir jetzt wissen, wie der Random Effect für `subject` aussieht, gehen wir dieselbe Prozedur für die Variable `word` durch. Dieses Mal fangen wir damit an, herauszufinden, ob pro Wort jeweils Messwerte von beiden Geschlechtern und von beiden Vokalen vorliegen:

```{r}
table(df$word, df$gender)
table(df$word, df$vowel)
```

Das scheint für Geschlecht, nicht aber für Vokal der Fall zu sein, d.h. der Effekt von Geschlecht auf die Lautstärke kann sich pro Wort unterscheiden. Der Effekt von Vokal auf die Lautstärke kann sich jedoch nicht von Wort zu Wort unterscheiden, weil in jedem Wort *entweder* /a/ *oder* /i/ vorkam. Die maximale Random Effects Struktur, die wir also für die Variable `word` erstellen könnten, ist `(1 + gender | word)` (sowohl Random Intercept als auch Random Slope in Relation zum Geschlecht).

Zuerst schauen wir, ob wir überhaupt ein Random Intercept für `word` benötigen (wobei die wortspezifischen Boxplots oben schon ein guter Hinweis darauf sind, dass wir ein Random Intercept benötigen). Wir nutzen wieder einen ähnlichen Plot wie zuvor, sodass wir pro Wort die Lautstärke-Mittelwerte für Frauen vergleichen können (`f` war das Level der Variable `gender`, das im Intercept verarbeitet wird). Natürlich könnten Sie diese Mittelwerte aber auch mittels *tidyverse*-Funktionen berechnen. Die Wörter in der oberen Zeile der Abbildung enthalten /a/, die in der unteren Zeile enthalten /i/.

```{r}
ggplot(df) + 
  aes(x = gender, y = db) + 
  geom_point() + 
  facet_wrap(~word) + 
  geom_point(data = df %>% 
               group_by(word, gender) %>% 
               summarise(db = mean(db)),
             color = "orange", size = 3, stroke = 2, shape = 4) + 
  geom_line(data = df %>% 
              group_by(word, gender) %>% 
              summarise(db = mean(db)),
            aes(x = gender, y = db, group = word),
             lty = "dashed")
```

Der Lautstärke-Mittelwert für Frauen (jeweils das linke orange Kreuz pro Panel) unterscheidet sich für verschiedene Wörter. Wort `w1` hat zum Beispiel einen deutlich niedrigeren Lautstärke-Mittelwert als `w3`, wenn es von Frauen produziert wurde. Auch die Mittelwerte für Frauen in Wörtern, die /i/ enthalten (`w4`-`w6`) unterscheiden sich z.T. stark voneinander. Dementsprechend ist die Berechnung eines Random Intercepts für `word` angebracht: `(1 | word)`.

Interessanterweise sind die gestrichelten Linien, die die orangen Kreuze verbinden, quasi parallel für die jeweils drei Wörter pro Zeile in der Abbildung. Das heißt, der Effekt von Geschlecht auf die Wörter `w1`, `w2` und `w3` ist gleich (oder zumindest sehr ähnlich). Gleiches gilt für die Wörter `w4`, `w5` und `w6`, die sich in der Steigung der gestrichelten Linie kaum unterscheiden. Innerhalb einer Vokalgruppe ist der Effekt von Geschlecht auf die verschiedenen Wörter also gleich. Das bedeutet, dass eine wortspezifische Random Slope in Relation zum Geschlecht `(1 + gender | word)` für diesen Datensatz nicht angebracht wäre. Es bleibt bei `(1 | word)` als Random Effect Struktur für die Variable `word`.

# LMER in R

Das klassische Paket, das für LMERs benutzt wird, heißt `lme4`. Wir nutzen hier stattdessen `lmerTest`, das aber ein Wrapper für `lme4` ist. Die Funktion für das Berechnen eines Mixed Models heißt `lmer()`. Unsere vollständige Formel enthält den üblichen ersten Teil für die Fixed Effects (inklusive Interaktion, wenn nötig), und dann die Random Effects für `subject` und `word`. Dieses Mal schauen wir uns die Ergebnisse des Mixed Models mit der Funktion `summary()` an. Die Funktion bekommt optional das Argument `corr = F`, um die Anzeige einer bestimmten Korrelationstabelle zu unterdrücken, die für uns keine Bedeutung hat.

<div class="green">
**Weiterführende Infos: LMER Ergebnisse anzeigen**

Anstelle von `summary()` können Sie sich auch das Paket [`broom.mixed`](https://cran.r-project.org/web/packages/broom.mixed/vignettes/broom_mixed_intro.html) herunterladen, das die Funktionen `tidy()`, `augment()` und `glance()` für LMERs bereitstellt.
</div>

Die Funktion `lmer()` bekommt die Formel, die wir uns mühsam erarbeitet haben, mit der bekannten Fixed Effects Struktur `db ~ gender * vowel` und dann, durch Pluszeichen verbunden, die Random Effects. Zusätzlich wird der Data Frame mit dem Argument `data` angegeben. In diesem Fall spezifizieren wir außerdem `REML = F`. `REML` steht für *Restricted Maximum Likelihood*. Indem wir der Funktion das Argument `REML = F` gegeben haben, wird nun stattdessen eine echte Maximum Likelihood Schätzung der gewünschten Parameter ausgeführt (mehr dazu später).

```{r}
df.lmer <- lmer(db ~ gender * vowel + (1 + vowel | subject) + (1 | word), data = df, REML = F)
df.lmer %>% summary(corr = F)
```

Hier wird uns zuerst angezeigt, was genau für ein Modell berechnet und welche Formel dafür verwendet wurde. Es folgt eine Aufzählung von Maßen für die *model selection*, nämlich `AIC` (*Akaike information criterion*), `BIC` (*Bayesian information criterion*) und `logLik` (logarithmierte Likelihood). Diese absoluten Werte haben keine Bedeutung; wenn sie jedoch verglichen werden mit Werten aus anderen Modellen für dieselben Daten, stehen niedrigere Werte von `AIC` und `BIC` sowie höhere Werte von `logLik` für eine bessere Passgenauigkeit des Modells in Bezug auf die Daten. `df.resid` steht für *residual degrees of freedom*; das ist die Anzahl der Datenpunkte minus die Anzahl der geschätzten Parameter. Der Data Frame `df` hat 120 Beobachtungen (Zeilen, Datenpunkte), und es wurden neun Parameter von unserem Modell geschätzt: 

- für die Fixed Effects: `(Intercept)`, `genderm`, `voweli`, `genderm:voweli`
- für die Random Effects: `subject (Intercept)`, `subject voweli`, `word (Intercept)`, `Residual`, `Corr` für die sprecherspezifische Slope in Relation zum Vokal.

Es folgen die üblichen Kennzahlen für die Residuals. Neu für uns ist die Tabelle für die Random Effects, altbekannt ist die Tabelle für die Fixed Effects. Diese beiden Tabellen schauen wir uns im Folgenden genauer an.

## Fixed Effects

![](img/fixed_effects.png)

Die Tabelle für die Fixed Effects kommt uns bekannt vor. Wir sehen die Schätzung für das Intercept und dann die Schätzungen für die Slopes `genderm`, `voweli` und die Interaktion `genderm:voweli`. Der geschätzte dB-Mittelwert für das /a/ von Frauen liegt bei 48.2 dB. Männer sprechen offenbar deutlich lauter, denn die Slope für `genderm` ist hoch und positiv. Das heißt das /a/ von Männern liegt bei ca. $48.2 + 40.2 = 88.4$ dB. Der Vokal /i/ wird hingegen leiser produziert als /a/, denn die Slope ist negativ mit -10 dB, d.h. das /i/ von Frauen wurde mit ca. $48.2 - 10.0 = 38.2$ dB produziert. Zuletzt sehen wir, dass für das /i/ von Männern nochmal 20.2 dB abgezogen werden müssen, das heißt der geschätzte Mittelwert für das /i/ von Männern liegt bei $48.2 + 40.2 + (-10.0) + (-20.2) = 58.2$ dB. 

Dazu werden uns die Standard Errors gegeben und die Ergebnisse der $t$-Statistik, die testet, ob sich die Regressionskoeffizienten signifikant von Null unterscheiden. Laut dieser Statistik hatte Geschlecht einen signifikanten Einfluss auf die Lautstärke ($t$[9.5] = 4.3, $p$ < 0.01) und die Interaktion zwischen Geschlecht und Vokal war ebenfalls signifikant ($t$[9.2] = 2.9, $p$ < 0.05). Der Vokal hatte hingegen keinen signifikanten Einfluss auf die Lautstärke. Wie Sie sehen, wird die $t$-Statistik hier mitsamt der Freiheitsgrade aus der Spalte `df` berichtet, denn die Freiheitsgrade sind der bestimmende Parameter für die Student-$t$-Verteilung. Die Freiheitsgrade sind hier ebenfalls eine Schätzung, deshalb sind die Werte häufig Dezimalzahlen.

*Da es eine signifikante Interaktion zwischen den kategorialen unabhängigen Variablen gibt, werden wir später noch post-hoc Tests mit `emmeans` ausführen.*

## Random Effects

![](img/random_effects.png)

Nun schauen wir uns die Random Effects genauer an. Wie vorhin angedeutet, werden hier nicht z.B. pro Versuchsperson je ein Intercept und eine Slope berechnet, sondern es wird die Standardabweichung der personenspezifischen Intercepts und Slopes vom geschätzten "allgemeinen" Intercept und der geschätzten "allgemeinen" Slope geschätzt. Jeder Wert in der Spalte `Std.Dev` ist deshalb ein vom Mixed Model geschätzter Parameter. Die Standardabweichung für die sprecherspezifischen Intercepts liegt hier bei 14.7 dB, d.h. die Sprechervariation um das "allgemeine" Intercept von 48.2 dB liegt bei $\pm 14.7$ dB, was in Relation recht viel ist. Wir können hier die 68-95-99.7 Regel anwenden: 95% der sprecherspezifischen Intercepts sollten im Bereich von $Intercept \pm 2 \cdot Std.Dev$ liegen, also zwischen 77.6 dB und 18.8 dB. Dieser große Wertebereich zeigt, dass es tatsächlich große Unterschiede in den Intercepts der verschiedenen Versuchspersonen gab. 

Die Sprechervariation um die "allgemeine" Slope für `voweli` von -10 dB liegt bei 10.5 dB, was eine sehr große Standardabweichung ist: das heißt, das 95% der sprecherspezifischen Slopes im Bereich von -31 dB bis 11 dB liegen, was wiederum bedeutet, dass der Vokaleffekt pro Sprecher äußerst unterschiedlich war. Das rechtfertigt nochmal die Berechnung der sprecherspezifischen Random Slope. Zusätzlich steht in der Spalte `Corr` der Korrelationswert $r$ für die Korrelation zwischen dem sprecherspezifischen Random Intercept und der sprecherspezifischen Random Slope. Da die Korrelation hier negativ ist, bedeutet das, dass Probanden mit einem höheren Intercept eine steilere negative Slope für /i/ haben. Wenn also jemand das /a/ besonders laut produzierte, produzierte die Person auch ein leiseres /i/ (Achtung: wir beschreiben hier keine Kausalität, sondern eine Korrelation!). Die Korrelation `Corr` gilt als ein weiterer Parameter, der vom Modell geschätzt wurde.

# Konvergenzprobleme und Modell vereinfachen

Beim Berechnen von LMERs treten regelmäßig sogenannte Konvergenzprobleme auf. Der häufigste Fehler ist folgender:

![](img/konvergenz_error.png)

Dieser Fehler bedeutet, grob gesagt, dass das gewünschte Modell nicht geschätzt werden konnte. Das liegt meist daran, dass man unnötig komplexe Random Effect Strukturen und/oder insgesamt zu viele unabhängige Variablen und Interaktionen eingebaut hat. Man sollte sich also wirklich gut überlegen, welche Formel man für das Mixed Model verwendet; jeder Fixed Effect, jede Interaktion und jeder Random Effect sollte sinnvoll sein für die Daten und für die Fragestellung.

Zuvor haben wir festgestellt, dass es für die Daten in `df` nicht sinnvoll wäre, eine Random Slope für Geschlecht gegeben Wort `(1 + gender | word)` schätzen zu lassen. Das machen wir jetzt trotzdem, um zu zeigen, wie man mit dem auftretenden Fehler umgehen sollte.

```{r}
df.wrong <- lmer(db ~ gender * vowel + (1 + vowel | subject) + (1 + gender | word), data = df, REML = F)
df.wrong %>% summary(corr = F)
```

Durch die Einführung der wortspezifischen Random Slope ist tatsächlich ein Konvergenzproblem entstanden -- und trotzdem wird uns das Ergebnis angezeigt. Dieses Ergebnis ist jedoch nicht vertrauenswürdig und darf unter keinen Umständen berichtet werden.

Schauen wir uns den Random Effect `word` an. Hier wurde zuerst ein Random Intercept geschätzt, für das eine Standardabweichung von 14 dB geschätzt wurde. So weit so gut. Dann sehen wir aber, dass die Random Slope eine extrem kleine Standardabweichung hat mit nur 0.3 dB. Das bedeutet, dass das Wort gar nicht mit dem Geschlecht der Versuchspersonen variiert (das hatten wir zuvor mittels einer Abbildung festgestellt). Das Mixed Model hat aber trotzdem versucht, die Random Slope und die Korrelation zwischen Random Intercept und Random Slope zu schätzen -- und ist gescheitert. Das sehen Sie daran, dass $r = -1$ (auch bei $r = 1$) in der Spalte `Corr`, denn eine perfekte Korrelation gibt es nicht. Die 1 oder -1 zeigt, dass die Korrelation nicht geschätzt werden konnte.

Für solche Fälle gibt es die `step()` Funktion aus dem `lmerTest` Paket. Diese Funktion schaut sich alle Fixed und Random Effects Strukturen im Modell an und berechnet, welche davon signifikant sind und damit etwas Essentielles zum Modell beitragen und welche nicht. Die Variablen, die nichts beitragen, werden eliminiert. So bleibt am Ende ein Modell, das nur die statistisch relevanten Variablen mit einbezieht. Das heißt aber im übrigen *nicht*, dass Sie nicht trotzdem Variablen im Modell behalten können, die nicht signifikant sind (solange das Modell dann noch konvergiert)! Hier sehen Sie das Ergebnis von `step()`:

```{r}
df.step <- df.wrong %>% step()
df.step
```

Es kommt vor, dass die Funktion mehrfach den obigen Fehler wirft, und zwar immer dann, wenn sie ein neues Modell mit einer angepassten Formel berechnet und der Fehler immer noch da ist. Zuerst sehen wir im Ergebnis die *Backward reduced random-effect table*. Die *backward reduction* ist ein Prozess, bei dem zuerst das komplexeste Modell ausprobiert wird und dann werden nach und nach die nicht signifikanten Terme entfernt. Hier sehen wir, dass die Random Slope `gender` in dem Random Effect `(1 + gender | word)` nicht signifikant ist, und deshalb entfernt wurde. Es bleibt daher nur noch das Random Intercept `(1 | word)` übrig. Auch dieses wurde nochmal evaluiert, in Zeile 3, und für wichtig befunden. Auch die Random Slope `vowel` in `(1 + vowel | subject)` kann bleiben. Es folgt dieselbe Prozedur für die Fixed Effects.

Zuletzt zeigt uns `step()` an, welches Modell es schließlich ausgewählt hat, nämlich:

`db ~ gender + vowel + (1 + vowel | subject) + (1 | word) + gender:vowel`

Unsere Fixed Effects werden hier ausgeschrieben als `gender + vowel + ... gender:vowel`. Random Intercept und Slope für Versuchspersonen sind unverändert, und für den Random Factor `word` wird jetzt nur noch ein Intercept geschätzt. Also erhalten wir hier genau das Modell, das wir zuvor schon berechnet haben. Mit der Funktion `get_model()`, angewendet auf das Ergebnis der `step()`-Funktion, können wir uns die Ergebnisse des vereinfachten Modells anzeigen lassen:

```{r}
df.lmer.new <- df.step %>% get_model()
df.lmer.new %>% summary(corr = F)
```

# Ergebnisse berichten

Wir kennen die Ergebnisse der $t$-Tests für die Regressionskoeffizienten und haben festgestellt, dass die Interaktion zwischen den beiden kategorialen unabhängigen Variablen signifikant war. Deshalb führen wir hier der Vollständigkeit halber die paarweisen Vergleiche mit `emmeans` aus:

```{r}
emmeans(df.lmer.new, pairwise ~ vowel | gender)$contrasts
```

```{r}
emmeans(df.lmer.new, pairwise ~ gender | vowel)$contrasts
```

Wenn Sie Ihre Ergebnisse berichten, beschreiben Sie auch das Modell ausführlich; in einer wissenschaftlichen Publikation verwenden Sie zusätzlich einige Zeilen darauf, die Richtung der Fixed Effects zu beschreiben (z.B. "Das Mixed Model zeigte, dass /a/ lauter produziert wurde als /i/, insbesondere von männlichen Probanden") und ob das mit Ihren Erwartungen/Hypothesen übereinstimmt.

Hier ist ein Bespiel für einen Ergebnisbericht zu der Fragestellung: Wird die Lautstärke von dem Geschlecht und dem Vokal beeinflusst?

**Es wurde eine Linear Mixed Effect Regression durchgeführt mit der Lautstärke in Dezibel als abhängiger Variable, sowie mit Fixed Effects für Geschlecht und Vokal inklusive der Interaktion zwischen den unabhängigen Variablen, und den Random Effects für Versuchspersonen und Wörter. Für Versuchspersonen wurden sowohl Random Intercept als auch Random Slope und für Wörter nur das Random Intercept geschätzt. Das Modell ergab einen signifikanten Einfluss von Geschlecht ($t$[9.5] = 4.3, $p$ < 0.01) auf die Lautstärke. Es gab zusätzlich eine signifikante Interaktion zwischen Geschlecht und Vokal ($t$[9.2] = 2.9, $p$ < 0.05). Post-hoc t-Tests zeigten eine signifikante Differenz zwischen Frauen und Männern für /a/ ($t$[10.8] = 4.0, $p$ < 0.01), jedoch nicht für /i/.**

# Gütekriterien für Mixed Models

## Marginal und Conditional $R^2$

Sie haben vielleicht bemerkt, dass uns die `summary()` Funktion weder $R^2$ noch die $F$-Statistik ausgegeben hat, wie das zuvor bei den Regressionen mit `lm()` der Fall war. Im Fall von $R^2$ liegt das daran, dass jetzt separat für Fixed und Random Effects evaluiert werden muss, wie viel Varianz in den Daten sie jeweils beschreiben. Das Paket `MuMIn` stellt die Funktion `r.squaredGLMM()` bereit, die wir auf unser Mixed Model anwenden können:

```{r}
df.lmer %>% r.squaredGLMM()
```

Hier werden uns jetzt folgerichtig zwei $R^2$-Werte angezeigt. Der erste, `R2m` (das steht für **marginal $R^2$**), ist die Proportion der Varianz in den gemessenen Lautstärkewerten, die durch die Fixed Effects beschrieben wird, hier also 46%. Der zweite, `R2c` (für **conditional $R^2$**), ist die Proportion der Varianz in den gemessenen Lautstärkewerten, die durch die Fixed Effects und Random Effects *zusammen* beschrieben wird. Mit 96.4% haben wir also quasi die gesamte Varianz in den Daten beschrieben! (Solch hohe Werte werden Sie in echten linguistischen Studien niemals sehen). Hieraus können wir ableiten, dass die Random Effects $96.4 - 45.9 = 50.5$ Prozent der Varianz beschreiben. Es war also für den Datensatz `df` sehr wichtig, dass wir die Random Effects im Modell hatten.

## Likelihood Ratio Tests

Im Ergebnis des Modells fehlte außer dem $R^2$-Wert ein Indikator für die Güte des Modells, wie es die $F$-Statistik bei `lm()` war. Bei LMERs wird die Güte mit **Likelihood Ratio Tests** evaluiert. Dabei wird das verwendete Mixed Model mit einem anderen Mixed Model verglichen, bei dem ein Fixed oder Random Effect eliminiert wurde. So kann eingeschätzt werden, ob durch das Weglassen eines Faktors die Güte des Modells sinkt oder gleich bleibt.

An dieser Stelle sollten wir uns mit dem Begriff *likelihood* beschäftigen, vor allem im Unterschied zu *probability* (denn beide werden im Deutschen leider mit "Wahrscheinlichkeit" übersetzt). Im statistischen Kontext ist *probability* die Wahrscheinlichkeit eines Ergebnisses, wenn die Parameter (z.B. Regressionskoeffizienten) gegeben sind. Die $p$-Werte beschreiben diese Art von Wahrscheinlichkeit. Die *likelihood* hingegen ist die Wahrscheinlichkeit von Parametern, wenn die Daten gegeben sind. Wie wahrscheinlich (*likely*) ist die Schätzung für das Intercept (hier: 48.2 dB) für die Daten in `df`? Oder wie wahrscheinlich ist die Schätzung für die Slope for Geschlecht (40.2 dB) für die Daten in `df`? Mixed Models führen diese Schätzungen nach dem **Prinzip der Maximum Likelihood** aus (nachdem wir `REML = F` in `lmer()` angegeben haben), d.h. das Ziel des Mixed Models ist es, die Regressionskoeffizienten zu finden, die am wahrscheinlichsten (*likely*) für die gegebenen Daten sind.

![](img/information_criteria.png)

Im Ergebnis eines Mixed Models mit `lmer()` finden wir unter anderem die drei Werte AIC, BIC und log Likelihood. Letztere ist die logarithmierte maximierte Likelihood; dieser Wert ist immer negativ; je näher er jedoch an Null ist, desto besser ist das Modell. Der Likelihood Ratio Test vergleicht die log Likelihoods zweier Mixed Models und findet heraus, ob das gewählte Modell die Daten signifikant besser beschreibt als ein Modell, bei dem eine Variable weggelassen wird. Hier vergleichen wir unser Modell `df.lmer` mit verschiedenen anderen Modellen; zuerst mit einem Modell, bei dem die Variable `gender` weggelassen wurde.

```{r}
df.gender <- lmer(db ~ vowel + (1 + vowel | subject) + (1 | word), data = df, REML = F)
```

Die Funktion `anova()` testet den Vergleich der Modelle auf Signifikanz mithilfe eines $\chi^2$-Tests (sprich: Chi-Square oder Chi-Quadrat). "Anova" steht eigentlich für *analysis of variance*; der *default* der `anova()`-Funktion ist allerdings der $\chi^2$-Test durch das Argument `test = "Chisq"`. Ansonsten bekommt diese Funktion nur die Namen der beiden Modelle übergeben. Der $\chi^2$-Test prüft, ob sich die logarithmierte Likelihood der beiden Modelle signifikant voneinander unterscheidet.

```{r}
anova(df.lmer, df.gender)
```

Im Ergebnis dieses Tests sehen wir die beiden Modelle, die miteinander verglichen wurden. In der ersten Spalte der Tabelle, `npar`, steht die Anzahl an Parametern, die für das Modell geschätzt wurde. Anschließend sehen wir die Informationskriterien `AIC`, `BIC`, `logLik` sowie `deviance`, die wir schon aus den Ergebnissen von `lmer()` kennen. In der oberen Zeile stehen die Werte für das restriktive Modell, in der unteren Zeile die unseres Modells. Für unser Modell wird zusätzlich der $\chi^2$-Wert in der Spalte `Chisq` angegeben. Die $\chi^2$-Verteilung wird durch den Parameter `Df` (Freiheitsgrade) beschrieben. Die Freiheitsgrade sind hier die Anzahl an Regressionskoeffizienten, die im restriktiven Modell nicht geschätzt wurden (hier: Fixed Effect für `gender` und Interaktion zwischen `gender` und `vowel`; auch berechenbar aus der Differenz der beiden Werte in Spalte `npar`). In der Spalte `Pr(>Chisq)` findet sich schließlich der $p$-Wert, der aus der $\chi^2$-Verteilung mit zwei Freiheitsgraden für den Wert 10.59 abgelesen wurde. Da der $p$-Wert unter 0.05 liegt, unterscheiden sich die beiden Modelle signifikant; da die log Likelihood für unser Modell höher ist als die des restriktiven Modells (und AIC sowie BIC niedriger sind für unser als für das restriktive Modell), passt unser Modell besser zu den Daten als ein Modell ohne die Variable `gender`.

Die Formel für den $\chi^2$-Wert ist:

$$
\begin{aligned}
 LR &= -2ln \cdot \left( \frac{L_{m_1}}{L_{m_2}} \right) \\
 &= 2 \cdot (log(L_{m_2}) - log(L_{m_1}))
\end{aligned}
$$

$LR$ steht hier für *likelihood ratio*, deshalb enthält die Formel auch eine Division, nämlich die der Likelihood $L$ für das restriktive Modell $m_1$ geteilt durch die Likelihood für unser volles berechnetes Modell $m_2$. $ln$ steht für den natürlichen Logarithmus. Diese Formel lässt sich jedoch so umformen, dass wir anstatt den uns unbekannten Likelihoods unsere log Likelihoods aus dem Ergebnis von `lmer` verwenden können. Für den Likelihood Ratio Test oben kann man den $\chi^2$-Wert also manuell wie folgt berechnen:

```{r}
2 * (-416.36 - (-421.65))
```

<div class="green">
**Weiterführende Infos: $\chi^2$-Verteilung**

Genau wie Sie das schon für die Normal-, $t$- und $F$-Verteilung kennengelernt haben, geben Ihnen die Funktionen `dchisq()`, `pchisq()` und `qchisq()` die Möglichkeit, selbst mit der $\chi^2$-Verteilung zu arbeiten. Sie können sich also selber, wenn gewünscht, eine $\chi^2$-Verteilung mit den gewünschten Freiheitsgraden zeichnen oder den $p$-Wert für einen bestimmten $\chi^2$-Wert berechnen. Siehe auch:

```{r, eval = F}
?dchisq
```

</div>

Zuletzt berichten wir das Ergebnis des Likelihood Ratio Tests: **Ein Likelihood Ratio Test ergab, dass das gewählte Modell bessere Schätzungen für die Modellparameter abgab als ein vergleichbares Mixed Model ohne die Variable `gender` ($\chi^2$[2] = 10.6, $p$ < 0.01)...**

Nun können wir dasselbe für einige weitere Vergleiche machen:

```{r}
df.vowel <- lmer(db ~ gender + (1 | subject) + (1 | word), data = df, REML = F)
anova(df.lmer, df.vowel)
```

**...und als ein vergleichbares Mixed Model ohne die Variable `vowel` ($\chi^2$[4] = 96.7, $p$ < 0.001)...**

```{r}
df.subject <- lmer(db ~ gender * vowel + (1 | word), data = df, REML = F)
anova(df.lmer, df.subject)
```

**... und als ein vergleichbares Mixed Model ohne die Variable `subject` ($\chi^2$[3] = 173.6, $p$ < 0.001)...**

```{r}
df.word <- lmer(db ~ gender * vowel + (1 + vowel | subject), data = df, REML = F)
anova(df.lmer, df.word)
```

**... und schließlich als ein vergleichbares Mixed Model ohne die Variable `word` ($\chi^2$[1] = 191.0, $p$ < 0.001).**
