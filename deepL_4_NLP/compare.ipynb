{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD6I_7U41s0J"
   },
   "source": [
    "# Exercise Sheet 9: Domain Adaptation on TweetQA using T5\n",
    "\n",
    "In this exercise, you will evaluate T5 on question answering in the twitter domain, and then finetune the model.\n",
    "\n",
    "You should complete the parts of the exercise that are marked as **TODO**.\n",
    "A correctly completed **TODO** gives 2 bonus points. Partially correct answers give 1 bonus point.\n",
    "Some **TODO**s are inside a comment in a code block: Here, you should complete the line of code.\n",
    "Other **TODO**s are inside a text block: Here, you should write a few sentences to answer the question.\n",
    "\n",
    "**Important:** Some students were under the impression that you have to complete a TODO in a _single_ line of code. That is not the case, you can use as many lines as you need.\n",
    "\n",
    "**Submission deadline:** 03.02.2021, 23:59 Central European Time\n",
    "\n",
    "**Instructions for submission:** After completing the exercise, save a copy of the notebook as exercise9_tweetqa_MATRIKELNUMMER.ipynb, where MATRIKELNUMMER is your student ID number. Then upload the notebook to moodle (submission exercise sheet 9).\n",
    "\n",
    "In order to understand the code, it can be helpful to experiment a bit during development, e.g., to print tensors or their shapes. But please remove these changes before submitting the notebook. If we cannot run your notebook, or if a print statement is congesting stdout too much, then we cannot grade it. \n",
    "\n",
    "To make the most of this exercise, you should try to read and understand the entire code, not just the parts that contain a **TODO**. If you have questions, write them down for the exercise, which will happen in the week after the submission deadline.\n",
    "\n",
    "**CUDA:** You can use a GPU for this exercise (on colab: Runtime -> Change Runtime Type -> GPU). This is not mandatory, but it will speed up training epochs, thereby allowing you to test more hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMPNVCmemFNo"
   },
   "source": [
    "# Libraries and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Si054iqL1rqP"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.2.0\n",
    "!pip install -q datasets==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyhDLbn8PL1A"
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_U1tW1iVrv0"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 3 if torch.cuda.is_available() else 1\n",
    "PERCENTILES = (95, 100) if torch.cuda.is_available() else (80, 100)\n",
    "\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 64\n",
    "WARMUP_STEPS = 200\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 100\n",
    "LEARNING_RATE = 5e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "614DnldV5JnO"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuvFvP98mKA-"
   },
   "source": [
    "#Data preprocessing\n",
    "\n",
    "In this exercise, we will load the [tweet_qa dataset](https://huggingface.co/datasets/tweet_qa), which is a closed question answering dataset. Each data sample consists of a question, a tweet as context which contains the necessary information, and the correct answer. Luckily for us, T5 has already been trained on SQUAD, which is a similar task, but with the context consisting of snippets from Wikipedia articles. This means that we just need to bring our dataset into the right format, and then we can already take a look at how well T5 performs on it. After that, we will finetune on tweet_qa to improve performance (domain adaptation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xciWSN_H5hJd"
   },
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4nyW4c8ryic"
   },
   "source": [
    "Here, we are going to bring our dataset into the right format, tokenize it, and truncate the inputs using the same method as in the last exercise. Hint: to find out what format T5 expects questions and answers in, take a look at the [T5 paper](https://arxiv.org/pdf/1910.10683.pdf), specifically Appendix D.15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRd0lKNwAf18"
   },
   "outputs": [],
   "source": [
    "def reformat_for_t5(example):\n",
    "  # TODO: bring the question and context into the format that T5 is used to from SQUAD, for each example. You should also lowercase the inputs.\n",
    "  example['src_texts'] = None\n",
    "  return example\n",
    "\n",
    "def get_max_length(tokenizer, train_dataset, column, percentile):\n",
    "  def get_lengths(batch):\n",
    "    return tokenizer(batch, padding=False, return_length=True)\n",
    "\n",
    "  lengths = train_dataset.map(get_lengths, input_columns=column, batched=True)['length']\n",
    "  return int(np.percentile(lengths, percentile)) +1\n",
    "\n",
    "ANSWER_SEP = ':::::'\n",
    "dataset = load_dataset('tweet_qa')\n",
    "dataset = dataset.map(reformat_for_t5)\n",
    "dataset = dataset.map(lambda x: {'tgt_texts': ANSWER_SEP.join(x['Answer'])}) \n",
    "#note: this is a hack: The Seq2SeqTrainer we will use later does not allow us to pass multiple labels to it. However, our dataset contains multiple correct answers for each question in the validation part. Therefore we concatenate them here so we can later split them in compute_metrics. Don't worry about this too much.\n",
    "\n",
    "max_length = get_max_length(tokenizer, dataset['train'], 'src_texts', PERCENTILES[0])\n",
    "max_target_length = get_max_length(tokenizer, dataset['train'], 'tgt_texts', PERCENTILES[1])\n",
    "\n",
    "def tokenize(batch):\n",
    "  # TODO: call the prepare_seq2seq_batch function of the T5Tokenizer. Be careful to set padding to 'max_length'\n",
    "  return None\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format('torch', columns=['input_ids', 'labels', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byuQqZNnmYNN"
   },
   "source": [
    "# Defining Metrics\n",
    "\n",
    "We are going to evaluate with the same metrics used by SQUAD, Exact Match and Bag-of-Words F1. They're already available in ðŸ¤— datasets, but we need to get our predictions and answers into shape first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeUWhVvOCyYr"
   },
   "outputs": [],
   "source": [
    "def squad_metrics(eval_prediction):\n",
    "  # TODO: use the batch_decode function of the tokenizer to decode the predictions and the label_ids (hint: they're both in the only argument of this function). Remember to set skip_special_tokens so that we don't generate padding tokens.\n",
    "  predictions = None\n",
    "  answers = None\n",
    "  \n",
    "  answers = [answer.split(ANSWER_SEP) for answer in answers]\n",
    "  assert all([len(ans) == 2 for ans in answers])\n",
    "  \n",
    "  predictions = [{'id': str(i), 'prediction_text': pred.strip().lower()} \\\n",
    "                 for i, pred in enumerate(predictions)]\n",
    "  references = [{'id': str(i), 'answers': {'text': ans, 'answer_start': []}} \\\n",
    "                for i, ans in enumerate(answers)]\n",
    "\n",
    "  #TODO: load the metric \"squad\" using the function we've imported at the top of the notebook. \n",
    "  metric = None\n",
    "  #TODO: use the add_batch method to pass our predictions and references to the metric\n",
    "  #TODO: compute the metric\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YBiuXlFmeBg"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rc2Z_0iC92-q"
   },
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrMY-paFgzLk"
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir='./logs/',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "#TODO: instantiate the Seq2SeqTrainer. Remember to pass all the objects we've constructed so far that we can use here.\n",
    "\n",
    "trainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGRTeSqt5LM1"
   },
   "source": [
    "# Evaluation before Domain Adaptation\n",
    "\n",
    "First, let's test how T5 performs on tweet_qa without domain adaptation. It should work fairly well, given that it's been trained on SQUAD, which is a very similar task, just on a different domain. For evaluation, we'll perform beam search during generation with a beam size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cwsbNzSloo5"
   },
   "outputs": [],
   "source": [
    "print(trainer.evaluate(num_beams=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkVOFsS86fFv"
   },
   "source": [
    "# Domain Adaptation\n",
    "\n",
    "Now let's see how much we can improve our performance by finetuning on our in-domain data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOPyoaJ66fjH"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGnAvAZDUWYF"
   },
   "outputs": [],
   "source": [
    "print(trainer.evaluate())\n",
    "print(trainer.evaluate(num_beams=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv_0Tztp6pve"
   },
   "source": [
    "# Report\n",
    "\n",
    "**TODO:** Write a brief report of your results. Include the hyperparameters that you used, the results T5 achieved on the dataset before and after domain adaptation, and why the evaluation during training may have returned different results than the separate evaluations before and after training. Also report on the different evaluation speeds resulting from this, and give a brief explanation why finetuning on in-domain data was helpful for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzPfz0LP6roG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercise9_tweetqa.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
