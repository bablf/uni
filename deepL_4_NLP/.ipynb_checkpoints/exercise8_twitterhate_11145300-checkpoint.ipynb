{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njjBjUMNQthF"
   },
   "source": [
    "# Exercise 8: Hate Speech Detection with BERT\n",
    "\n",
    "In this exercise, you will finetune a BERT model to do hate speech detection on tweets. You will also modify the training dataset to make training more efficient.\n",
    "\n",
    "You should complete the parts of the exercise that are marked as **TODO**.\n",
    "A correctly completed **TODO** gives 2 bonus points. Partially correct answers give 1 bonus point.\n",
    "Some **TODO**s are inside a comment in a code block: Here, you should complete the line of code.\n",
    "Other **TODO**s are inside a text block: Here, you should write a few sentences to answer the question.\n",
    "\n",
    "**Important:** Some students were under the impression that you have to complete a TODO in a _single_ line of code. That is not the case, you can use as many lines as you need.\n",
    "\n",
    "**Submission deadline:** 27.01.2021, 23:59 Central European Time\n",
    "\n",
    "**Instructions for submission:** After completing the exercise, save a copy of the notebook as exercise8_twitterhate_MATRIKELNUMMER.ipynb, where MATRIKELNUMMER is your student ID number. Then upload the notebook to moodle (submission exercise sheet 8).\n",
    "\n",
    "In order to understand the code, it can be helpful to experiment a bit during development, e.g., to print tensors or their shapes. But please remove these changes before submitting the notebook. If we cannot run your notebook, or if a print statement is congesting stdout too much, then we cannot grade it. \n",
    "\n",
    "To make the most of this exercise, you should try to read and understand the entire code, not just the parts that contain a **TODO**. If you have questions, write them down for the exercise, which will happen in the week after the submission deadline.\n",
    "\n",
    "**CUDA:** You can use a GPU for this exercise (on colab: Runtime -> Change Runtime Type -> GPU). This is not mandatory, but it will speed up training epochs, thereby allowing you to test more hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcO-cVXuS5ZV"
   },
   "source": [
    "# Required libraries\n",
    "When working with ðŸ¤— transformers, or any fast-changing software library, you should be extra careful to fix the library versions when you begin your project, and not change versions while you're developing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IX1P2dCv0KCB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.2.0 in /home/flo/.local/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy in /home/flo/.local/lib/python3.8/site-packages (from transformers==4.2.0) (1.18.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/flo/.local/lib/python3.8/site-packages (from transformers==4.2.0) (4.41.1)\n",
      "Requirement already satisfied: sacremoses in /home/flo/.local/lib/python3.8/site-packages (from transformers==4.2.0) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/flo/.local/lib/python3.8/site-packages (from transformers==4.2.0) (2020.11.13)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.2.0) (2.22.0)\n",
      "Requirement already satisfied: packaging in /home/flo/.local/lib/python3.8/site-packages (from transformers==4.2.0) (20.8)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /home/flo/.local/lib/python3.8/site-packages (from transformers==4.2.0) (0.9.4)\n",
      "Requirement already satisfied: filelock in /home/flo/.local/lib/python3.8/site-packages (from transformers==4.2.0) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers==4.2.0) (1.14.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers==4.2.0) (7.0)\n",
      "Requirement already satisfied: joblib in /home/flo/.local/lib/python3.8/site-packages (from sacremoses->transformers==4.2.0) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/flo/.local/lib/python3.8/site-packages (from packaging->transformers==4.2.0) (2.4.7)\n",
      "Requirement already satisfied: datasets==1.2.0 in /home/flo/.local/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/flo/.local/lib/python3.8/site-packages (from datasets==1.2.0) (0.70.11.1)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /home/flo/.local/lib/python3.8/site-packages (from datasets==1.2.0) (4.41.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from datasets==1.2.0) (2.22.0)\n",
      "Requirement already satisfied: pandas in /home/flo/.local/lib/python3.8/site-packages (from datasets==1.2.0) (1.2.0)\n",
      "Requirement already satisfied: dill in /home/flo/.local/lib/python3.8/site-packages (from datasets==1.2.0) (0.3.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/flo/.local/lib/python3.8/site-packages (from datasets==1.2.0) (1.18.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/flo/.local/lib/python3.8/site-packages (from datasets==1.2.0) (2.0.0)\n",
      "Requirement already satisfied: xxhash in /home/flo/.local/lib/python3.8/site-packages (from datasets==1.2.0) (2.0.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas->datasets==1.2.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas->datasets==1.2.0) (2.7.3)\n",
      "Requirement already satisfied: tensorflow in /home/flo/.local/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/flo/.local/lib/python3.8/site-packages (from tensorflow) (1.18.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.9.2->tensorflow) (45.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/flo/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/flo/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/flo/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/flo/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/flo/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.22.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/flo/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/flo/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/flo/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/flo/.local/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/flo/.local/lib/python3.8/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers==4.2.0\n",
    "!pip3 install datasets==1.2.0\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qAtw5tkk0kQv"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, DistilBertTokenizerFast, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fiJ6rRX60a7S"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 64\n",
    "WARMUP_STEPS = 50\n",
    "WEIGHT_DECAY = 0.01\n",
    "LOGGING_STEPS = 50\n",
    "LEARNING_RATE = 5e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjuPF1VLTNfW"
   },
   "source": [
    "# Data\n",
    "In this exercise, we will finetune a BERT model to perform hate speech detection on data from twitter. Hate speech detection is the task of classifying sentences, or in this case, tweets, as hate speech or not hate speech, for example so that we can automatically report it or filter it out. The dataset we're using is from the ðŸ¤—/datasets library, so we can load it very easily: https://huggingface.co/datasets/tweets_hate_speech_detection \n",
    "As the dataset currently only contains a training portion, we are going to use the Slicing API (https://huggingface.co/docs/datasets/splits.html) to divide it into a training and a development set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bprkP_9n4HVF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0)\n",
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('tweets_hate_speech_detection', split='train[:80%]')\n",
    "dev_dataset = load_dataset('tweets_hate_speech_detection', split='train[80%:]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIVqHw7il91t"
   },
   "source": [
    "Now let's look at some examples of tweets containing hate speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mhYyyMRFmFEa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you might be a libtard if... #libtard  #sjw #liberal #politics \n",
      "@user take out the #trash america...  - i voted against #hate - i voted against  - i voted against  - i votÃ¢Â€Â¦ \n",
      "if you hold open a door for a woman because she's a woman and not because it's a nice thing to do, that's . don't even try to deny it\n",
      "@user this man ran for governor of ny, the state with the biggest african-american population    #Ã¢Â€Â¦\n",
      "#stereotyping #prejudice  offer no #hope or solutions but create the same old repetitive #hate #conflictÃ¢Â€Â¦ \n"
     ]
    }
   ],
   "source": [
    "label_mask = np.array(train_dataset['label']) == 1\n",
    "hate_speech_examples = train_dataset[label_mask][\"tweet\"]\n",
    "\n",
    "print('\\n'.join(hate_speech_examples[15:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF5QrwInnv9l"
   },
   "source": [
    "# Tokenization\n",
    "Now that we have the datasets loaded, we need to tokenize them. This is very easy with ðŸ¤— transformers, but to make our model faster we are first going to find out the smallest sequence length that we can comfortably work with. Tweets are very short, so we should be able to choose a sequence length that is a lot shorter than the standard 512 that most BERT models run with. We are going to tokenize the whole dataset with a very generous sequence length, choose our new sequence length so that at least 95% of all tweets are within this length, and then tokenize again while truncating those that are longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JbCeop4T0nV0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0/cache-9a9f5036699670ae.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21aac36e4e734f65becd22a43fd68673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193f52aad29446c8ae7408cc60d65a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_tokenizer(train_dataset, dev_dataset):\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased') #actually the same as BertTokenizerFast\n",
    "    def get_sequence_len(tokenizer, train_dataset, dev_dataset):\n",
    "\n",
    "        def tokenize_for_lengths(batch):\n",
    "            return tokenizer(batch['tweet'], padding=False, truncation=True, max_length=128, return_length = True)\n",
    "\n",
    "        train_dataset_for_lengths = train_dataset.map(tokenize_for_lengths, batched=True, batch_size=len(train_dataset))\n",
    "\n",
    "        tweet_lengths = np.array(train_dataset_for_lengths[:]['length'])\n",
    "        chosen_sequence_len = int(np.percentile(np.sort(tweet_lengths),95)+1)\n",
    "        \n",
    "        return chosen_sequence_len\n",
    "\n",
    "    chosen_sequence_len = get_sequence_len(tokenizer, train_dataset, dev_dataset)\n",
    "\n",
    "    def tokenize(batch, sequence_len):\n",
    "        return tokenizer(batch['tweet'], padding=\"max_length\", truncation=True, max_length=sequence_len)\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize, fn_kwargs={'sequence_len': chosen_sequence_len}, batched=True, batch_size=len(train_dataset))\n",
    "    dev_dataset = dev_dataset.map(tokenize, fn_kwargs={'sequence_len': chosen_sequence_len}, batched=True, batch_size=len(dev_dataset))\n",
    "\n",
    "    return (train_dataset, dev_dataset)\n",
    "\n",
    "train_dataset, dev_dataset = run_tokenizer(train_dataset, dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TFrpHCnS63dU"
   },
   "outputs": [],
   "source": [
    "def set_format(train_dataset, dev_dataset):\n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    dev_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    return (train_dataset, dev_dataset)\n",
    "\n",
    "train_dataset, dev_dataset = set_format(train_dataset, dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl8zKsO2pLF1"
   },
   "source": [
    "# Model Definition\n",
    "To make training as fast as possible, we are going to load the Distilbert model. To do finetuning, we are going to load our BERT model, add classification heads on top of it and then train with our dataset and specific task. In this case, we're doing binary sequence classification: we're classifying sequences, tweets, as either hate speech (label 1) or not hate speech (label 0). Luckily for us, in ðŸ¤— transformers, we only need to instantiate a BertForSequenceClassification model from a pretrained generic BERT model and specify how many labels we want for the classification. We will get a warning that some of the weights (those of the classification heads) have not been trained yet, but that's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JigsqEv1zniW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def define_model():\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2) \n",
    "    # according to the docs: kwargs can be used to update the configuration object\n",
    "    # I think this is the easier implementation, than creating a new Config Object\n",
    "    return model\n",
    "\n",
    "model = define_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QaSWbxsqKGl"
   },
   "source": [
    "Here, we are going to set up two things. The first is a function that we can then pass to the Trainer class to tell it what kinds of metrics we want to compute on our development set, and the other is a Early Stopping Callback so that just like in the last exercise sheet, we can stop training if the development performance isn't increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XuoMTUw4o9pK"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience= 2, early_stopping_threshold = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_oXuQTaqjn0"
   },
   "source": [
    "Now, we are going to define some training arguments that we are going to pass to the Trainer class which will handle the training for us. Very important here is that we have set the metric for best model to F1-measure and load_best_model_at_end to True, so that F1-measure is used for early stopping and we load the best model at the end, not the one for which F1-measure has already decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZRt_KeLho5aB"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    logging_dir='./logs/',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDGsr8W_reo2"
   },
   "source": [
    "Here we instantiate the Trainer class using our model, the training args, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mjWIXKLxo28O"
   },
   "outputs": [],
   "source": [
    "def define_trainer(model, training_args, train_dataset, dev_dataset, compute_metrics, early_stopping_callback):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = [early_stopping_callback]\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "trainer = define_trainer(model, training_args, train_dataset, dev_dataset, compute_metrics, early_stopping_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2AYacYNrjPW"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A9psEktG7gjd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flo/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='350' max='3198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 350/3198 02:11 < 17:54, 2.65 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.249589</td>\n",
       "      <td>0.930695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.677500</td>\n",
       "      <td>660.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.213300</td>\n",
       "      <td>0.168138</td>\n",
       "      <td>0.945401</td>\n",
       "      <td>0.415410</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.279910</td>\n",
       "      <td>10.042900</td>\n",
       "      <td>636.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.194200</td>\n",
       "      <td>0.290523</td>\n",
       "      <td>0.839643</td>\n",
       "      <td>0.421884</td>\n",
       "      <td>0.281203</td>\n",
       "      <td>0.844244</td>\n",
       "      <td>9.830400</td>\n",
       "      <td>650.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.199700</td>\n",
       "      <td>0.158480</td>\n",
       "      <td>0.955882</td>\n",
       "      <td>0.593660</td>\n",
       "      <td>0.820717</td>\n",
       "      <td>0.465011</td>\n",
       "      <td>9.887400</td>\n",
       "      <td>646.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.161324</td>\n",
       "      <td>0.946965</td>\n",
       "      <td>0.643533</td>\n",
       "      <td>0.602362</td>\n",
       "      <td>0.690745</td>\n",
       "      <td>10.346000</td>\n",
       "      <td>617.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.162100</td>\n",
       "      <td>0.155532</td>\n",
       "      <td>0.956195</td>\n",
       "      <td>0.621622</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.519187</td>\n",
       "      <td>10.378700</td>\n",
       "      <td>615.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.146445</td>\n",
       "      <td>0.954318</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>0.861244</td>\n",
       "      <td>0.406321</td>\n",
       "      <td>10.259700</td>\n",
       "      <td>623.019000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flo/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def train(trainer):\n",
    "    trainer.train()\n",
    "    return trainer\n",
    "\n",
    "trainer = train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2AY8YgZQ7hBC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(trainer):\n",
    "    trainer.evaluate()\n",
    "\n",
    "evaluate(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TeDlUu5ryRP"
   },
   "source": [
    "# What happened?\n",
    "\n",
    "It looks like our accuracy is 93%, but our Precision, Recall and F1-measure are all 0. What happened?\n",
    "\n",
    "Let's take a look at our dataset again. How are the classes distributed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kLjmGX4js_P3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset tweets_hate_speech_detection (/home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07014579813528565\n"
     ]
    }
   ],
   "source": [
    "hate_speech_dataset = load_dataset('tweets_hate_speech_detection', split='train')\n",
    "print(np.mean(np.array(hate_speech_dataset['label'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55_bWkT-tDYK"
   },
   "source": [
    "It looks like only about 7% of the training dataset is actually hate speech. This extreme imbalance means that the model takes the path of least resistance for the loss, which is to predict \"not hate speech\" all the time. \n",
    "\n",
    "Let's do something about that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVXqgcADTr8G"
   },
   "source": [
    "# Rebalancing the dataset\n",
    "\n",
    "To help with this, we are simply going to rebalance the dataset so that it contains all the examples for hate speech, but only as many negative examples, so that the dataset is balanced 50-50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "H1ZCyUnYTwEM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0/cache-c00e6f14a823ac30.arrow\n",
      "Loading cached processed dataset at /home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0/cache-49ce4ac3ab2f8d3c.arrow\n",
      "Loading cached sorted indices for dataset at /home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0/cache-b327a09a8b54561b.arrow\n",
      "Loading cached shuffled indices for dataset at /home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0/cache-57d433a9ee4a78c9.arrow\n"
     ]
    }
   ],
   "source": [
    "def rebalance_dataset(dataset):\n",
    "    num_label_1 = len(dataset.filter(lambda obj: obj['label']==1)) #TODO: find out how often the label 1 appears in hate_speech_dataset's label column\n",
    "    num_label_0 = len(dataset.filter(lambda obj: obj['label']==0)) #TODO: find out how often the label 0 appears in hate_speech_dataset's label column\n",
    "    sorted_dataset = dataset.sort('label')\n",
    "    balanced_dataset = sorted_dataset.select(list(range(0,num_label_1)) + list(range(num_label_0, num_label_0 + num_label_1)))\n",
    "    return balanced_dataset.shuffle(seed=42)\n",
    "\n",
    "balanced_dataset = rebalance_dataset(hate_speech_dataset)\n",
    "l = len(balanced_dataset)\n",
    "eighty_percent = int(l*.8)\n",
    "balanced_train_dataset = balanced_dataset.select(list(range(0,eighty_percent))) #using the dataset.select method, select the first 80% of the balanced dataset\n",
    "balanced_dev_dataset = balanced_dataset.select(list(range(eighty_percent,l))) #using the dataset.select method, select the last 20% of the balanced dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4sRzmmGts6i"
   },
   "source": [
    "Now that we have balanced the dataset, let's run everything again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PeQWilkruXdl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0/cache-4233158099525b21.arrow\n",
      "Loading cached processed dataset at /home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0/cache-ffed11ce5742e632.arrow\n",
      "Loading cached processed dataset at /home/flo/.cache/huggingface/datasets/tweets_hate_speech_detection/default/0.0.0/c32a982d8b2d6233065d820ac655454174f8aaa8faddc74979cf793486acd3b0/cache-3b5786ba96ed5a4c.arrow\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 01:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.367716</td>\n",
       "      <td>0.851728</td>\n",
       "      <td>0.857449</td>\n",
       "      <td>0.821355</td>\n",
       "      <td>0.896861</td>\n",
       "      <td>1.434600</td>\n",
       "      <td>625.278000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.396487</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.824108</td>\n",
       "      <td>0.912807</td>\n",
       "      <td>0.751121</td>\n",
       "      <td>1.477000</td>\n",
       "      <td>607.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.316887</td>\n",
       "      <td>0.867336</td>\n",
       "      <td>0.861144</td>\n",
       "      <td>0.897810</td>\n",
       "      <td>0.827354</td>\n",
       "      <td>1.496200</td>\n",
       "      <td>599.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.306965</td>\n",
       "      <td>0.885173</td>\n",
       "      <td>0.883616</td>\n",
       "      <td>0.890661</td>\n",
       "      <td>0.876682</td>\n",
       "      <td>1.512700</td>\n",
       "      <td>592.974000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.444368</td>\n",
       "      <td>0.861761</td>\n",
       "      <td>0.848780</td>\n",
       "      <td>0.930481</td>\n",
       "      <td>0.780269</td>\n",
       "      <td>1.503100</td>\n",
       "      <td>596.774000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.366218</td>\n",
       "      <td>0.877369</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.941704</td>\n",
       "      <td>1.467100</td>\n",
       "      <td>611.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>0.382991</td>\n",
       "      <td>0.886288</td>\n",
       "      <td>0.890323</td>\n",
       "      <td>0.855372</td>\n",
       "      <td>0.928251</td>\n",
       "      <td>1.472800</td>\n",
       "      <td>609.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.354292</td>\n",
       "      <td>0.894091</td>\n",
       "      <td>0.889919</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.860987</td>\n",
       "      <td>1.494600</td>\n",
       "      <td>600.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.126200</td>\n",
       "      <td>0.323049</td>\n",
       "      <td>0.904125</td>\n",
       "      <td>0.903153</td>\n",
       "      <td>0.907240</td>\n",
       "      <td>0.899103</td>\n",
       "      <td>1.434700</td>\n",
       "      <td>625.221000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "balanced_train_dataset, balanced_dev_dataset = run_tokenizer(balanced_train_dataset, balanced_dev_dataset)\n",
    "balanced_train_dataset, balanced_dev_dataset = set_format(balanced_train_dataset, balanced_dev_dataset)\n",
    "balanced_model = define_model()\n",
    "balanced_trainer = define_trainer(balanced_model, training_args, balanced_train_dataset, balanced_dev_dataset, compute_metrics, early_stopping_callback)\n",
    "balanced_trainer = train(balanced_trainer)\n",
    "evaluate(balanced_trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgn6H0YrHwnJ"
   },
   "source": [
    "That's better! **TODO**: Write a few sentences about how much F1-measure has improved, and why.\n",
    "\n",
    "So for me the F1 Measure isn't 0 before the balancing. But at the same time of course the F1 Measure is really bad. See below:\n",
    "\n",
    "Accuracy------F1-------Precision---Recall\n",
    "\n",
    "0.946965 \t0.643533 \t0.602362 \t0.690745\n",
    "\n",
    "But the Training never finished because of zero-division. \n",
    "\n",
    "After balancing the Datasets I get better results: \n",
    "\n",
    "Accuracy------F1-------Precision---Recall\n",
    "\n",
    "0.904125 \t0.903153 \t0.907240 \t0.899103\n",
    "\n",
    "And even though the accuracy went down a bit, the Precision and Recall increased by a lot, because the model stopped labeling everything as no-hate-speech. As a result, F1 obviously increased because of the increased Precision and Recall. (see formula for F1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "exercise8_twitterhate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
