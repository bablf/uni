{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Model, GPT2Tokenizer\n",
    "from torch.utils.data import IterableDataset, Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class UciGPT:\n",
    "    \"\"\"\n",
    "    This is a, on UCI notation finetuned, GPT-2 Model with the help of gpt-2-simple.\n",
    "    See file uic_gpt2.py\n",
    "    \"\"\"\n",
    "    config = GPT2Config.from_pretrained(\"gpt2-medium\")\n",
    "    config.output_hidden_states = True\n",
    "    model = GPT2Model.from_pretrained(\"uci_checkpoint/run1/model-18500.index\", from_tf=True, config=config)\n",
    "    tokenizer = GPT2Tokenizer(\"uci_checkpoint/run1/encoder.json\", \"uci_checkpoint/run1/vocab.bpe\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    name = \"UciGPT\"\n",
    "    notation = \"uci\"\n",
    "\n",
    "# create or open probing_dataset\n",
    "\n",
    "def train_probing_classifier(chess_model, dataset):\n",
    "    for x, boards in DataLoader(dataset, batch_size=1):\n",
    "        print(x)\n",
    "        #pt_outputs = chess_model.model(**games)  # todo check  shape (batch_size, sequence_length, hidden_size)\n",
    "        #probing_classifier(pt_outputs.last_hidden_state)\n",
    "        #exit()\n",
    "\n",
    "\n",
    "class ChessGamesDataset(IterableDataset):  # todo maybe iterabledataset if memory problems\n",
    "    def __init__(self, filename, model_name, tokenizer):\n",
    "        self.filename = filename\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.max_length = max_length\n",
    "        #self.tokenizer.model_max_length = self.max_length\n",
    "\n",
    "    def game_mapper(self, file_iter):\n",
    "        game = file_iter.split(\";\")\n",
    "        board = np.fromstring(game[-1][1:-1], dtype=int, sep=' ')\n",
    "        if self.model_name == \"UciGPT\":\n",
    "            print(\"uci\")\n",
    "            return self.tokenizer(game[1], return_tensors='pt', padding='max_length', return_length=True), board\n",
    "        elif self.model_name == \"PgnGPT\":\n",
    "            x = self.tokenizer(game[0], return_tensors='pt', padding='max_length', return_length=True)\n",
    "        elif self.model_name == \"SpecialGPT\":\n",
    "            pass\n",
    "        else:  # == PretrainedGPT\n",
    "            pass  # todo check what preprocessing is nesesary for this model        print(gameannotation)\n",
    "        return x, np.fromstring(game[-1][1:-1], dtype=int, sep=' ')\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Create an iterator\n",
    "        file_itr = open(self.filename, encoding='cp1252')\n",
    "        # Map each element using the line_mapper\n",
    "        mapped_itr = map(self.game_mapper, file_itr)\n",
    "        return mapped_itr\n",
    "        \n",
    "class ChessGamesDataset2(Dataset):\n",
    "    def __init__(self, data, model_name, tokenizer):\n",
    "        self.data = data\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.max_length = len(max(self.data, key=lambda x: len(x[0]))[0])\n",
    "        #self.tokenizer.model_max_length = self.max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pgn, uci, board = self.data[index]\n",
    "        if self.model_name == \"UciGPT\":\n",
    "            return self.tokenizer(uci, return_tensors='pt', padding='max_length', return_length=True), board\n",
    "        elif self.model_name == \"PgnGPT\":\n",
    "            return self.tokenizer(pgn, return_tensors='pt', padding='max_length'), board\n",
    "        elif self.model_name == \"SpecialGPT\":\n",
    "            pass  # todo convert strings to human readable text with feldman_gpt2\n",
    "            #   probably strip beginnning tags\n",
    "        else:  # == PretrainedGPT\n",
    "            pass  # todo check what preprocessing is nesesary for this model\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "for model in [UciGPT]:  # [\"pretrained_gpt\",\"pgn_gpt\", \"uci_gpt\", \"special_gpt\"\n",
    "    chess_models = [UciGPT]\n",
    "    for model in chess_models:  # [\"pretrained_gpt\",\"pgn_gpt\", \"uci_gpt\", \"special_gpt\"]\n",
    "        # Creating the iterable dataset object\n",
    "        data = pickle.load(open(\"data/probing_dataset.txt\", \"rb\"))\n",
    "        dataset = ChessGamesDataset2(data, model.name, model.tokenizer)\n",
    "\n",
    "        \n",
    "        #dataset = ChessGamesDataset(\"data/probing_dataset2.txt\", model.name, model.tokenizer)\n",
    "        \n",
    "        \n",
    "        train_probing_classifier(model, dataset)\n",
    "        #train_probing_classifier(model, dataset, probing_classifier)\n",
    "        \n",
    "        # todo save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
